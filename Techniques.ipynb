{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/ITSAIDI/NLP_Chapitre/blob/main/Techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwp2GLa1JOIB"
      },
      "source": [
        "# Les Concepts de base :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaAZxryLLNpY"
      },
      "source": [
        "## Libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0P1D9a9LMS7"
      },
      "outputs": [],
      "source": [
        "!pip install -U pip setuptools wheel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6s5Ab8jL-OX"
      },
      "outputs": [],
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download fr_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bypcYa6iMPWN"
      },
      "outputs": [],
      "source": [
        "# Restart the Kernel\n",
        "exit(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_Q18j-9PbbC",
        "outputId": "71146b73-bbe1-44c7-8acd-5f072edd8939"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.23.2-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.1)\n",
            "Downloading openai-1.23.2-py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.23.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eporz7XLIuQm"
      },
      "source": [
        "## Cosine Similarity :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2Ln1drTIuQv",
        "outputId": "69a6e379-80e4-42b5-c712-953282322d7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine Similarity: 0.5185449728701348\n",
            "Cosine Distance: 0.4814550271298652\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Define the two vectors representing your documents\n",
        "doc1 = np.array([3, 2, 0, 1, 4])\n",
        "doc2 = np.array([1, 3, 1, 2, 0])\n",
        "\n",
        "# Calculate cosine similarity\n",
        "cosine_sim = cosine_similarity([doc1], [doc2])[0][0]\n",
        "\n",
        "# Calculate cosine distance (complement of cosine similarity)\n",
        "cosine_dist = 1 - cosine_sim\n",
        "\n",
        "print(f\"Cosine Similarity: {cosine_sim}\")\n",
        "print(f\"Cosine Distance: {cosine_dist}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJ0Ytku_IuQ2"
      },
      "source": [
        "## Tokenization :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FF6RC36aIuQ3"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('fr_core_news_sm') # French Language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KTVczDriIuQ4"
      },
      "outputs": [],
      "source": [
        "Text = \"Je veux comprendre le NLP\"\n",
        "doc = nlp(Text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KcNNBeAIuQ5",
        "outputId": "f921eb56-043f-4a01-bf8c-bee2b4889963"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Je\n",
            "veux\n",
            "comprendre\n",
            "le\n",
            "NLP\n"
          ]
        }
      ],
      "source": [
        "for token in doc:\n",
        "    print(token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoZNj1n3IuQ6",
        "outputId": "80b36139-5765-4a2f-d4a6-219ea41a9075"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLP\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "# token here in an Object from the class Token of Spacy\n",
        "token = doc[4]\n",
        "print(token)\n",
        "print(token.like_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORa7LGdSIuQ7",
        "outputId": "9b4d9734-521d-4414-d092-f0f67ee5645b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "support@exemple.com\n",
            "refunds@exemple.com\n"
          ]
        }
      ],
      "source": [
        "# Extract emails from a text with spacy\n",
        "Text = \"Cher client,Nous vous remercions pour votre commande passée sur notre site.Votre colis sera expédié demain matin. Merci pour votre confiance.Si vous avez des questions, n'hésitez pas à nous contacter à l'adresse suivante : support@exemple.com. De plus, pour toute demande de remboursement, veuillez nous écrire à refunds@exemple.com.\"\n",
        "doc = nlp(Text)\n",
        "for token in doc:\n",
        "    if token.like_email:\n",
        "        print(token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DEqNsc2IuQ-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhFCstzdIuQ-"
      },
      "source": [
        "## Lemmatization :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaYcuYp-IuQ_",
        "outputId": "d1c89f57-7ca0-4af2-fdde-bd8762bea2ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rule\n",
            "['I', 'be', 'read', 'the', 'paper', '.']\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# English pipelines include a rule-based lemmatizer\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
        "print(lemmatizer.mode)  # 'rule'\n",
        "\n",
        "doc = nlp(\"I was reading the paper.\")\n",
        "print([token.lemma_ for token in doc])\n",
        "# ['I', 'be', 'read', 'the', 'paper', '.']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPVNnwTFIuQ_"
      },
      "source": [
        "## POS Tagging :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7KCpOgDIuQ_",
        "outputId": "0a5cb2d5-130e-48de-c1a0-c0fb73f6a62e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Apple PROPN\n",
            "is AUX\n",
            "looking VERB\n",
            "at ADP\n",
            "buying VERB\n",
            "U.K. PROPN\n",
            "startup NOUN\n",
            "for ADP\n",
            "$ SYM\n",
            "1 NUM\n",
            "billion NUM\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text,token.pos_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWgNR8GoIuRA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "supl_GziIuRA"
      },
      "source": [
        "## Text representation (Vector Space Model) :\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1uK0bZQPI0s"
      },
      "source": [
        "### BOW :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlMBSg6mPK5S",
        "outputId": "78aa9fdb-533d-45da-dce8-02bd73c03374"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['bird' 'cat' 'dog' 'hat' 'house' 'in' 'sky' 'the']\n",
            "[[0 1 0 1 0 1 0 2]\n",
            " [0 0 1 0 1 1 0 2]\n",
            " [1 0 0 0 0 1 1 2]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Sample sentences\n",
        "sentences = [\"The cat in the hat\",\n",
        "\"The dog in the house\", \"The bird in the sky\"]\n",
        "# Create a CountVectorizer object\n",
        "vectorizer = CountVectorizer()\n",
        "# Use the fit_transform method to transform the sentences into a bag of words\n",
        "bow = vectorizer.fit_transform(sentences)\n",
        "# Print the vocabulary (features) of the bag of words\n",
        "print(vectorizer.get_feature_names_out())\n",
        "# Print the bag of words\n",
        "print(bow.toarray())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHt4O5_AIuRV"
      },
      "source": [
        "### N-Gram :\n",
        "- Insted of creating a vocabulary singular tokens , we can create n-grams.(more meaningful)\n",
        "- N-gram with single words is the BOW."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rp7Kv5P6IuRV",
        "outputId": "be1a8c3e-d570-4acf-8e44-0ab323ad27a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'my': 5,\n",
              " 'name': 6,\n",
              " 'is': 2,\n",
              " 'simon': 7,\n",
              " 'and': 0,\n",
              " 'live': 3,\n",
              " 'in': 1,\n",
              " 'london': 4}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(ngram_range=(1,1))\n",
        "text = \"My name is Simon, and I live in London\"\n",
        "vectorizer.fit([text])\n",
        "vectorizer.vocabulary_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exarCLToKeqx"
      },
      "source": [
        "### TF-IDF :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kESeh863Kkoe",
        "outputId": "7079b25d-6587-4e82-bcb7-30200505b2c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['bird' 'cat' 'flying' 'in' 'jumped' 'roared' 'sky' 'the' 'tiger' 'white']\n",
            "[[0.         0.65249088 0.         0.         0.65249088 0.\n",
            "  0.         0.38537163 0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.54645401\n",
            "  0.         0.32274454 0.54645401 0.54645401]\n",
            " [0.47952794 0.         0.47952794 0.47952794 0.         0.\n",
            "  0.47952794 0.28321692 0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Example documents\n",
        "docs = [\"The cat jumped\",\n",
        "        \"The white tiger roared\",\n",
        "        \"Bird flying in the sky\"]\n",
        "# Create a TfidfVectorizer object\n",
        "vectorizer = TfidfVectorizer()\n",
        "# Use the fit_transform method to transform the documents into a TF-IDF matrix\n",
        "tfidf = vectorizer.fit_transform(docs)\n",
        "# Print the vocabulary (features) of the TF-IDF matrix\n",
        "print(vectorizer.get_feature_names_out())\n",
        "# Print the TF-IDF matrix\n",
        "print(tfidf.toarray())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21ryqYvLIuRX"
      },
      "source": [
        "### Word Embidding :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY_0yZMTIuRY"
      },
      "source": [
        "#### Word2Vec :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxKTBzFdIuRY",
        "outputId": "5498f27a-e873-4e54-c72e-03cfdc0ffe22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector representation of 'NLP': [-8.2426779e-03  9.2993546e-03 -1.9766092e-04 -1.9672764e-03\n",
            "  4.6036304e-03 -4.0953159e-03  2.7431143e-03  6.9399667e-03\n",
            "  6.0654259e-03 -7.5107943e-03  9.3823504e-03  4.6718083e-03\n",
            "  3.9661205e-03 -6.2435055e-03  8.4599797e-03 -2.1501649e-03\n",
            "  8.8251876e-03 -5.3620026e-03 -8.1294188e-03  6.8245591e-03\n",
            "  1.6711927e-03 -2.1985089e-03  9.5136007e-03  9.4938548e-03\n",
            " -9.7740470e-03  2.5052286e-03  6.1566923e-03  3.8724565e-03\n",
            "  2.0227872e-03  4.3050171e-04  6.7363144e-04 -3.8206363e-03\n",
            " -7.1402504e-03 -2.0888723e-03  3.9238976e-03  8.8186832e-03\n",
            "  9.2591504e-03 -5.9759365e-03 -9.4026709e-03  9.7643770e-03\n",
            "  3.4297847e-03  5.1661171e-03  6.2823449e-03 -2.8042626e-03\n",
            "  7.3227035e-03  2.8302716e-03  2.8710044e-03 -2.3803699e-03\n",
            " -3.1282497e-03 -2.3701417e-03  4.2764368e-03  7.6057913e-05\n",
            " -9.5842788e-03 -9.6655441e-03 -6.1481940e-03 -1.2856961e-04\n",
            "  1.9974159e-03  9.4319675e-03  5.5843508e-03 -4.2906962e-03\n",
            "  2.7831673e-04  4.9643586e-03  7.6983096e-03 -1.1442233e-03\n",
            "  4.3234206e-03 -5.8143795e-03 -8.0419064e-04  8.1000505e-03\n",
            " -2.3600650e-03 -9.6634552e-03  5.7792603e-03 -3.9298222e-03\n",
            " -1.2228728e-03  9.9805174e-03 -2.2563506e-03 -4.7570644e-03\n",
            " -5.3293873e-03  6.9808899e-03 -5.7088719e-03  2.1136629e-03\n",
            " -5.2556600e-03  6.1207139e-03  4.3573068e-03  2.6063549e-03\n",
            " -1.4910829e-03 -2.7460635e-03  8.9929365e-03  5.2157748e-03\n",
            " -2.1625196e-03 -9.4703101e-03 -7.4260519e-03 -1.0637414e-03\n",
            " -7.9494715e-04 -2.5629092e-03  9.6827205e-03 -4.5852066e-04\n",
            "  5.8737611e-03 -7.4475873e-03 -2.5060738e-03 -5.5498634e-03]\n",
            "The most similar to 'NLP': ('artificial', 0.17846830189228058)\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Load the English tokenizer, tagger, parser, NER, and word vectors\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text corpus\n",
        "corpus = [\n",
        "    \"Natural Language Processing (NLP) is a subfield of artificial intelligence concerned with the interaction between computers and humans through natural language.\",\n",
        "    \"NLP techniques are used to analyze, understand, and generate human language in a meaningful way.\",\n",
        "    \"NLP applications include sentiment analysis, machine translation, and text summarization, among others.\"\n",
        "]\n",
        "\n",
        "# Tokenize the corpus using spaCy\n",
        "tokenized_corpus = [[token.text.lower() for token in nlp(sentence)] for sentence in corpus]\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Get word vector for a specific word\n",
        "word_vector = model.wv[\"nlp\"]\n",
        "print(\"Vector representation of 'NLP':\", word_vector)\n",
        "\n",
        "# Find similar words\n",
        "similar_words = model.wv.most_similar(\"nlp\")\n",
        "print(\"The most similar to 'NLP':\", similar_words[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7nmrUaqIuRZ"
      },
      "source": [
        "#### FastText :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2S_m2emIuRa",
        "outputId": "aa0bc604-b683-454f-f71d-5f902fefedb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector representation of 'NLP': [ 1.1960485e-03 -2.9092235e-04  1.9134219e-03  8.3163154e-04\n",
            " -3.0929055e-05  1.7030224e-03  7.7937357e-04 -3.1467727e-05\n",
            " -7.4096117e-04 -7.5729890e-04 -1.3574185e-03  7.0388489e-03\n",
            "  3.7478858e-03 -3.1448994e-03 -1.5703861e-04  9.5289381e-04\n",
            " -2.5773861e-03 -1.3878810e-03  4.0132743e-03 -2.9223438e-03\n",
            " -4.2920368e-05  2.5927429e-03  3.1289794e-03  2.5748101e-03\n",
            " -1.0549383e-03  4.0679947e-03  4.9459550e-04 -1.5208727e-03\n",
            " -1.2969562e-03  1.9869744e-03 -2.6628955e-03  1.2238234e-03\n",
            "  6.7922048e-04 -3.9998475e-03  2.0863060e-03  3.9487863e-03\n",
            "  1.0499651e-04 -2.3633079e-03 -4.6515991e-03 -2.0481956e-04\n",
            " -1.2271138e-04  2.9759947e-04  2.5361029e-03  1.8999064e-05\n",
            "  3.4250200e-03 -3.1439296e-03  4.6821169e-05  9.5083343e-04\n",
            " -2.3060869e-03 -5.0475413e-04 -1.6812599e-03  3.3164748e-03\n",
            "  2.0913444e-03 -2.6730620e-03 -1.1126806e-03 -3.9982670e-03\n",
            "  3.5831039e-05  1.0785143e-03  8.6692470e-04  2.3685240e-03\n",
            " -1.8620813e-03  2.4699348e-03 -5.8734924e-05 -2.6510321e-03\n",
            "  4.6091168e-03  2.2406667e-03 -3.1925520e-04  2.4266567e-03\n",
            " -6.0954940e-04 -3.4017104e-03  2.5063264e-03  1.3833935e-04\n",
            " -9.1372337e-04 -1.9689968e-03 -1.2137487e-03  4.4851177e-03\n",
            " -3.0141049e-03  3.5602527e-03  9.4035454e-04 -1.1246542e-03\n",
            "  2.3715927e-03 -8.7879237e-04 -1.3904960e-04 -9.8470564e-04\n",
            " -7.9615216e-04  3.6090147e-03  4.6635866e-03 -2.2247785e-03\n",
            "  1.3348857e-03 -4.3150317e-04 -2.7892622e-03  1.0582549e-03\n",
            " -1.9816316e-03  1.4910152e-03  4.8703547e-03  6.5138255e-04\n",
            " -7.8690378e-04 -3.6457442e-03  7.7514950e-04 -2.3549462e-03]\n",
            "The most similar to 'NLP': [('in', 0.21139265596866608), ('to', 0.1589159220457077), ('between', 0.1546584963798523)]\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from gensim.models import FastText\n",
        "\n",
        "model = FastText(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
        "# Get word vector for a specific word\n",
        "word_vector = model.wv[\"nlp\"]\n",
        "print(\"Vector representation of 'NLP':\", word_vector)\n",
        "# Find similar words\n",
        "similar_words = model.wv.most_similar(\"nlp\")\n",
        "print(\"The most similar to 'NLP':\", similar_words[0:3])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NqQOfYlIuRa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u196kGPIuRb"
      },
      "source": [
        "#### GPT :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3dp7uEOIuRb"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = \"YOUR_API_KEY\"\n",
        "text = \"Natural Language Processing (NLP) is a subfield of artificial intelligence concerned with the interaction between computers and humans through natural language.\"\n",
        "response = openai.Completion.create(engine=\"text-embedding-ada-002\", prompt=text, max_tokens=50)\n",
        "embedding = response[\"choices\"][0][\"embedding\"]\n",
        "print(\"Embedding:\", embedding)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NLP with Classification models : Sentiment Analitycs "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\hp\\anaconda3\\envs\\webapp\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: click in c:\\users\\hp\\anaconda3\\envs\\webapp\\lib\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\envs\\webapp\\lib\\site-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\anaconda3\\envs\\webapp\\lib\\site-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\envs\\webapp\\lib\\site-packages (from nltk) (4.66.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\envs\\webapp\\lib\\site-packages (from click->nltk) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Functions and Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to\n",
            "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\twitter_samples.zip.\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# run this cell to import nltk\n",
        "import nltk\n",
        "from os import getcwd\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imported Functions\n",
        "\n",
        "Download the data needed for this assignment. Check out the [documentation for the twitter_samples dataset](http://www.nltk.org/howto/twitter.html).\n",
        "\n",
        "* twitter_samples: if you're running this notebook on your local computer, you will need to download it using:\n",
        "```Python\n",
        "nltk.download('twitter_samples')\n",
        "```\n",
        "\n",
        "* stopwords: if you're running this notebook on your local computer, you will need to download it using:\n",
        "```python\n",
        "nltk.download('stopwords')\n",
        "```\n",
        "\n",
        "#### Create some helper functions that we provided in the utils.py file:\n",
        "* process_tweet: cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.\n",
        "* build_freqs: this counts how often a word in the 'corpus' (the entire set of tweets) was associated with a positive label '1' or a negative label '0', then builds the 'freqs' dictionary, where each key is the (word,label) tuple, and the value is the count of its frequency within the corpus of tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "\n",
        "def process_tweet(tweet):\n",
        "    \"\"\"Process tweet function.\n",
        "    Input:\n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "\n",
        "    \"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks    \n",
        "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and  # remove stopwords\n",
        "                word not in string.punctuation):  # remove punctuation\n",
        "            # tweets_clean.append(word)\n",
        "            stem_word = stemmer.stem(word)  # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "\n",
        "    return tweets_clean\n",
        "\n",
        "\n",
        "def build_freqs(tweets, ys):\n",
        "    \"\"\"Build frequencies.\n",
        "    Input:\n",
        "        tweets: a list of tweets\n",
        "        ys: an m x 1 array with the sentiment label of each tweet\n",
        "            (either 0 or 1)\n",
        "    Output:\n",
        "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
        "        frequency\n",
        "    \"\"\"\n",
        "    # Convert np array to list since zip needs an iterable.\n",
        "    # The squeeze is necessary or the list ends up with one element.\n",
        "    # Also note that this is just a NOP if ys is already a list.\n",
        "    yslist = np.squeeze(ys).tolist()\n",
        "\n",
        "    # Start with an empty dictionary and populate it by looping over all tweets\n",
        "    # and over all processed words in each tweet.\n",
        "    freqs = {}\n",
        "    for y, tweet in zip(yslist, tweets):\n",
        "        for word in process_tweet(tweet):\n",
        "            pair = (word, y)\n",
        "            if pair in freqs:\n",
        "                freqs[pair] += 1\n",
        "            else:\n",
        "                freqs[pair] = 1\n",
        "\n",
        "    return freqs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import twitter_samples "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare the Data\n",
        "* The `twitter_samples` contains subsets of five thousand positive_tweets, five thousand negative_tweets, and the full set of 10,000 tweets.  \n",
        "    * If you used all three datasets, we would introduce duplicates of the positive tweets and negative tweets.  \n",
        "    * You will select just the five thousand positive tweets and five thousand negative tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# select the set of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Train test split: 20% will be in the test set, and 80% in the training set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# split the data into two pieces, one for training and one for testing (validation set) \n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg \n",
        "test_x = test_pos + test_neg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Create the numpy array of positive labels and negative labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# combine positive and negative labels\n",
        "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
        "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_y.shape = (8000, 1)\n",
            "test_y.shape = (2000, 1)\n"
          ]
        }
      ],
      "source": [
        "# Print the shape train and test sets\n",
        "print(\"train_y.shape = \" + str(train_y.shape))\n",
        "print(\"test_y.shape = \" + str(test_y.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Create the frequency dictionary using the imported build_freqs function.  \n",
        "    * We highly recommend that you open utils.py and read the build_freqs function to understand what it is doing.\n",
        "    * To view the file directory, go to the menu and click File->Open.\n",
        "\n",
        "```Python\n",
        "    for y,tweet in zip(ys, tweets):\n",
        "        for word in process_tweet(tweet):\n",
        "            pair = (word, y)\n",
        "            if pair in freqs:\n",
        "                freqs[pair] += 1\n",
        "            else:\n",
        "                freqs[pair] = 1\n",
        "```\n",
        "* Notice how the outer for loop goes through each tweet, and the inner for loop steps through each word in a tweet.\n",
        "* The 'freqs' dictionary is the frequency dictionary that's being built. \n",
        "* The key is the tuple (word, label), such as (\"happy\",1) or (\"happy\",0).  The value stored for each key is the count of how many times the word \"happy\" was associated with a positive label, or how many times \"happy\" was associated with a negative label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type(freqs) = <class 'dict'>\n",
            "len(freqs) = 11427\n"
          ]
        }
      ],
      "source": [
        "# create frequency dictionary\n",
        "freqs = build_freqs(train_x, train_y)\n",
        "\n",
        "# check the output\n",
        "print(\"type(freqs) = \" + str(type(freqs)))\n",
        "print(\"len(freqs) = \" + str(len(freqs.keys())))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Expected output\n",
        "```\n",
        "type(freqs) = <class 'dict'>\n",
        "len(freqs) = 11436\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Process Tweet\n",
        "The given function 'process_tweet' tokenizes the tweet into individual words, removes stop words and applies stemming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is an example of a positive tweet: \n",
            " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "\n",
            "This is an example of the processed version of the tweet: \n",
            " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
          ]
        }
      ],
      "source": [
        "# test the function below\n",
        "print('This is an example of a positive tweet: \\n', train_x[0])\n",
        "print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Expected output\n",
        "```\n",
        "This is an example of a positive tweet: \n",
        " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
        " \n",
        "This is an example of the processes version: \n",
        " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='1'></a>\n",
        "## 1 - Logistic Regression \n",
        "\n",
        "<a name='1-1'></a>\n",
        "### 1.1 - Sigmoid\n",
        "You will learn to use logistic regression for text classification. \n",
        "* The sigmoid function is defined as: \n",
        "\n",
        "$$ h(z) = \\frac{1}{1+\\exp^{-z}} \\tag{1}$$\n",
        "\n",
        "It maps the input 'z' to a value that ranges between 0 and 1, and so it can be treated as a probability. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid(z): \n",
        "    '''\n",
        "    Input:\n",
        "        z: is the input (can be a scalar or an array)\n",
        "    Output:\n",
        "        h: the sigmoid of z\n",
        "    '''\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # calculate the sigmoid of z\n",
        "    h = 1/(1+np.exp(-z))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SUCCESS!\n",
            "CORRECT!\n"
          ]
        }
      ],
      "source": [
        "# Testing your function \n",
        "if (sigmoid(0) == 0.5):\n",
        "    print('SUCCESS!')\n",
        "else:\n",
        "    print('Oops!')\n",
        "\n",
        "if (sigmoid(4.92) == 0.9927537604041685):\n",
        "    print('CORRECT!')\n",
        "else:\n",
        "    print('Oops again!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 - Cost function and Gradient\n",
        "\n",
        "The cost function used for logistic regression is the average of the log loss across all training examples:\n",
        "\n",
        "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)}))\\tag{5} $$\n",
        "* $m$ is the number of training examples\n",
        "* $y^{(i)}$ is the actual label of training example 'i'.\n",
        "* $h(z^{(i)})$ is the model's prediction for the training example 'i'.\n",
        "\n",
        "The loss function for a single training example is\n",
        "$$ Loss = -1 \\times \\left( y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) \\right)$$\n",
        "\n",
        "* All the $h$ values are between 0 and 1, so the logs will be negative. That is the reason for the factor of -1 applied to the sum of the two loss terms.\n",
        "* Note that when the model predicts 1 ($h(z(\\theta)) = 1$) and the label 'y' is also 1, the loss for that training example is 0. \n",
        "* Similarly, when the model predicts 0 ($h(z(\\theta)) = 0$) and the actual label is also 0, the loss for that training example is 0. \n",
        "* However, when the model prediction is close to 1 ($h(z(\\theta)) = 0.9999$) and the label is 0, the second term of the log loss becomes a large negative number, which is then multiplied by the overall factor of -1 to convert it to a positive loss value. $-1 \\times (1 - 0) \\times log(1 - 0.9999) \\approx 9.2$ The closer the model prediction gets to 1, the larger the loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Likewise, if the model predicts close to 0 ($h(z) = 0.0001$) but the actual label is 1, the first term in the loss function becomes a large number: $-1 \\times log(0.0001) \\approx 9.2$.  The closer the prediction is to zero, the larger the loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Update the weights\n",
        "\n",
        "To update your weight vector $\\theta$, you will apply gradient descent to iteratively improve your model's predictions.  \n",
        "The gradient of the cost function $J$ with respect to one of the weights $\\theta_j$ is:\n",
        "\n",
        "$$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x^{(i)}_j \\tag{5}$$\n",
        "* 'i' is the index across all 'm' training examples.\n",
        "* 'j' is the index of the weight $\\theta_j$, so $x^{(i)}_j$ is the feature associated with weight $\\theta_j$\n",
        "\n",
        "* To update the weight $\\theta_j$, we adjust it by subtracting a fraction of the gradient determined by $\\alpha$:\n",
        "$$\\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta) $$\n",
        "* The learning rate $\\alpha$ is a value that we choose to control how big a single update will be.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Implement gradient descent function.**\n",
        "* The number of iterations 'num_iters\" is the number of times that you'll use the entire training set.\n",
        "* For each iteration, you'll calculate the cost function using all training examples (there are 'm' training examples), and for all features.\n",
        "* Instead of updating a single weight $\\theta_i$ at a time, we can update all the weights in the column vector:  \n",
        "$$\\mathbf{\\theta} = \\begin{pmatrix}\n",
        "\\theta_0\n",
        "\\\\\n",
        "\\theta_1\n",
        "\\\\ \n",
        "\\theta_2 \n",
        "\\\\ \n",
        "\\vdots\n",
        "\\\\ \n",
        "\\theta_n\n",
        "\\end{pmatrix}$$\n",
        "* $\\mathbf{\\theta}$ has dimensions (n+1, 1), where 'n' is the number of features, and there is one more element for the bias term $\\theta_0$ (note that the corresponding feature value $\\mathbf{x_0}$ is 1).\n",
        "* The 'logits', 'z', are calculated by multiplying the feature matrix 'x' with the weight vector 'theta'.  $z = \\mathbf{x}\\mathbf{\\theta}$\n",
        "    * $\\mathbf{x}$ has dimensions (m, n+1) \n",
        "    * $\\mathbf{\\theta}$: has dimensions (n+1, 1)\n",
        "    * $\\mathbf{z}$: has dimensions (m, 1)\n",
        "* The prediction 'h', is calculated by applying the sigmoid to each element in 'z': $h(z) = sigmoid(z)$, and has dimensions (m,1).\n",
        "* The cost function $J$ is calculated by taking the dot product of the vectors 'y' and 'log(h)'.  Since both 'y' and 'h' are column vectors (m,1), transpose the vector to the left, so that matrix multiplication of a row vector with column vector performs the dot product.\n",
        "$$J = \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)$$\n",
        "* The update of theta is also vectorized.  Because the dimensions of $\\mathbf{x}$ are (m, n+1), and both $\\mathbf{h}$ and $\\mathbf{y}$ are (m, 1), we need to transpose the $\\mathbf{x}$ and place it on the left in order to perform matrix multiplication, which then yields the (n+1, 1) answer we need:\n",
        "$$\\mathbf{\\theta} = \\mathbf{\\theta} - \\frac{\\alpha}{m} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "    '''\n",
        "    Input:\n",
        "        x: matrix of features which is (m,n+1)\n",
        "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
        "        theta: weight vector of dimension (n+1,1)\n",
        "        alpha: learning rate\n",
        "        num_iters: number of iterations you want to train your model for\n",
        "    Output:\n",
        "        J: the final cost\n",
        "        theta: your final weight vector\n",
        "    '''\n",
        "    # get 'm', the number of rows in matrix x\n",
        "    m = len(y)\n",
        "    \n",
        "    for i in range(0, num_iters):\n",
        "        # get z, the dot product of x and theta\n",
        "        z = np.dot(x, theta)\n",
        "        \n",
        "        # get the sigmoid of z\n",
        "        h = 1 / (1 + np.exp(-z))\n",
        "        \n",
        "        # calculate the cost function\n",
        "        J = (-1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
        "\n",
        "        # update the weights theta\n",
        "        theta = theta - (alpha / m) * np.dot(x.T, (h - y))\n",
        "        \n",
        "    return J, theta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The cost after training is 0.67094970.\n",
            "The resulting vector of weights is [4.1e-07, 0.00035658, 7.309e-05]\n"
          ]
        }
      ],
      "source": [
        "# Check the function\n",
        "# Construct a synthetic test case using numpy PRNG functions\n",
        "np.random.seed(1)\n",
        "# X input is 10 x 3 with ones for the bias terms\n",
        "tmp_X = np.append(np.ones((10, 1)), np.random.rand(10, 2) * 2000, axis=1)\n",
        "# Y Labels are 10 x 1\n",
        "tmp_Y = (np.random.rand(10, 1) > 0.35).astype(float)\n",
        "\n",
        "# Apply gradient descent\n",
        "tmp_J, tmp_theta = gradientDescent(tmp_X, tmp_Y, np.zeros((3, 1)), 1e-8, 700)\n",
        "print(f\"The cost after training is {tmp_J:.8f}.\")\n",
        "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(tmp_theta)]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Expected output\n",
        "```\n",
        "The cost after training is 0.67094970.\n",
        "The resulting vector of weights is [4.1e-07, 0.00035658, 7.309e-05]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Extracting the Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "* Given a list of tweets, extract the features and store them in a matrix. You will extract two features.\n",
        "    * The first feature is the number of positive words in a tweet.\n",
        "    * The second feature is the number of negative words in a tweet. \n",
        "* Then train your logistic regression classifier on these features.\n",
        "* Test the classifier on a validation set. \n",
        "\n",
        "Implement the extract_features function. \n",
        "* This function takes in a single tweet.\n",
        "* Process the tweet using the imported `process_tweet` function and save the list of tweet words.\n",
        "* Loop through each word in the list of processed words\n",
        "    * For each word, check the 'freqs' dictionary for the count when that word has a positive '1' label. (Check for the key (word, 1.0)\n",
        "    * Do the same for the count for when the word is associated with the negative label '0'. (Check for the key (word, 0.0).)\n",
        "\n",
        "**Note:** In the implementation instructions provided above, the prediction of being positive or negative depends on feature vector which counts-in duplicate words - this is different from what you have seen in the lecture videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_features(tweet, freqs, process_tweet=process_tweet):\n",
        "    '''\n",
        "    Input: \n",
        "        tweet: a string containing one tweet\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "    Output: \n",
        "        x: a feature vector of dimension (1,3)\n",
        "    '''\n",
        "    # process_tweet tokenizes, stems, and removes stopwords\n",
        "    word_l = process_tweet(tweet)\n",
        "    \n",
        "    # 3 elements for [bias, positive, negative] counts\n",
        "    x = np.zeros(3) \n",
        "    \n",
        "    # bias term is set to 1\n",
        "    x[0] = 1 \n",
        "    \n",
        "    # loop through each word in the list of words\n",
        "    for word in word_l:\n",
        "        # increment the word count for the positive label 1\n",
        "        x[1] += freqs.get((word, 1), 0)\n",
        "        \n",
        "        # increment the word count for the negative label 0\n",
        "        x[2] += freqs.get((word, 0), 0)\n",
        "    \n",
        "    x = x[None, :]  # adding batch dimension for further processing\n",
        "    assert(x.shape == (1, 3))\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.000e+00 3.133e+03 6.100e+01]]\n"
          ]
        }
      ],
      "source": [
        "# Check your function\n",
        "# test 1\n",
        "# test on training data\n",
        "tmp1 = extract_features(train_x[0], freqs)\n",
        "print(tmp1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Expected output\n",
        "```\n",
        "[[1.000e+00 3.133e+03 6.100e+01]]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "# test 2:\n",
        "# check for when the words are not in the freqs dictionary\n",
        "tmp2 = extract_features('blorb bleeeeb bloooob', freqs)\n",
        "print(tmp2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Expected output\n",
        "```\n",
        "[[1. 0. 0.]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='3'></a>\n",
        "## 3 - Training Your Model\n",
        "\n",
        "To train the model:\n",
        "* Stack the features for all training examples into a matrix X. \n",
        "* Call `gradientDescent`, which you've implemented above.\n",
        "\n",
        "This section is given to you.  Please read it for understanding and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The cost after training is 0.22521264.\n",
            "The resulting vector of weights is [6e-08, 0.0005382, -0.0005583]\n"
          ]
        }
      ],
      "source": [
        "# collect the features 'x' and stack them into a matrix 'X'\n",
        "X = np.zeros((len(train_x), 3))\n",
        "for i in range(len(train_x)):\n",
        "    X[i, :]= extract_features(train_x[i], freqs)\n",
        "\n",
        "# training labels corresponding to X\n",
        "Y = train_y\n",
        "\n",
        "# Apply gradient descent\n",
        "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
        "print(f\"The cost after training is {J:.8f}.\")\n",
        "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected Output**: \n",
        "\n",
        "```\n",
        "The cost after training is 0.22522315.\n",
        "The resulting vector of weights is [6e-08, 0.00053818, -0.0005583]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='4'></a>\n",
        "## 4 -  Test your Logistic Regression\n",
        "\n",
        "It is time for you to test your logistic regression function on some new input that your model has not seen before. \n",
        "<a name='ex-4'></a>\n",
        "### predict_tweet\n",
        "Implement `predict_tweet`.\n",
        "Predict whether a tweet is positive or negative.\n",
        "\n",
        "* Given a tweet, process it, then extract the features.\n",
        "* Apply the model's learned weights on the features to get the logits.\n",
        "* Apply the sigmoid to the logits to get the prediction (a value between 0 and 1).\n",
        "\n",
        "$$y_{pred} = sigmoid(\\mathbf{x} \\cdot \\theta)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_tweet(tweet, freqs, theta):\n",
        "    '''\n",
        "    Input: \n",
        "        tweet: a string\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "        theta: (3,1) vector of weights\n",
        "    Output: \n",
        "        y_pred: the probability of a tweet being positive or negative\n",
        "    '''\n",
        "    # extract the features of the tweet and store it into x\n",
        "    x = extract_features(tweet, freqs)\n",
        "    \n",
        "    # make the prediction using x and theta\n",
        "    z = np.dot(x, theta)\n",
        "    y_pred = sigmoid(z)\n",
        "    \n",
        "    return y_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am happy -> 0.519275\n",
            "I am bad -> 0.494347\n",
            "this movie should have been great. -> 0.515980\n",
            "great -> 0.516065\n",
            "great great -> 0.532097\n",
            "great great great -> 0.548063\n",
            "great great great great -> 0.563930\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_12080\\1003351.py:3: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print( '%s -> %f' % (tweet, predict_tweet(tweet, freqs, theta)))\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to test your function\n",
        "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
        "    print( '%s -> %f' % (tweet, predict_tweet(tweet, freqs, theta)))    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.83110764]])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Feel free to check the sentiment of your own tweet below\n",
        "my_tweet = 'I am learning :)'\n",
        "predict_tweet(my_tweet, freqs, theta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='4-1'></a>\n",
        "### 4.1 -  Check the Performance using the Test Set\n",
        "After training your model using the training set above, check how your model might perform on real, unseen data, by testing it against the test set.\n",
        "\n",
        "<a name='ex-5'></a>\n",
        "### test_logistic_regression\n",
        "Implement `test_logistic_regression`. \n",
        "* Given the test data and the weights of your trained model, calculate the accuracy of your logistic regression model. \n",
        "* Use your 'predict_tweet' function to make predictions on each tweet in the test set.\n",
        "* If the prediction is > 0.5, set the model's classification 'y_hat' to 1, otherwise set the model's classification 'y_hat' to 0.\n",
        "* A prediction is accurate when the y_hat equals the test_y.  Sum up all the instances when they are equal and divide by m.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_logistic_regression(test_x, test_y, freqs, theta, predict_tweet=predict_tweet):\n",
        "    \"\"\"\n",
        "    Input: \n",
        "        test_x: a list of tweets\n",
        "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
        "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
        "        theta: weight vector of dimension (3, 1)\n",
        "    Output: \n",
        "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
        "    \"\"\"\n",
        "    \n",
        "    # the list for storing predictions\n",
        "    y_hat = []\n",
        "    \n",
        "    for tweet in test_x:\n",
        "        # get the label prediction for the tweet\n",
        "        y_pred = predict_tweet(tweet, freqs, theta)\n",
        "        \n",
        "        if y_pred > 0.5:\n",
        "            # append 1.0 to the list\n",
        "            y_hat.append(1.0)\n",
        "        else:\n",
        "            # append 0 to the list\n",
        "            y_hat.append(0.0)\n",
        "\n",
        "    # Convert both to one-dimensional arrays in order to compare them using the '==' operator\n",
        "    y_hat = np.array(y_hat)\n",
        "    test_y = np.squeeze(test_y)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = np.mean((y_hat == test_y).astype(int))\n",
        "    \n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic regression model's accuracy = 0.9950\n"
          ]
        }
      ],
      "source": [
        "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
        "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5 - Predict with your own Tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ridicul', 'bright', 'movi', 'plot', 'terribl', 'sad', 'end']\n",
            "[[0.48125421]]\n",
            "Negative sentiment\n"
          ]
        }
      ],
      "source": [
        "# Feel free to change the tweet below\n",
        "my_tweet = 'This is a ridiculously bright movie. The plot was terrible and I was sad until the ending!'\n",
        "print(process_tweet(my_tweet))\n",
        "y_hat = predict_tweet(my_tweet, freqs, theta)\n",
        "print(y_hat)\n",
        "if y_hat > 0.5:\n",
        "    print('Positive sentiment')\n",
        "else: \n",
        "    print('Negative sentiment')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NLP with Classification models :Translation Machine "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to\n",
            "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pdb\n",
        "import pickle\n",
        "import string\n",
        "import time\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "from utils import (cosine_similarity, get_dict_0,\n",
        "                   process_tweet)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data :\n",
        "- we load the english and french embedding data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "en_embeddings_subset = pickle.load(open(r\"data\\NLP with Classification models Translation Machine\\en_embeddings.p\", \"rb\"))\n",
        "fr_embeddings_subset = pickle.load(open(r\"data\\NLP with Classification models Translation Machine\\fr_embeddings.p\", \"rb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Look at the data**\n",
        "\n",
        "* en_embeddings_subset: the key is an English word, and the value is a\n",
        "300 dimensional array, which is the embedding for that word.\n",
        "```\n",
        "'the': array([ 0.08007812,  0.10498047,  0.04980469,  0.0534668 , -0.06738281, ....\n",
        "```\n",
        "\n",
        "* fr_embeddings_subset: the key is a French word, and the value is a 300\n",
        "dimensional array, which is the embedding for that word.\n",
        "```\n",
        "'la': array([-6.18250e-03, -9.43867e-04, -8.82648e-03,  3.24623e-02,...\n",
        "```\n",
        "\n",
        "- **Load two dictionaries mapping the English to French words**\n",
        "* A training dictionary\n",
        "* and a testing dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The length of the English to French training dictionary is 5000\n",
            "The length of the English to French test dictionary is 1500\n"
          ]
        }
      ],
      "source": [
        "# loading the english to french dictionaries\n",
        "en_fr_train = get_dict_0(r'data\\NLP with Classification models Translation Machine\\en-fr.train.txt')\n",
        "print('The length of the English to French training dictionary is', len(en_fr_train))\n",
        "en_fr_test = get_dict_0(r'data\\NLP with Classification models Translation Machine\\en-fr.test.txt')\n",
        "print('The length of the English to French test dictionary is', len(en_fr_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Looking at the English French dictionary**\n",
        "\n",
        "* `en_fr_train` is a dictionary where the key is the English word and the value\n",
        "is the French translation of that English word.\n",
        "```\n",
        "{'the': 'la',\n",
        " 'and': 'et',\n",
        " 'was': 'était',\n",
        " 'for': 'pour',\n",
        "```\n",
        "\n",
        "* `en_fr_test` is similar to `en_fr_train`, but is a test set.  We won't look at it\n",
        "until we get to testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformation matrix R:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**get_matrices**\n",
        "\n",
        "Translating English dictionary to French by using embeddings.\n",
        "\n",
        "You will now implement a function `get_matrices`, which takes the loaded data\n",
        "and returns matrices `X` and `Y`.\n",
        "\n",
        "Inputs:\n",
        "- `en_fr` : English to French dictionary\n",
        "- `en_embeddings` : English to embeddings dictionary\n",
        "- `fr_embeddings` : French to embeddings dictionary\n",
        "\n",
        "Returns:\n",
        "- Matrix `X` and matrix `Y`, where each row in X is the word embedding for an\n",
        "english word, and the same row in Y is the word embedding for the French\n",
        "version of that English word.\n",
        "\n",
        "- The `en_fr` dictionary to ensure that the ith row in the `X` matrix\n",
        "corresponds to the ith row in the `Y` matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_matrices(en_fr, french_vecs, english_vecs):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        en_fr: English to French dictionary\n",
        "        french_vecs: French words to their corresponding word embeddings.\n",
        "        english_vecs: English words to their corresponding word embeddings.\n",
        "    Output: \n",
        "        X: a matrix where the columns are the English embeddings.\n",
        "        Y: a matrix where the columns correspong to the French embeddings.\n",
        "        R: the projection matrix that minimizes the F norm ||X R -Y||^2.\n",
        "    \"\"\"\n",
        "\n",
        "    # X_l and Y_l are lists of the english and french word embeddings\n",
        "    X_l = list()\n",
        "    Y_l = list()\n",
        "\n",
        "    # get the english words (the keys in the dictionary) and store in a set()\n",
        "    english_set = english_vecs.keys()\n",
        "\n",
        "    # get the french words (keys in the dictionary) and store in a set()\n",
        "    french_set =  french_vecs.keys()\n",
        "\n",
        "    # store the french words that are part of the english-french dictionary (these are the values of the dictionary)\n",
        "    french_words = set(en_fr.values())\n",
        "\n",
        "    # loop through all english, french word pairs in the english french dictionary\n",
        "    for en_word, fr_word in en_fr.items():\n",
        "\n",
        "        # check that the french word has an embedding and that the english word has an embedding\n",
        "        if fr_word in french_set and en_word in english_set:\n",
        "\n",
        "            # get the english embedding\n",
        "            en_vec = english_vecs[en_word]\n",
        "\n",
        "            # get the french embedding\n",
        "            fr_vec = french_vecs[fr_word]\n",
        "\n",
        "            # add the english embedding to the list\n",
        "            X_l.append(en_vec)\n",
        "\n",
        "            # add the french embedding to the list\n",
        "            Y_l.append(fr_vec)\n",
        "\n",
        "    # stack the vectors of X_l into a matrix X\n",
        "    X = np.array(X_l)\n",
        "\n",
        "    # stack the vectors of Y_l into a matrix Y\n",
        "    Y = np.array(Y_l)\n",
        "\n",
        "    return X, Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# getting the training set:\n",
        "X_train, Y_train = get_matrices(\n",
        "    en_fr_train, fr_embeddings_subset, en_embeddings_subset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given dictionaries of English and French word embeddings you will create a transformation matrix `R`\n",
        "* Given an English word embedding, $\\mathbf{e}$, you can multiply $\\mathbf{eR}$ to get a new word embedding $\\mathbf{f}$.\n",
        "    * Both $\\mathbf{e}$ and $\\mathbf{f}$ are [row vectors](https://en.wikipedia.org/wiki/Row_and_column_vectors).\n",
        "* You can then compute the nearest neighbors to `f` in the french embeddings and recommend the word that is most similar to the transformed word embedding.\n",
        "\n",
        "Find a matrix `R` that minimizes the following equation. \n",
        "\n",
        "$$\\arg \\min _{\\mathbf{R}}\\| \\mathbf{X R} - \\mathbf{Y}\\|_{F}\\tag{1} $$\n",
        "\n",
        "**Frobenius norm**\n",
        "\n",
        "The Frobenius norm of a matrix $A$ (assuming it is of dimension $m,n$) is defined as the square root of the sum of the absolute squares of its elements:\n",
        "\n",
        "$$\\|\\mathbf{A}\\|_{F} \\equiv \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n}\\left|a_{i j}\\right|^{2}}\\tag{2}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Actual loss function**\n",
        "In the real world applications, the Frobenius norm loss:\n",
        "\n",
        "$$\\| \\mathbf{XR} - \\mathbf{Y}\\|_{F}$$\n",
        "\n",
        "is often replaced by it's squared value divided by $m$:\n",
        "\n",
        "$$ \\frac{1}{m} \\|  \\mathbf{X R} - \\mathbf{Y} \\|_{F}^{2}$$\n",
        "\n",
        "where $m$ is the number of examples (rows in $\\mathbf{X}$).\n",
        "\n",
        "* The same R is found when using this loss function versus the original Frobenius norm.\n",
        "* The reason for taking the square is that it's easier to compute the gradient of the squared Frobenius.\n",
        "* The reason for dividing by $m$ is that we're more interested in the average loss per embedding than the  loss for the entire training set.\n",
        "    * The loss for all training set increases with more words (training examples),\n",
        "    so taking the average helps us to track the average loss regardless of the size of the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Computing the loss**\n",
        "* The loss function will be squared Frobenius norm of the difference between\n",
        "matrix and its approximation, divided by the number of training examples $m$.\n",
        "* Its formula is:\n",
        "$$ L(X, Y, R)=\\frac{1}{m}\\sum_{i=1}^{m} \\sum_{j=1}^{n}\\left( a_{i j} \\right)^{2}$$\n",
        "\n",
        "where $a_{i j}$ is value in $i$th row and $j$th column of the matrix $\\mathbf{XR}-\\mathbf{Y}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_loss(X, Y, R):\n",
        "    '''\n",
        "    Inputs: \n",
        "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
        "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
        "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
        "    Outputs:\n",
        "        L: a matrix of dimension (m,n) - the value of the loss function for given X, Y and R.\n",
        "    '''\n",
        "    # m is the number of rows in X\n",
        "    m = X.shape[0]\n",
        "        \n",
        "    # diff is XR - Y    \n",
        "    diff = np.dot(X,R)-Y\n",
        "\n",
        "    # diff_squared is the element-wise square of the difference    \n",
        "    diff_squared = diff**2\n",
        "\n",
        "    # sum_diff_squared is the sum of the squared elements\n",
        "    sum_diff_squared = np.sum(diff_squared)\n",
        "\n",
        "    # loss i is the sum_diff_squared divided by the number of examples (m)\n",
        "    loss = sum_diff_squared/m\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Computing the gradient of loss with respect to transform matrix R**\n",
        "\n",
        "* Calculate the gradient of the loss with respect to transform matrix `R`.\n",
        "* The gradient is a matrix that encodes how much a small change in `R`\n",
        "affect the change in the loss function.\n",
        "* The gradient gives us the direction in which we should decrease `R`\n",
        "to minimize the loss.\n",
        "* $m$ is the number of training examples (number of rows in $X$).\n",
        "* The formula for the gradient of the loss function $𝐿(𝑋,𝑌,𝑅)$ is:\n",
        "\n",
        "$$\\frac{d}{dR}𝐿(𝑋,𝑌,𝑅)=\\frac{d}{dR}\\Big(\\frac{1}{m}\\| X R -Y\\|_{F}^{2}\\Big) = \\frac{2}{m}X^{T} (X R - Y)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_gradient(X, Y, R):\n",
        "    '''\n",
        "    Inputs: \n",
        "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
        "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
        "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
        "    Outputs:\n",
        "        g: a scalar value - gradient of the loss function L for given X, Y and R.\n",
        "    '''\n",
        "    # m is the number of rows in X\n",
        "    m = X.shape[0]\n",
        "    # gradient is X^T(XR - Y) * 2/m    \n",
        "    gradient = np.dot(X.T,(np.dot(X,R)-Y))*(2/m)\n",
        "    return gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Calculate gradient $g$ of the loss with respect to the matrix $R$.\n",
        "2. Update $R$ with the formula:\n",
        "$$R_{\\text{new}}= R_{\\text{old}}-\\alpha g$$\n",
        "\n",
        "Where $\\alpha$ is the learning rate, which is a scalar.\n",
        "\n",
        "**Learning Rate**\n",
        "\n",
        "* The learning rate or \"step size\" $\\alpha$ is a coefficient which decides how much we want to change $R$ in each step.\n",
        "* If we change $R$ too much, we could skip the optimum by taking too large of a step.\n",
        "* If we make only small changes to $R$, we will need many steps to reach the optimum.\n",
        "* Learning rate $\\alpha$ is used to control those changes.\n",
        "* Values of $\\alpha$ are chosen depending on the problem, and we'll use `learning_rate`$=0.0003$ as the default value for our algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def align_embeddings(X, Y, train_steps=100, learning_rate=0.0003, verbose=True, compute_loss=compute_loss, compute_gradient=compute_gradient):\n",
        "    '''\n",
        "    Inputs:\n",
        "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
        "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
        "        train_steps: positive int - describes how many steps will gradient descent algorithm do.\n",
        "        learning_rate: positive float - describes how big steps will  gradient descent algorithm do.\n",
        "    Outputs:\n",
        "        R: a matrix of dimension (n,n) - the projection matrix that minimizes the F norm ||X R -Y||^2\n",
        "    '''\n",
        "    np.random.seed(129)\n",
        "\n",
        "    # the number of columns in X is the number of dimensions for a word vector (e.g. 300)\n",
        "    # R is a square matrix with length equal to the number of dimensions in th  word embedding\n",
        "    R = np.random.rand(X.shape[1], X.shape[1])\n",
        "\n",
        "    for i in range(train_steps):\n",
        "        if verbose and i % 25 == 0:\n",
        "            print(f\"loss at iteration {i} is: {compute_loss(X, Y, R):.4f}\")\n",
        "        # use the function that you defined to compute the gradient\n",
        "        gradient = compute_gradient(X, Y, R)\n",
        "\n",
        "        # update R by subtracting the learning rate times gradient\n",
        "        R -= learning_rate*gradient\n",
        "    return R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss at iteration 0 is: 963.0146\n",
            "loss at iteration 25 is: 97.8292\n",
            "loss at iteration 50 is: 26.8329\n",
            "loss at iteration 75 is: 9.7893\n"
          ]
        }
      ],
      "source": [
        "R_train = align_embeddings(X_train, Y_train, train_steps=400, learning_rate=0.8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the translation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### k-Nearest Neighbors Algorithm\n",
        "\n",
        "[k-Nearest neighbors algorithm](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) \n",
        "* k-NN is a method which takes a vector as input and finds the other vectors in the dataset that are closest to it. \n",
        "* The 'k' is the number of \"nearest neighbors\" to find (e.g. k=2 finds the closest two neighbors).\n",
        "\n",
        "#### Searching for the Translation Embedding\n",
        "Since we're approximating the translation function from English to French embeddings by a linear transformation matrix $\\mathbf{R}$, most of the time we won't get the exact embedding of a French word when we transform embedding $\\mathbf{e}$ of some particular English word into the French embedding space. \n",
        "* This is where $k$-NN becomes really useful! By using $1$-NN with $\\mathbf{eR}$ as input, we can search for an embedding $\\mathbf{f}$ (as a row) in the matrix $\\mathbf{Y}$ which is the closest to the transformed vector $\\mathbf{eR}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Cosine Similarity\n",
        "Cosine similarity between vectors $u$ and $v$ calculated as the cosine of the angle between them.\n",
        "The formula is \n",
        "\n",
        "$$\\cos(u,v)=\\frac{u\\cdot v}{\\left\\|u\\right\\|\\left\\|v\\right\\|}$$\n",
        "\n",
        "* $\\cos(u,v)$ = $1$ when $u$ and $v$ lie on the same line and have the same direction.\n",
        "* $\\cos(u,v)$ is $-1$ when they have exactly opposite directions.\n",
        "* $\\cos(u,v)$ is $0$ when the vectors are orthogonal (perpendicular) to each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Note: Distance and similarity are pretty much opposite things.\n",
        "* We can obtain distance metric from cosine similarity, but the cosine similarity can't be used directly as the distance metric. \n",
        "* When the cosine similarity increases (towards $1$), the \"distance\" between the two vectors decreases (towards $0$). \n",
        "* We can define the cosine distance between $u$ and $v$ as\n",
        "$$d_{\\text{cos}}(u,v)=1-\\cos(u,v)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def nearest_neighbor(v, candidates, k=1, cosine_similarity=cosine_similarity):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "      - v, the vector you are going find the nearest neighbor for\n",
        "      - candidates: a set of vectors where we will find the neighbors\n",
        "      - k: top k nearest neighbors to find\n",
        "    Output:\n",
        "      - k_idx: the indices of the top k closest vectors in sorted form\n",
        "    \"\"\"\n",
        "    similarity_l = []\n",
        "\n",
        "    # for each candidate vector...\n",
        "    for row in candidates:\n",
        "        # get the cosine similarity\n",
        "        cos_similarity = cosine_similarity(v,row)\n",
        "\n",
        "        # append the similarity to the list\n",
        "        similarity_l.append(cos_similarity)\n",
        "\n",
        "    # sort the similarity list and get the indices of the sorted list    \n",
        "    sorted_ids =np.argsort(similarity_l)\n",
        "    \n",
        "    # Reverse the order of the sorted_ids array\n",
        "    sorted_ids = sorted_ids[::-1]\n",
        "    \n",
        "    # get the indices of the k most similar candidate vectors\n",
        "    k_idx = sorted_ids[0:k]\n",
        "    return k_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**test_vocabulary**\n",
        "Complete the function `test_vocabulary` which takes in English\n",
        "embedding matrix $X$, French embedding matrix $Y$ and the $R$\n",
        "matrix and returns the accuracy of translations from $X$ to $Y$ by $R$.\n",
        "\n",
        "* Iterate over transformed English word embeddings and check if the\n",
        "closest French word vector belongs to French word that is the actual\n",
        "translation.\n",
        "* Obtain an index of the closest French embedding by using\n",
        "`nearest_neighbor` (with argument `k=1`), and compare it to the index\n",
        "of the English embedding you have just transformed.\n",
        "* Keep track of the number of times you get the correct translation.\n",
        "* Calculate accuracy as $$\\text{accuracy}=\\frac{\\#(\\text{correct predictions})}{\\#(\\text{total predictions})}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_vocabulary(X, Y, R, nearest_neighbor=nearest_neighbor):\n",
        "    '''\n",
        "    Input:\n",
        "        X: a matrix where the columns are the English embeddings.\n",
        "        Y: a matrix where the columns correspong to the French embeddings.\n",
        "        R: the transform matrix which translates word embeddings from\n",
        "        English to French word vector space.\n",
        "    Output:\n",
        "        accuracy: for the English to French capitals\n",
        "    '''\n",
        "    # The prediction is X times R\n",
        "    pred = X @ R\n",
        "\n",
        "    # initialize the number correct to zero\n",
        "    num_correct = 0\n",
        "\n",
        "    # loop through each row in pred (each transformed embedding)\n",
        "    for i in range(len(pred)):\n",
        "        # get the index of the nearest neighbor of pred at row 'i'; also pass in the candidates in Y\n",
        "        pred_idx = nearest_neighbor(pred[i], Y)\n",
        "\n",
        "        # if the index of the nearest neighbor equals the row of i...\n",
        "        if pred_idx == i:\n",
        "            # increment the number correct by 1.\n",
        "            num_correct += 1\n",
        "\n",
        "    # accuracy is the number correct divided by the number of rows in 'pred' (also number of rows in X)\n",
        "    accuracy = num_correct / len(pred)\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy on test set is 0.557\n"
          ]
        }
      ],
      "source": [
        "acc = test_vocabulary(X_val, Y_val, R_train)  # this might take a minute or two\n",
        "print(f\"accuracy on test set is {acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You managed to translate words from one language to another language\n",
        "without ever seing them with almost 56% accuracy by using some basic\n",
        "linear algebra and learning a mapping of words from one language to another!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "def translate(En_Word,R,fr_embeddings_subset,en_embeddings_subset):\n",
        "    En_Word = En_Word.lower()\n",
        "    if En_Word in en_embeddings_subset.keys():\n",
        "        En_embedding = en_embeddings_subset[En_Word]\n",
        "        Fr_embedding = np.dot(En_embedding,R)\n",
        "        fr_matrix = np.vstack(list(fr_embeddings_subset.values()))\n",
        "        Fr_index = nearest_neighbor(Fr_embedding,fr_matrix,k=1)\n",
        "        Fr_word = list(fr_embeddings_subset.keys())[Fr_index[0]]\n",
        "        return Fr_word\n",
        "        #print(f\"French word: {Fr_Word} {fr_embeddings_subset[Fr_Word]}\")\n",
        "    else :\n",
        "        return \"Word not in vocabulary\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "yes : effectivement\n",
            "no : Word not in vocabulary\n",
            "dog : chien\n",
            "chat : conversation\n",
            "lenght : longueurs\n",
            "Man : homme\n"
          ]
        }
      ],
      "source": [
        "print(\"yes :\",translate(\"yes\",R_train,fr_embeddings_subset,en_embeddings_subset))\n",
        "print(\"no :\",translate(\"no\",R_train,fr_embeddings_subset,en_embeddings_subset))\n",
        "print(\"dog :\",translate(\"dog\",R_train,fr_embeddings_subset,en_embeddings_subset))\n",
        "print(\"chat :\",translate(\"chat\",R_train,fr_embeddings_subset,en_embeddings_subset))\n",
        "print(\"lenght :\",translate(\"length\",R_train,fr_embeddings_subset,en_embeddings_subset))\n",
        "print(\"Man :\",translate(\"Man\",R_train,fr_embeddings_subset,en_embeddings_subset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NLP with Probabilistic models : Autocorrect "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Libraries :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As in any other machine learning task, the first thing you have to do is process your data set. \n",
        "- Many courses load in pre-processed data for you. \n",
        "- However, in the real world, when you build these NLP systems, you load the datasets and process them.\n",
        "- So let's get some real world practice in pre-processing the data! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**function `process_data` which** \n",
        "\n",
        "1) Reads in a corpus (text file)\n",
        "\n",
        "2) Changes everything to lowercase\n",
        "\n",
        "3) Returns a list of words. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_data(file_name):\n",
        "    \"\"\"\n",
        "    Input: \n",
        "        A file_name. \n",
        "    Output: \n",
        "        words: a list containing all the words in the corpus (text file you read) in lower case. \n",
        "    \"\"\"\n",
        "    words = []  # return this variable correctly\n",
        "    # Open the file, read its contents into a string variable\n",
        "    with open(file_name, 'r', encoding='utf-8') as file:\n",
        "        data = file.read()\n",
        "\n",
        "    # Convert all letters to lower case\n",
        "    data = data.lower()\n",
        "    # Extract words using regular expression\n",
        "    words = re.findall(r'\\w+', data)\n",
        "\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The first ten words in the text are: \n",
            "['o', 'for', 'a', 'muse', 'of', 'fire', 'that', 'would', 'ascend', 'the']\n",
            "There are 6116 unique words in the vocabulary.\n"
          ]
        }
      ],
      "source": [
        "word_l = process_data(r\"data\\NLP with Probabilistic models  Autocorrect\\shakespeare.txt\")\n",
        "vocab = set(word_l)  # this will be your new vocabulary\n",
        "print(f\"The first ten words in the text are: \\n{word_l[0:10]}\")\n",
        "print(f\"There are {len(vocab)} unique words in the vocabulary.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**`get_count` function that returns a dictionary**\n",
        "- The dictionary's keys are words\n",
        "- The value for each word is the number of times that word appears in the corpus. \n",
        "\n",
        "For example, given the following sentence: **\"I am happy because I am learning\"**, your dictionary should return the following: \n",
        "<table style=\"width:20%\">\n",
        "\n",
        "  <tr>\n",
        "    <td> <b>Key </b>  </td>\n",
        "    <td> <b>Value </b> </td> \n",
        "\n",
        "\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> I  </td>\n",
        "    <td> 2</td> \n",
        " \n",
        "  </tr>\n",
        "   \n",
        "  <tr>\n",
        "    <td>am</td>\n",
        "    <td>2</td> \n",
        "  </tr>\n",
        "\n",
        "  <tr>\n",
        "    <td>happy</td>\n",
        "    <td>1</td> \n",
        "  </tr>\n",
        "  \n",
        "   <tr>\n",
        "    <td>because</td>\n",
        "    <td>1</td> \n",
        "  </tr>\n",
        "  \n",
        "   <tr>\n",
        "    <td>learning</td>\n",
        "    <td>1</td> \n",
        "  </tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_count(word_l):\n",
        "    '''\n",
        "    Input:\n",
        "        word_l: a set of words representing the corpus. \n",
        "    Output:\n",
        "        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n",
        "    '''\n",
        "    word_count_dict = {}  # fill this with word counts\n",
        "    c = 0\n",
        "    for word_i in word_l:\n",
        "        for word_j in word_l:\n",
        "            if word_i == word_j:\n",
        "                c+=1\n",
        "        word_count_dict[word_i] = c\n",
        "        c = 0\n",
        "    return word_count_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "can take around 8 min to finish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 6116 key values pairs\n",
            "The count for the word 'thee' is 240\n"
          ]
        }
      ],
      "source": [
        "word_count_dict = get_count(word_l)\n",
        "print(f\"There are {len(word_count_dict)} key values pairs\")\n",
        "print(f\"The count for the word 'thee' is {word_count_dict.get('thee',0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " **get_probs**\n",
        "- Given the dictionary of word counts, compute the probability that each word will appear if randomly selected from the corpus of words.\n",
        "\n",
        "$$P(w_i) = \\frac{C(w_i)}{M} \\tag{Eqn-2}$$\n",
        "where \n",
        "\n",
        "$C(w_i)$ is the total number of times $w_i$ appears in the corpus.\n",
        "\n",
        "$M$ is the total number of words in the corpus.\n",
        "\n",
        "For example, the probability of the word 'am' in the sentence **'I am happy because I am learning'** is:\n",
        "\n",
        "$$P(am) = \\frac{C(w_i)}{M} = \\frac {2}{7} \\tag{Eqn-3}.$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_probs(word_count_dict):\n",
        "    '''\n",
        "    Input:\n",
        "        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n",
        "    Output:\n",
        "        probs: A dictionary where keys are the words and the values are the probability that a word will occur. \n",
        "    '''\n",
        "    probs = {}  # return this variable correctly\n",
        "    M = 0\n",
        "    for word in word_count_dict:\n",
        "        M += word_count_dict[word] # Sum of frequences\n",
        "    for word in word_count_dict:\n",
        "        probs[word] = word_count_dict[word]/(M)\n",
        "    # get the total count of words for all words in the dictionary\n",
        "    return probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of probs is 6116\n",
            "P('thee') is 0.0045\n"
          ]
        }
      ],
      "source": [
        "probs = get_probs(word_count_dict)\n",
        "print(f\"Length of probs is {len(probs)}\")\n",
        "print(f\"P('thee') is {probs['thee']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## String Manipulations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that you have computed $P(w_i)$ for all the words in the corpus, you will write a few functions to manipulate strings so that you can edit the erroneous strings and return the right spellings of the words. In this section, you will implement four functions: \n",
        "\n",
        "* `delete_letter`: given a word, it returns all the possible strings that have **one character removed**. \n",
        "* `switch_letter`: given a word, it returns all the possible strings that have **two adjacent letters switched**.\n",
        "* `replace_letter`: given a word, it returns all the possible strings that have **one character replaced by another different letter**.\n",
        "* `insert_letter`: given a word, it returns all the possible strings that have an **additional character inserted**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Implement a `delete_letter()` function**\n",
        "- that, given a word, returns a list of strings with one character deleted. \n",
        "\n",
        "For example, given the word **nice**, it would return the set: {'ice', 'nce', 'nic', 'nie'}. \n",
        "\n",
        "**Step 1:** Create a list of 'splits'. This is all the ways you can split a word into Left and Right: For example,   \n",
        "'nice is split into : `[('', 'nice'), ('n', 'ice'), ('ni', 'ce'), ('nic', 'e'), ('nice', '')]`\n",
        "This is common to all four functions (delete, replace, switch, insert).\n",
        "\n",
        "**Step 2:** This is specific to `delete_letter`. Here, we are generating all words that result from deleting one character.  \n",
        "This can be done in a single line with a list comprehension. You can make use of this type of syntax:  \n",
        "`[f(a,b) for a, b in splits if condition]`  \n",
        "\n",
        "For our 'nice' example you get: \n",
        "['ice', 'nce', 'nie', 'nic']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def delete_letter(word, verbose=False):\n",
        "    '''\n",
        "    Input:\n",
        "        word: the string/word for which you will generate all possible words \n",
        "                in the vocabulary which have 1 missing character\n",
        "    Output:\n",
        "        delete_l: a list of all possible strings obtained by deleting 1 character from word\n",
        "    '''\n",
        "    \n",
        "    delete_l = []\n",
        "    split_l = []\n",
        "    \n",
        "    # Step 1: Create a list of 'splits'\n",
        "    for i in range(len(word)):\n",
        "        split = (word[:i], word[i:])\n",
        "        split_l.append(split)\n",
        "    \n",
        "    # Step 2: Generate words with one character deleted\n",
        "    for left, right in split_l:\n",
        "        if right:\n",
        "            delete_l.append(left + right[1:])\n",
        "    \n",
        "    if verbose: \n",
        "        print(f\"input word {word}, \\nsplit_l = {split_l}, \\ndelete_l = {delete_l}\")\n",
        "\n",
        "    return delete_l\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input word cans, \n",
            "split_l = [('', 'cans'), ('c', 'ans'), ('ca', 'ns'), ('can', 's')], \n",
            "delete_l = ['ans', 'cns', 'cas', 'can']\n"
          ]
        }
      ],
      "source": [
        "delete_word_l = delete_letter(word=\"cans\",\n",
        "                        verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**switch_letter()**\n",
        "\n",
        "Now implement a function that switches two letters in a word. It takes in a word and returns a list of all the possible switches of two letters **that are adjacent to each other**. \n",
        "- For example, given the word 'eta', it returns {'eat', 'tea'}, but does not return 'ate'.\n",
        "\n",
        "**Step 1:** is the same as in delete_letter()  \n",
        "**Step 2:** A list comprehension or for loop which forms strings by swapping adjacent letters. This is of the form:  \n",
        "`[f(L,R) for L, R in splits if condition]`  where 'condition' will test the length of R in a given iteration. See below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def switch_letter(word, verbose=False):\n",
        "    '''\n",
        "    Input:\n",
        "        word: input string\n",
        "     Output:\n",
        "        switches: a list of all possible strings with one adjacent charater switched\n",
        "    ''' \n",
        "    \n",
        "    switch_l = []\n",
        "    split_l = list(word)\n",
        "    \n",
        "    for i in range(len(split_l) - 1):\n",
        "        # Swap characters at position i and i+1\n",
        "        split_l[i], split_l[i+1] = split_l[i+1], split_l[i]\n",
        "        # Append the switched word to the switch_l list\n",
        "        switch_l.append(''.join(split_l))\n",
        "        # Swap back to restore the original word for the next iteration\n",
        "        split_l[i], split_l[i+1] = split_l[i+1], split_l[i]\n",
        "    \n",
        "    if verbose: \n",
        "        print(f\"Input word = {word} \\nsplit_l = {split_l} \\nswitch_l = {switch_l}\") \n",
        "    \n",
        "    return switch_l\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input word = eta \n",
            "split_l = ['e', 't', 'a'] \n",
            "switch_l = ['tea', 'eat']\n"
          ]
        }
      ],
      "source": [
        "switch_word_l = switch_letter(word=\"eta\",\n",
        "                         verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**replace_letter**\n",
        "\n",
        "**Instructions for replace_letter()**: Now implement a function that takes in a word and returns a list of strings with one **replaced letter** from the original word. \n",
        "\n",
        "**Step 1:** is the same as in `delete_letter()`\n",
        "\n",
        "**Step 2:** A list comprehension or for loop which form strings by replacing letters.  This can be of the form:  \n",
        "`[f(a,b,c) for a, b in splits if condition for c in string]`   Note the use of the second for loop.  \n",
        "It is expected in this routine that one or more of the replacements will include the original word. For example, replacing the first letter of 'ear' with 'e' will return 'ear'.\n",
        "\n",
        "**Step 3:** Remove the original input letter from the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def replace_letter(word, verbose=False):\n",
        "    '''\n",
        "    Input:\n",
        "        word: the input string/word \n",
        "    Output:\n",
        "        replaces: a list of all possible strings where we replaced one letter from the original word. \n",
        "    ''' \n",
        "    \n",
        "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    \n",
        "    replace_l = []\n",
        "    split_l = list(word)\n",
        "    \n",
        "    for i in range(len(split_l)):\n",
        "        for letter in letters:\n",
        "            # Skip if the letter is the same as the original character\n",
        "            if letter == split_l[i]:\n",
        "                continue\n",
        "            # Replace the character at position i with the current letter\n",
        "            split_l[i] = letter\n",
        "            # Append the replaced word to the replace_l list\n",
        "            replace_l.append(''.join(split_l))\n",
        "            # Restore the original character for the next iteration\n",
        "            split_l[i] = word[i]\n",
        "    \n",
        "    if verbose: \n",
        "        print(f\"Input word = {word} \\nsplit_l = {split_l} \\nreplace_l {replace_l} \\nSize of replace_l = {len(replace_l)}\")   \n",
        "    \n",
        "    return replace_l\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input word = can \n",
            "split_l = ['c', 'a', 'n'] \n",
            "replace_l ['aan', 'ban', 'dan', 'ean', 'fan', 'gan', 'han', 'ian', 'jan', 'kan', 'lan', 'man', 'nan', 'oan', 'pan', 'qan', 'ran', 'san', 'tan', 'uan', 'van', 'wan', 'xan', 'yan', 'zan', 'cbn', 'ccn', 'cdn', 'cen', 'cfn', 'cgn', 'chn', 'cin', 'cjn', 'ckn', 'cln', 'cmn', 'cnn', 'con', 'cpn', 'cqn', 'crn', 'csn', 'ctn', 'cun', 'cvn', 'cwn', 'cxn', 'cyn', 'czn', 'caa', 'cab', 'cac', 'cad', 'cae', 'caf', 'cag', 'cah', 'cai', 'caj', 'cak', 'cal', 'cam', 'cao', 'cap', 'caq', 'car', 'cas', 'cat', 'cau', 'cav', 'caw', 'cax', 'cay', 'caz'] \n",
            "Size of replace_l = 75\n"
          ]
        }
      ],
      "source": [
        "replace_l = replace_letter(word='can',\n",
        "                              verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**insert_letter**\n",
        "\n",
        "**Step 1:** is the same as in `delete_letter()`\n",
        "\n",
        "**Step 2:** This can be a list comprehension of the form:  \n",
        "`[f(a,b,c) for a, b in splits if condition for c in string]`   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def insert_letter(word, verbose=False):\n",
        "    '''\n",
        "    Input:\n",
        "        word: the input string/word \n",
        "    Output:\n",
        "        inserts: a set of all possible strings with one new letter inserted at every offset\n",
        "    ''' \n",
        "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    insert_l = []\n",
        "    split_l = list(word)\n",
        "    \n",
        "    for i in range(len(split_l) + 1):  # Iterate over each position in the word and one position past the end\n",
        "        for letter in letters:\n",
        "            # Insert the letter at position i\n",
        "            new_word = split_l[:i] + [letter] + split_l[i:]\n",
        "            # Append the resulting word to the insert_l list\n",
        "            insert_l.append(''.join(new_word))\n",
        "    \n",
        "    if verbose: \n",
        "        print(f\"Input word {word} \\nsplit_l = {split_l} \\ninsert_l = {insert_l}\")\n",
        "    \n",
        "    return insert_l\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input word at \n",
            "split_l = ['a', 't'] \n",
            "insert_l = ['aat', 'bat', 'cat', 'dat', 'eat', 'fat', 'gat', 'hat', 'iat', 'jat', 'kat', 'lat', 'mat', 'nat', 'oat', 'pat', 'qat', 'rat', 'sat', 'tat', 'uat', 'vat', 'wat', 'xat', 'yat', 'zat', 'aat', 'abt', 'act', 'adt', 'aet', 'aft', 'agt', 'aht', 'ait', 'ajt', 'akt', 'alt', 'amt', 'ant', 'aot', 'apt', 'aqt', 'art', 'ast', 'att', 'aut', 'avt', 'awt', 'axt', 'ayt', 'azt', 'ata', 'atb', 'atc', 'atd', 'ate', 'atf', 'atg', 'ath', 'ati', 'atj', 'atk', 'atl', 'atm', 'atn', 'ato', 'atp', 'atq', 'atr', 'ats', 'att', 'atu', 'atv', 'atw', 'atx', 'aty', 'atz']\n",
            "Number of strings output by insert_letter('at') is 78\n"
          ]
        }
      ],
      "source": [
        "insert_l = insert_letter('at', True)\n",
        "print(f\"Number of strings output by insert_letter('at') is {len(insert_l)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Combining the Edits:\n",
        "Now that you have implemented the string manipulations, you will create two functions that, given a string, will return all the possible single and double edits on that string. These will be `edit_one_letter()` and `edit_two_letters()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`edit_one_letter` \n",
        "\n",
        "function to get all the possible edits that are one edit away from a word. The edits  consist of the replace, insert, delete, and optionally the switch operation. You should use the previous functions you have already implemented to complete this function. The 'switch' function  is a less common edit function, so its use will be selected by an \"allow_switches\" input argument.\n",
        "\n",
        "Note that those functions return *lists* while this function should return a *python set*. Utilizing a set eliminates any duplicate entries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def edit_one_letter(word, allow_switches=True):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        word: the string/word for which we will generate all possible words that are one edit away.\n",
        "        allow_switches: a boolean indicating whether to allow switches of adjacent characters\n",
        "    Output:\n",
        "        edit_one_set: a set of words with one possible edit. Please return a set and not a list.\n",
        "    \"\"\"\n",
        "    \n",
        "    edit_one_set = set()\n",
        "    \n",
        "    # Generate all words with one deletion\n",
        "    for i in range(len(word)):\n",
        "        edit_one_set.add(word[:i] + word[i+1:])\n",
        "    \n",
        "    # Generate all words with one insertion\n",
        "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    for i in range(len(word) + 1):\n",
        "        for letter in letters:\n",
        "            edit_one_set.add(word[:i] + letter + word[i:])\n",
        "    \n",
        "    # Generate all words with one replacement\n",
        "    for i in range(len(word)):\n",
        "        for letter in letters:\n",
        "            if letter != word[i]:\n",
        "                edit_one_set.add(word[:i] + letter + word[i+1:])\n",
        "    \n",
        "    # Generate all words with one switch (if allowed)\n",
        "    if allow_switches:\n",
        "        for i in range(len(word) - 1):\n",
        "            edit_one_set.add(word[:i] + word[i+1] + word[i] + word[i+2:])\n",
        "    \n",
        "    return edit_one_set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input word at \n",
            "edit_one_l \n",
            "['a', 'aa', 'aat', 'ab', 'abt', 'ac', 'act', 'ad', 'adt', 'ae', 'aet', 'af', 'aft', 'ag', 'agt', 'ah', 'aht', 'ai', 'ait', 'aj', 'ajt', 'ak', 'akt', 'al', 'alt', 'am', 'amt', 'an', 'ant', 'ao', 'aot', 'ap', 'apt', 'aq', 'aqt', 'ar', 'art', 'as', 'ast', 'ata', 'atb', 'atc', 'atd', 'ate', 'atf', 'atg', 'ath', 'ati', 'atj', 'atk', 'atl', 'atm', 'atn', 'ato', 'atp', 'atq', 'atr', 'ats', 'att', 'atu', 'atv', 'atw', 'atx', 'aty', 'atz', 'au', 'aut', 'av', 'avt', 'aw', 'awt', 'ax', 'axt', 'ay', 'ayt', 'az', 'azt', 'bat', 'bt', 'cat', 'ct', 'dat', 'dt', 'eat', 'et', 'fat', 'ft', 'gat', 'gt', 'hat', 'ht', 'iat', 'it', 'jat', 'jt', 'kat', 'kt', 'lat', 'lt', 'mat', 'mt', 'nat', 'nt', 'oat', 'ot', 'pat', 'pt', 'qat', 'qt', 'rat', 'rt', 'sat', 'st', 't', 'ta', 'tat', 'tt', 'uat', 'ut', 'vat', 'vt', 'wat', 'wt', 'xat', 'xt', 'yat', 'yt', 'zat', 'zt']\n",
            "\n",
            "The type of the returned object should be a set <class 'set'>\n",
            "Number of outputs from edit_one_letter('at') is 129\n"
          ]
        }
      ],
      "source": [
        "tmp_word = \"at\"\n",
        "tmp_edit_one_set = edit_one_letter(tmp_word)\n",
        "# turn this into a list to sort it, in order to view it\n",
        "tmp_edit_one_l = sorted(list(tmp_edit_one_set))\n",
        "\n",
        "print(f\"input word {tmp_word} \\nedit_one_l \\n{tmp_edit_one_l}\\n\")\n",
        "print(f\"The type of the returned object should be a set {type(tmp_edit_one_set)}\")\n",
        "print(f\"Number of outputs from edit_one_letter('at') is {len(edit_one_letter('at'))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`edit_two_letters`\n",
        "\n",
        "Now you can generalize this to implement to get two edits on a word. To do so, you would have to get all the possible edits on a single word and then for each modified word, you would have to modify it again. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def edit_two_letters(word, allow_switches=True):\n",
        "    '''\n",
        "    Input:\n",
        "        word: the input string/word \n",
        "    Output:\n",
        "        edit_two_set: a set of strings with all possible two edits\n",
        "    '''\n",
        "    \n",
        "    edit_two_set = set()\n",
        "    \n",
        "    # Generate all strings with one edit and then generate all possible edits for each of those strings\n",
        "    edit_one = edit_one_letter(word, allow_switches=allow_switches)\n",
        "    for w in edit_one:\n",
        "        edit_two = edit_one_letter(w, allow_switches=allow_switches)\n",
        "        edit_two_set.update(edit_two)\n",
        "    \n",
        "    return edit_two_set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of strings with edit distance of two: 7154\n",
            "First 10 strings ['', 'a', 'aa', 'aaa', 'aaat', 'aab', 'aabt', 'aac', 'aact', 'aad']\n",
            "Last 10 strings ['zwt', 'zx', 'zxat', 'zxt', 'zy', 'zyat', 'zyt', 'zz', 'zzat', 'zzt']\n",
            "The data type of the returned object should be a set <class 'set'>\n",
            "Number of strings that are 2 edit distances from 'at' is 7154\n"
          ]
        }
      ],
      "source": [
        "tmp_edit_two_set = edit_two_letters(\"at\")\n",
        "tmp_edit_two_l = sorted(list(tmp_edit_two_set))\n",
        "print(f\"Number of strings with edit distance of two: {len(tmp_edit_two_l)}\")\n",
        "print(f\"First 10 strings {tmp_edit_two_l[:10]}\")\n",
        "print(f\"Last 10 strings {tmp_edit_two_l[-10:]}\")\n",
        "print(f\"The data type of the returned object should be a set {type(tmp_edit_two_set)}\")\n",
        "print(f\"Number of strings that are 2 edit distances from 'at' is {len(edit_two_letters('at'))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get the corrections:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " `get_corrections`\n",
        " \n",
        "which returns a list of zero to n possible suggestion tuples of the form (word, probability_of_word). \n",
        "\n",
        "**Step 1:** Generate suggestions for a supplied word: You'll use the edit functions you have developed. The 'suggestion algorithm' should follow this logic: \n",
        "* If the word is in the vocabulary, suggest the word. \n",
        "* Otherwise, if there are suggestions from `edit_one_letter` that are in the vocabulary, use those. \n",
        "* Otherwise, if there are suggestions from `edit_two_letters` that are in the vocabulary, use those. \n",
        "* Otherwise, suggest the input word.*  \n",
        "* The idea is that words generated from fewer edits are more likely than words with more edits.\n",
        "\n",
        "The logical `or` could be used to implement the suggestion algorithm very compactly. Alternately, if/elif/else constructs could be used.\n",
        " \n",
        "**Step 2**: Create a 'best_words' dictionary where the 'key' is a suggestion and the 'value' is the probability of that word in your vocabulary. If the word is not in the vocabulary, assign it a probability of 0.\n",
        "\n",
        "**Step 3**: Select the n best suggestions. There may be fewer than n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def known(words, vocab):\n",
        "    \"\"\"Return the subset of words that are actually in the vocabulary.\"\"\"\n",
        "    return set(w for w in words if w in vocab)\n",
        "\n",
        "def edits0(word):\n",
        "    \"\"\"Return all strings that are zero edits away from the input word.\"\"\"\n",
        "    return {word}\n",
        "\n",
        "def edits1(word):\n",
        "    \"\"\"Return all strings that are one edit away from the input word.\"\"\"\n",
        "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "    deletes = [left + right[1:] for left, right in splits if right]\n",
        "    inserts = [left + letter + right for left, right in splits for letter in letters]\n",
        "    replaces = [left + letter + right[1:] for left, right in splits if right for letter in letters]\n",
        "    switches = [left + right[1] + right[0] + right[2:] for left, right in splits if len(right) > 1]\n",
        "    return set(deletes + inserts + replaces + switches)\n",
        "\n",
        "def edits2(word):\n",
        "    \"\"\"Return all strings that are two edits away from the input word.\"\"\"\n",
        "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}\n",
        "\n",
        "def get_corrections(word, probs, vocab, n=2, verbose=False):\n",
        "    \"\"\"\n",
        "    Input: \n",
        "        word: a user entered string to check for suggestions\n",
        "        probs: a dictionary that maps each word to its probability in the corpus\n",
        "        vocab: a set containing all the vocabulary\n",
        "        n: number of possible word corrections you want returned in the dictionary\n",
        "    Output: \n",
        "        n_best: a list of tuples with the most probable n corrected words and their probabilities.\n",
        "    \"\"\"\n",
        "    suggestions = []\n",
        "    n_best = []\n",
        "    \n",
        "    # Step 1: Create suggestions as described above    \n",
        "    suggestions = (known(edits0(word), vocab) or \n",
        "                   known(edits1(word), vocab) or \n",
        "                   known(edits2(word), vocab) or \n",
        "                   [word])\n",
        "                    \n",
        "    # Step 2: Determine the probability of suggestions\n",
        "    probs_suggestions = {w: probs[w] if w in probs else 0 for w in suggestions}\n",
        "    \n",
        "    # Step 3: Get the best words and return the most probable top n_suggested words as n_best\n",
        "    n_best = Counter(probs_suggestions).most_common(n)\n",
        "    \n",
        "    if verbose: \n",
        "        print(\"Entered word =\", word, \"\\nSuggestions =\", suggestions)\n",
        "\n",
        "    return n_best\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word 0: dear, probability 0.000839\n",
            "word 1: death, probability 0.000783\n",
            "data type of corrections <class 'list'>\n"
          ]
        }
      ],
      "source": [
        "# Test your implementation - feel free to try other words in my word\n",
        "my_word = 'deah' \n",
        "tmp_corrections = get_corrections(my_word, probs, vocab, 2) # keep verbose=True\n",
        "for i, word_prob in enumerate(tmp_corrections):\n",
        "    print(f\"word {i}: {word_prob[0]}, probability {word_prob[1]:.6f}\")\n",
        "\n",
        "# CODE REVIEW COMMENT: using \"tmp_corrections\" insteads of \"cors\". \"cors\" is not defined\n",
        "print(f\"data type of corrections {type(tmp_corrections)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NLP with Probabilistic models : AutoComplete"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Preprocess Data\n",
        "You will use twitter data.\n",
        "Load the data and view the first few sentences by running the next cell.\n",
        "\n",
        "Notice that data is a long string that contains many many tweets.\n",
        "Observe that there is a line break \"\\n\" between tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data type: <class 'str'>\n",
            "Number of letters: 3335471\n",
            "First 300 letters of the data\n",
            "-------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\\nWhen you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\\nthey've decided its more fun if I don't.\\nSo Tired D; Played Lazer Tag & Ran A \""
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------\n",
            "Last 300 letters of the data\n",
            "-------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"ust had one a few weeks back....hopefully we will be back soon! wish you the best yo\\nColombia is with an 'o'...“: We now ship to 4 countries in South America (fist pump). Please welcome Columbia to the Stunner Family”\\n#GutsiestMovesYouCanMake Giving a cat a bath.\\nCoffee after 5 was a TERRIBLE idea.\\n\""
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------\n"
          ]
        }
      ],
      "source": [
        "with open(r\"data\\NLP with Probabilistic models  AutoComplete\\en_US.twitter.txt\", \"r\") as f:\n",
        "    data = f.read()\n",
        "print(\"Data type:\", type(data))\n",
        "print(\"Number of letters:\", len(data))\n",
        "print(\"First 300 letters of the data\")\n",
        "print(\"-------\")\n",
        "display(data[0:300])\n",
        "print(\"-------\")\n",
        "\n",
        "print(\"Last 300 letters of the data\")\n",
        "print(\"-------\")\n",
        "display(data[-300:])\n",
        "print(\"-------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pre-process the Data\n",
        "\n",
        "Preprocess this data with the following steps:\n",
        "\n",
        "1. Split data into sentences using \"\\n\" as the delimiter.\n",
        "1. Split each sentence into tokens. Note that in this assignment we use \"token\" and \"words\" interchangeably.\n",
        "1. Assign sentences into train or test sets.\n",
        "1. Find tokens that appear at least N times in the training data.\n",
        "1. Replace tokens that appear less than N times by `<unk>`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_to_sentences(data):\n",
        "    \"\"\"\n",
        "    Split data by linebreak \"\\n\"\n",
        "    \n",
        "    Args:\n",
        "        data: str\n",
        "    \n",
        "    Returns:\n",
        "        A list of sentences\n",
        "    \"\"\"\n",
        "    # Split the data by linebreak \"\\n\" to get individual sentences\n",
        "    sentences = data.split(\"\\n\")\n",
        "    \n",
        "    # Remove leading and trailing spaces from each sentence\n",
        "    sentences = [s.strip() for s in sentences]\n",
        "    \n",
        "    # Drop sentences if they are empty strings.\n",
        "    sentences = [s for s in sentences if len(s) > 0]\n",
        "    \n",
        "    return sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "I have a pen.\n",
            "I have an apple. \n",
            "Ah\n",
            "Apple pen.\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['I have a pen.', 'I have an apple.', 'Ah', 'Apple pen.']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test\n",
        "x = \"\"\"\n",
        "I have a pen.\\nI have an apple. \\nAh\\nApple pen.\\n\n",
        "\"\"\"\n",
        "print(x)\n",
        "\n",
        "split_to_sentences(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### tokenize_sentences\n",
        "The next step is to tokenize sentences (split a sentence into a list of words). \n",
        "- Convert all tokens into lower case so that words which are capitalized (for example, at the start of a sentence) in the original text are treated the same as the lowercase versions of the words.\n",
        "- Append each tokenized list of words into a list of tokenized sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def tokenize_sentences(sentences):\n",
        "    \"\"\"\n",
        "    Tokenize sentences into tokens (words) using NLTK tokenizer\n",
        "    \n",
        "    Args:\n",
        "        sentences: List of strings\n",
        "    \n",
        "    Returns:\n",
        "        List of lists of tokens\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize the list of lists of tokenized sentences\n",
        "    tokenized_sentences = []\n",
        "    \n",
        "    # Go through each sentence\n",
        "    for sentence in sentences:\n",
        "        \n",
        "        # Convert to lowercase letters\n",
        "        sentence = sentence.lower()\n",
        "        \n",
        "        # Tokenize the sentence using NLTK tokenizer\n",
        "        tokenized = word_tokenize(sentence)\n",
        "        \n",
        "        # append the list of words to the list of lists\n",
        "        tokenized_sentences.append(tokenized)\n",
        "    \n",
        "    return tokenized_sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['sky', 'is', 'blue', '.'],\n",
              " ['leaves', 'are', 'green', '.'],\n",
              " ['roses', 'are', 'red', '.']]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences = [\"Sky is blue.\", \"Leaves are green.\", \"Roses are red.\"]\n",
        "tokenize_sentences(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**get_tokenized_data**\n",
        "\n",
        "\n",
        "Use the two functions that you have just implemented to get the tokenized data.\n",
        "- split the data into sentences\n",
        "- tokenize those sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_tokenized_data(data):\n",
        "    \"\"\"\n",
        "    Make a list of tokenized sentences\n",
        "    \n",
        "    Args:\n",
        "        data: String\n",
        "    \n",
        "    Returns:\n",
        "        List of lists of tokens\n",
        "    \"\"\"    \n",
        "    # Get the sentences by splitting up the data\n",
        "    sentences = split_to_sentences(data)\n",
        "    \n",
        "    # Get the list of lists of tokens by tokenizing the sentences\n",
        "    tokenized_sentences = tokenize_sentences(sentences)    \n",
        "    return tokenized_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['sky', 'is', 'blue', '.'],\n",
              " ['leaves', 'are', 'green'],\n",
              " ['roses', 'are', 'red', '.']]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = \"Sky is blue.\\nLeaves are green\\nRoses are red.\"\n",
        "get_tokenized_data(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_data = get_tokenized_data(data)\n",
        "random.seed(87)\n",
        "random.shuffle(tokenized_data)\n",
        "\n",
        "train_size = int(len(tokenized_data) * 0.8)\n",
        "train_data = tokenized_data[0:train_size]\n",
        "test_data = tokenized_data[train_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "47961 data are split into 38368 train and 9593 test set\n",
            "First training sample:\n",
            "['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the', 'team', 'local', 'company', 'and', 'quality', 'production']\n",
            "First test sample\n",
            "['that', 'picture', 'i', 'just', 'seen', 'whoa', 'dere', '!', '!', '>', '>', '>', '>', '>', '>', '>']\n"
          ]
        }
      ],
      "source": [
        "print(\"{} data are split into {} train and {} test set\".format(\n",
        "    len(tokenized_data), len(train_data), len(test_data)))\n",
        "\n",
        "print(\"First training sample:\")\n",
        "print(train_data[0])\n",
        "      \n",
        "print(\"First test sample\")\n",
        "print(test_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**count_words**\n",
        "\n",
        "You won't use all the tokens (words) appearing in the data for training. Instead, you will use the more frequently used words.  \n",
        "- You will focus on the words that appear at least N times in the data.\n",
        "- First count how many times each word appears in the data.\n",
        "\n",
        "You will need a double for-loop, one for sentences and the other for tokens within a sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_words(tokenized_sentences):\n",
        "    \"\"\"\n",
        "    Count the number of word appearence in the tokenized sentences\n",
        "    \n",
        "    Args:\n",
        "        tokenized_sentences: List of lists of strings\n",
        "    \n",
        "    Returns:\n",
        "        dict that maps word (str) to the frequency (int)\n",
        "    \"\"\"\n",
        "            \n",
        "    word_counts = {}\n",
        "    # Loop through each sentence\n",
        "    for sentence in tokenized_sentences: # complete this line\n",
        "        \n",
        "        # Go through each token in the sentence\n",
        "        for token in sentence: # complete this line\n",
        "\n",
        "            # If the token is not in the dictionary yet, set the count to 1\n",
        "            if token not in word_counts: # complete this line with the proper condition\n",
        "                word_counts[token] = 1\n",
        "            \n",
        "            # If the token is already in the dictionary, increment the count by 1\n",
        "            else:\n",
        "                word_counts[token] += 1\n",
        "    \n",
        "    return word_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'sky': 1,\n",
              " 'is': 1,\n",
              " 'blue': 1,\n",
              " '.': 3,\n",
              " 'leaves': 1,\n",
              " 'are': 2,\n",
              " 'green': 1,\n",
              " 'roses': 1,\n",
              " 'red': 1}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_sentences = [['sky', 'is', 'blue', '.'],\n",
        "                       ['leaves', 'are', 'green', '.'],\n",
        "                       ['roses', 'are', 'red', '.']]\n",
        "count_words(tokenized_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Handling 'Out of Vocabulary' words**\n",
        "\n",
        "If your model is performing autocomplete, but encounters a word that it never saw during training, it won't have an input word to help it determine the next word to suggest. The model will not be able to predict the next word because there are no counts for the current word. \n",
        "- This 'new' word is called an 'unknown word', or <b>out of vocabulary (OOV)</b> words.\n",
        "- The percentage of unknown words in the test set is called the <b> OOV </b> rate. \n",
        "\n",
        "To handle unknown words during prediction, use a special token to represent all unknown words 'unk'. \n",
        "- Modify the training data so that it has some 'unknown' words to train on.\n",
        "- Words to convert into \"unknown\" words are those that do not occur very frequently in the training set.\n",
        "- Create a list of the most frequent words in the training set, called the <b> closed vocabulary </b>. \n",
        "- Convert all the other words that are not part of the closed vocabulary to the token 'unk'. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**get_words_with_nplus_frequency**\n",
        "\n",
        "You will now create a function that takes in a text document and a threshold `count_threshold`.\n",
        "- Any word whose count is greater than or equal to the threshold `count_threshold` is kept in the closed vocabulary.\n",
        "- Returns the word closed vocabulary list. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_words_with_nplus_frequency(tokenized_sentences, count_threshold):\n",
        "    \"\"\"\n",
        "    Find the words that appear N times or more\n",
        "    \n",
        "    Args:\n",
        "        tokenized_sentences: List of lists of sentences\n",
        "        count_threshold: minimum number of occurrences for a word to be in the closed vocabulary.\n",
        "    \n",
        "    Returns:\n",
        "        List of words that appear N times or more\n",
        "    \"\"\"\n",
        "    # Initialize an empty list to contain the words that\n",
        "    # appear at least 'minimum_freq' times.\n",
        "    closed_vocab = []\n",
        "    \n",
        "    # Get the word couts of the tokenized sentences\n",
        "    # Use the function that you defined earlier to count the words\n",
        "    word_counts = count_words(tokenized_sentences)\n",
        "\n",
        "    # for each word and its count\n",
        "    for word, cnt in word_counts.items(): # complete this line\n",
        "        \n",
        "        # check that the word's count\n",
        "        # is at least as great as the minimum count\n",
        "        if cnt >= count_threshold: # complete this line with the proper condition\n",
        "            \n",
        "            # append the word to the list\n",
        "            closed_vocab.append(word)\n",
        "    \n",
        "    return closed_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closed vocabulary:\n",
            "['.', 'are']\n"
          ]
        }
      ],
      "source": [
        "tokenized_sentences = [['sky', 'is', 'blue', '.'],\n",
        "                       ['leaves', 'are', 'green', '.'],\n",
        "                       ['roses', 'are', 'red', '.']]\n",
        "tmp_closed_vocab = get_words_with_nplus_frequency(tokenized_sentences, count_threshold=2)\n",
        "print(f\"Closed vocabulary:\")\n",
        "print(tmp_closed_vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**replace_oov_words_by_unk**\n",
        "\n",
        "The words that appear `count_threshold` times or more are in the closed vocabulary. \n",
        "- All other words are regarded as `unknown`.\n",
        "- Replace words not in the closed vocabulary with the token `<unk>`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n",
        "    \"\"\"\n",
        "    Replace words not in the given vocabulary with '<unk>' token.\n",
        "    \n",
        "    Args:\n",
        "        tokenized_sentences: List of lists of strings\n",
        "        vocabulary: List of strings that we will use\n",
        "        unknown_token: A string representing unknown (out-of-vocabulary) words\n",
        "    \n",
        "    Returns:\n",
        "        List of lists of strings, with words not in the vocabulary replaced\n",
        "    \"\"\"\n",
        "    \n",
        "    # Place vocabulary into a set for faster search\n",
        "    vocabulary = set(vocabulary)\n",
        "    \n",
        "    # Initialize a list that will hold the sentences\n",
        "    # after less frequent words are replaced by the unknown token\n",
        "    replaced_tokenized_sentences = []\n",
        "    \n",
        "    # Go through each sentence\n",
        "    for sentence in tokenized_sentences:\n",
        "        \n",
        "        # Initialize the list that will contain\n",
        "        # a single sentence with \"unknown_token\" replacements\n",
        "        replaced_sentence = []\n",
        "\n",
        "        # for each token in the sentence\n",
        "        for token in sentence: # complete this line\n",
        "            \n",
        "            # Check if the token is in the closed vocabulary\n",
        "            if token in vocabulary: # complete this line with the proper condition\n",
        "                # If so, append the word to the replaced_sentence\n",
        "                replaced_sentence.append(token)\n",
        "            else:\n",
        "                # otherwise, append the unknown token instead\n",
        "                replaced_sentence.append(unknown_token)\n",
        "        # Append the list of tokens to the list of lists\n",
        "        replaced_tokenized_sentences.append(replaced_sentence)\n",
        "    return replaced_tokenized_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original sentence:\n",
            "[['dogs', 'run'], ['cats', 'sleep']]\n",
            "tokenized_sentences with less frequent words converted to '<unk>':\n",
            "[['dogs', '<unk>'], ['<unk>', 'sleep']]\n"
          ]
        }
      ],
      "source": [
        "tokenized_sentences = [[\"dogs\", \"run\"], [\"cats\", \"sleep\"]]\n",
        "vocabulary = [\"dogs\", \"sleep\"]\n",
        "tmp_replaced_tokenized_sentences = replace_oov_words_by_unk(tokenized_sentences, vocabulary)\n",
        "print(f\"Original sentence:\")\n",
        "print(tokenized_sentences)\n",
        "print(f\"tokenized_sentences with less frequent words converted to '<unk>':\")\n",
        "print(tmp_replaced_tokenized_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**preprocess_data**\n",
        "\n",
        "Now we are ready to process our data by combining the functions.\n",
        "\n",
        "1. Find tokens that appear at least count_threshold times in the training data.\n",
        "1. Replace tokens that appear less than count_threshold times by \"<unk\\>\" both for training and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_data(train_data, test_data, count_threshold, unknown_token=\"<unk>\", \n",
        "                    get_words_with_nplus_frequency=get_words_with_nplus_frequency, \n",
        "                    replace_oov_words_by_unk=replace_oov_words_by_unk):\n",
        "    \"\"\"\n",
        "    Preprocess data, i.e.,\n",
        "        - Find tokens that appear at least N times in the training data.\n",
        "        - Replace tokens that appear less than N times by \"<unk>\" both for training and test data.        \n",
        "    Args:\n",
        "        train_data, test_data: List of lists of strings.\n",
        "        count_threshold: Words whose count is less than this are treated as unknown.\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of\n",
        "        - training data with low frequent words replaced by \"<unk>\"\n",
        "        - test data with low frequent words replaced by \"<unk>\"\n",
        "        - vocabulary of words that appear n times or more in the training data\n",
        "    \"\"\"\n",
        "    # Get the closed vocabulary using the train data\n",
        "    vocabulary = get_words_with_nplus_frequency(train_data, count_threshold)\n",
        "    \n",
        "    # For the train data, replace less common words with \"<unk>\"\n",
        "    train_data_replaced = replace_oov_words_by_unk(train_data, vocabulary, unknown_token)\n",
        "    \n",
        "    # For the test data, replace less common words with \"<unk>\"\n",
        "    test_data_replaced = replace_oov_words_by_unk(test_data, vocabulary, unknown_token)\n",
        "    \n",
        "    return train_data_replaced, test_data_replaced, vocabulary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tmp_train_repl\n",
            "[['sky', 'is', 'blue', '.'], ['leaves', 'are', 'green']]\n",
            "\n",
            "tmp_test_repl\n",
            "[['<unk>', 'are', '<unk>', '.']]\n",
            "\n",
            "tmp_vocab\n",
            "['sky', 'is', 'blue', '.', 'leaves', 'are', 'green']\n"
          ]
        }
      ],
      "source": [
        "tmp_train = [['sky', 'is', 'blue', '.'],\n",
        "     ['leaves', 'are', 'green']]\n",
        "tmp_test = [['roses', 'are', 'red', '.']]\n",
        "\n",
        "tmp_train_repl, tmp_test_repl, tmp_vocab = preprocess_data(tmp_train, \n",
        "                                                           tmp_test, \n",
        "                                                           count_threshold = 1\n",
        "                                                          )\n",
        "\n",
        "print(\"tmp_train_repl\")\n",
        "print(tmp_train_repl)\n",
        "print()\n",
        "print(\"tmp_test_repl\")\n",
        "print(tmp_test_repl)\n",
        "print()\n",
        "print(\"tmp_vocab\")\n",
        "print(tmp_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "minimum_freq = 2\n",
        "train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, \n",
        "                                                                        test_data, \n",
        "                                                                        minimum_freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First preprocessed training sample:\n",
            "['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the', 'team', 'local', 'company', 'and', 'quality', 'production']\n",
            "\n",
            "First preprocessed test sample:\n",
            "['that', 'picture', 'i', 'just', 'seen', 'whoa', 'dere', '!', '!', '>', '>', '>', '>', '>', '>', '>']\n",
            "\n",
            "First 10 vocabulary:\n",
            "['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the']\n",
            "\n",
            "Size of vocabulary: 14823\n"
          ]
        }
      ],
      "source": [
        "print(\"First preprocessed training sample:\")\n",
        "print(train_data_processed[0])\n",
        "print()\n",
        "print(\"First preprocessed test sample:\")\n",
        "print(test_data_processed[0])\n",
        "print()\n",
        "print(\"First 10 vocabulary:\")\n",
        "print(vocabulary[0:10])\n",
        "print()\n",
        "print(\"Size of vocabulary:\", len(vocabulary))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Develop n-gram based Language Models\n",
        "\n",
        "In this section, we will develop the n-grams language model.\n",
        "- Assume the probability of the next word depends only on the previous n-gram.\n",
        "- The previous n-gram is the series of the previous 'n' words.\n",
        "\n",
        "The conditional probability for the word at position 't' in the sentence, given that the words preceding it are $w_{t-n}\\cdots w_{t-2}, w_{t-1}$ is:\n",
        "\n",
        "$$ P(w_t | w_{t-n}\\dots w_{t-1} ) \\tag{1}$$\n",
        "\n",
        "You can estimate this probability  by counting the occurrences of these series of words in the training data.\n",
        "- The probability can be estimated as a ratio, where\n",
        "- The numerator is the number of times word 't' appears after words t-n through t-1 appear in the training data.\n",
        "- The denominator is the number of times word t-n through t-1 appears in the training data.\n",
        "\n",
        "\n",
        "$$ \\hat{P}(w_t | w_{t-n} \\dots w_{t-1}) = \\frac{C(w_{t-n}\\dots w_{t-1}, w_t)}{C(w_{t-n}\\dots w_{t-1})} \\tag{2} $$\n",
        "\n",
        "\n",
        "- The function $C(\\cdots)$ denotes the number of occurence of the given sequence. \n",
        "- $\\hat{P}$ means the estimation of $P$. \n",
        "- Notice that denominator of the equation (2) is the number of occurence of the previous $n$ words, and the numerator is the same sequence followed by the word $w_t$.\n",
        "\n",
        "Later, you will modify the equation (2) by adding k-smoothing, which avoids errors when any counts are zero.\n",
        "\n",
        "The equation (2) tells us that to estimate probabilities based on n-grams, you need the counts of n-grams (for denominator) and (n+1)-grams (for numerator)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Exercise 8 - count_n_grams**\n",
        "\n",
        "function that computes the counts of n-grams for an arbitrary number $n$.\n",
        "\n",
        "When computing the counts for n-grams, prepare the sentence beforehand by prepending $n-1$ starting markers \"<s\\>\" to indicate the beginning of the sentence.  \n",
        "- For example, in the tri-gram model (n=3), a sequence with two start tokens \"<s\\>\" should predict the first word of a sentence.\n",
        "- So, if the sentence is \"I like food\", modify it to be \"<s\\> <s\\> I like food\".\n",
        "- Also prepare the sentence for counting by appending an end token \"<e\\>\" so that the model can predict when to finish a sentence.\n",
        "\n",
        "Technical note: In this implementation, you will store the counts as a dictionary.\n",
        "- The key of each key-value pair in the dictionary is a **tuple** of n words (and not a list)\n",
        "- The value in the key-value pair is the number of occurrences.  \n",
        "- The reason for using a tuple as a key instead of a list is because a list in Python is a mutable object (it can be changed after it is first created).  A tuple is \"immutable\", so it cannot be altered after it is first created.  This makes a tuple suitable as a data type for the key in a dictionary.\n",
        "- Although for a n-gram you need to use n-1 starting markers for a sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_n_grams(data, n, start_token='<s>', end_token='<e>'):\n",
        "    \"\"\"\n",
        "    Count all n-grams in the data\n",
        "    \n",
        "    Args:\n",
        "        data: List of lists of words\n",
        "        n: number of words in a sequence\n",
        "    \n",
        "    Returns:\n",
        "        A dictionary that maps a tuple of n-words to its frequency\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize dictionary of n-grams and their counts\n",
        "    n_grams = {}\n",
        "\n",
        "    # Go through each sentence in the data\n",
        "    for sentence in data:\n",
        "        # prepend start token n times, and  append the end token one time\n",
        "        sentence = [start_token] * (n) + sentence + [end_token]\n",
        "        \n",
        "        # convert list to tuple\n",
        "        # So that the sequence of words can be used as\n",
        "        # a key in the dictionary\n",
        "        sentence = tuple(sentence)\n",
        "        \n",
        "        # Use 'i' to indicate the start of the n-gram\n",
        "        # from index 0\n",
        "        # to the last index where the end of the n-gram\n",
        "        # is within the sentence.\n",
        "        for i in range(len(sentence) - n + 1):\n",
        "            # Get the n-gram from i to i+n\n",
        "            n_gram = sentence[i:i + n]\n",
        "            \n",
        "            # check if the n-gram is in the dictionary\n",
        "            if n_gram in n_grams:\n",
        "                # Increment the count for this n-gram\n",
        "                n_grams[n_gram] += 1\n",
        "            else:\n",
        "                # Initialize this n-gram count to 1\n",
        "                n_grams[n_gram] = 1\n",
        "    \n",
        "    return n_grams\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uni-gram:\n",
            "{('<s>',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('<e>',): 2, ('this',): 1, ('dog',): 1, ('is',): 1}\n",
            "Bi-gram:\n",
            "{('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '<e>'): 2, ('<s>', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1}\n"
          ]
        }
      ],
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "print(\"Uni-gram:\")\n",
        "print(count_n_grams(sentences, 1))\n",
        "print(\"Bi-gram:\")\n",
        "print(count_n_grams(sentences, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**estimate_probability**\n",
        "\n",
        "Next, estimate the probability of a word given the prior 'n' words using the n-gram counts.\n",
        "\n",
        "$$ \\hat{P}(w_t | w_{t-n} \\dots w_{t-1}) = \\frac{C(w_{t-n}\\dots w_{t-1}, w_t)}{C(w_{t-n}\\dots w_{t-1})} \\tag{2} $$\n",
        "\n",
        "This formula doesn't work when a count of an n-gram is zero..\n",
        "- Suppose we encounter an n-gram that did not occur in the training data.  \n",
        "- Then, the equation (2) cannot be evaluated (it becomes zero divided by zero).\n",
        "\n",
        "A way to handle zero counts is to add k-smoothing.  \n",
        "- K-smoothing adds a positive constant $k$ to each numerator and $k \\times |V|$ in the denominator, where $|V|$ is the number of words in the vocabulary.\n",
        "\n",
        "$$ \\hat{P}(w_t | w_{t-n} \\dots w_{t-1}) = \\frac{C(w_{t-n}\\dots w_{t-1}, w_t) + k}{C(w_{t-n}\\dots w_{t-1}) + k|V|} \\tag{3} $$\n",
        "\n",
        "\n",
        "For n-grams that have a zero count, the equation (3) becomes $\\frac{1}{|V|}$.\n",
        "- This means that any n-gram with zero count has the same probability of $\\frac{1}{|V|}$.\n",
        "\n",
        "Define a function that computes the probability estimate (3) from n-gram counts and a constant $k$.\n",
        "\n",
        "- The function takes in a dictionary 'n_gram_counts', where the key is the n-gram and the value is the count of that n-gram.\n",
        "- The function also takes another dictionary n_plus1_gram_counts, which you'll use to find the count for the previous n-gram plus the current word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def estimate_probability(word, previous_n_gram, \n",
        "                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
        "    \"\"\"\n",
        "    Estimate the probabilities of a next word using the n-gram counts with k-smoothing\n",
        "    \n",
        "    Args:\n",
        "        word: next word\n",
        "        previous_n_gram: A sequence of words of length n (Contexte)\n",
        "        n_gram_counts: Dictionary of counts of n-grams\n",
        "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
        "        vocabulary_size: number of words in the vocabulary\n",
        "        k: positive constant, smoothing parameter\n",
        "    \n",
        "    Returns:\n",
        "        A probability\n",
        "    \"\"\"\n",
        "    # convert list to tuple to use it as a dictionary key\n",
        "    previous_n_gram = tuple(previous_n_gram)\n",
        "    \n",
        "    # Set the denominator\n",
        "    # If the previous n-gram exists in the dictionary of n-gram counts,\n",
        "    # Get its count.  Otherwise set the count to zero\n",
        "    # Use the dictionary that has counts for n-grams\n",
        "    previous_n_gram_count = n_gram_counts.get(previous_n_gram, 0)\n",
        "            \n",
        "    # Calculate the denominator using the count of the previous n gram\n",
        "    # and apply k-smoothing\n",
        "    denominator = previous_n_gram_count + k * vocabulary_size\n",
        "\n",
        "    # Define n plus 1 gram as the previous n-gram plus the current word as a tuple\n",
        "    n_plus1_gram = previous_n_gram + (word,)\n",
        "  \n",
        "    # Set the count to the count in the dictionary,\n",
        "    # otherwise 0 if not in the dictionary\n",
        "    # use the dictionary that has counts for the n-gram plus current word    \n",
        "    n_plus1_gram_count = n_plus1_gram_counts.get(n_plus1_gram, 0)\n",
        "            \n",
        "    # Define the numerator use the count of the n-gram plus current word,\n",
        "    # and apply smoothing\n",
        "    numerator = n_plus1_gram_count + k\n",
        "        \n",
        "    # Calculate the probability as the numerator divided by denominator\n",
        "    probability = numerator / denominator\n",
        "    \n",
        "    return probability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The estimated probability of word 'cat' given the previous n-gram 'a' is: 0.3333\n"
          ]
        }
      ],
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "tmp_prob = estimate_probability(\"cat\", [\"a\"], unigram_counts, bigram_counts, len(unique_words), k=1)\n",
        "\n",
        "print(f\"The estimated probability of word 'cat' given the previous n-gram 'a' is: {tmp_prob:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The function defined below loops over all words in vocabulary to calculate probabilities for all possible words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, end_token='<e>', unknown_token=\"<unk>\",  k=1.0):\n",
        "    \"\"\"\n",
        "    Estimate the probabilities of next words using the n-gram counts with k-smoothing\n",
        "    \n",
        "    Args:\n",
        "        previous_n_gram: A sequence of words of length n\n",
        "        n_gram_counts: Dictionary of counts of n-grams\n",
        "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
        "        vocabulary: List of words\n",
        "        k: positive constant, smoothing parameter\n",
        "    \n",
        "    Returns:\n",
        "        A dictionary mapping from next words to the probability.\n",
        "    \"\"\"\n",
        "    # convert list to tuple to use it as a dictionary key\n",
        "    previous_n_gram = tuple(previous_n_gram)    \n",
        "    \n",
        "    # add <e> <unk> to the vocabulary\n",
        "    # <s> is not needed since it should not appear as the next word\n",
        "    vocabulary = vocabulary + [end_token, unknown_token]    \n",
        "    vocabulary_size = len(vocabulary)    \n",
        "    \n",
        "    probabilities = {}\n",
        "    for word in vocabulary:\n",
        "        probability = estimate_probability(word, previous_n_gram, \n",
        "                                           n_gram_counts, n_plus1_gram_counts, \n",
        "                                           vocabulary_size, k=k)\n",
        "                \n",
        "        probabilities[word] = probability\n",
        "\n",
        "    return probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'cat': 0.09090909090909091,\n",
              " 'dog': 0.09090909090909091,\n",
              " 'like': 0.09090909090909091,\n",
              " 'i': 0.09090909090909091,\n",
              " 'this': 0.09090909090909091,\n",
              " 'is': 0.09090909090909091,\n",
              " 'a': 0.2727272727272727,\n",
              " '<e>': 0.09090909090909091,\n",
              " '<unk>': 0.09090909090909091}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "\n",
        "estimate_probabilities([\"like\"], unigram_counts, bigram_counts, unique_words, k=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Count and probability matrices**\n",
        "\n",
        "As we have seen so far, the n-gram counts computed above are sufficient for computing the probabilities of the next word.  \n",
        "- It can be more intuitive to present them as count or probability matrices.\n",
        "- The functions defined in the next cells return count or probability matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_count_matrix(n_plus1_gram_counts, vocabulary):\n",
        "    # add <e> <unk> to the vocabulary\n",
        "    # <s> is omitted since it should not appear as the next word\n",
        "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
        "    \n",
        "    # obtain unique n-grams\n",
        "    n_grams = []\n",
        "    for n_plus1_gram in n_plus1_gram_counts.keys():\n",
        "        n_gram = n_plus1_gram[0:-1]        \n",
        "        n_grams.append(n_gram)\n",
        "    n_grams = list(set(n_grams))\n",
        "    \n",
        "    # mapping from n-gram to row\n",
        "    row_index = {n_gram:i for i, n_gram in enumerate(n_grams)}    \n",
        "    # mapping from next word to column\n",
        "    col_index = {word:j for j, word in enumerate(vocabulary)}    \n",
        "    \n",
        "    nrow = len(n_grams)\n",
        "    ncol = len(vocabulary)\n",
        "    count_matrix = np.zeros((nrow, ncol))\n",
        "    for n_plus1_gram, count in n_plus1_gram_counts.items():\n",
        "        n_gram = n_plus1_gram[0:-1]\n",
        "        word = n_plus1_gram[-1]\n",
        "        if word not in vocabulary:\n",
        "            continue\n",
        "        i = row_index[n_gram]\n",
        "        j = col_index[word]\n",
        "        count_matrix[i, j] = count\n",
        "    \n",
        "    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)\n",
        "    return count_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bigram counts\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cat</th>\n",
              "      <th>dog</th>\n",
              "      <th>like</th>\n",
              "      <th>i</th>\n",
              "      <th>this</th>\n",
              "      <th>is</th>\n",
              "      <th>a</th>\n",
              "      <th>&lt;e&gt;</th>\n",
              "      <th>&lt;unk&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>(is,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(i,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(&lt;s&gt;,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(cat,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(like,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(a,)</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(this,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(dog,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         cat  dog  like    i  this   is    a  <e>  <unk>\n",
              "(is,)    0.0  0.0   1.0  0.0   0.0  0.0  0.0  0.0    0.0\n",
              "(i,)     0.0  0.0   1.0  0.0   0.0  0.0  0.0  0.0    0.0\n",
              "(<s>,)   0.0  0.0   0.0  1.0   1.0  0.0  0.0  0.0    0.0\n",
              "(cat,)   0.0  0.0   0.0  0.0   0.0  0.0  0.0  2.0    0.0\n",
              "(like,)  0.0  0.0   0.0  0.0   0.0  0.0  2.0  0.0    0.0\n",
              "(a,)     2.0  0.0   0.0  0.0   0.0  0.0  0.0  0.0    0.0\n",
              "(this,)  0.0  1.0   0.0  0.0   0.0  0.0  0.0  0.0    0.0\n",
              "(dog,)   0.0  0.0   0.0  0.0   0.0  1.0  0.0  0.0    0.0"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "\n",
        "print('bigram counts')\n",
        "display(make_count_matrix(bigram_counts, unique_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following function calculates the probabilities of each word given the previous n-gram, and stores this in matrix form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n",
        "    count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)\n",
        "    count_matrix += k\n",
        "    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n",
        "    return prob_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bigram probabilities\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cat</th>\n",
              "      <th>dog</th>\n",
              "      <th>like</th>\n",
              "      <th>i</th>\n",
              "      <th>this</th>\n",
              "      <th>is</th>\n",
              "      <th>a</th>\n",
              "      <th>&lt;e&gt;</th>\n",
              "      <th>&lt;unk&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>(is,)</th>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(i,)</th>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(&lt;s&gt;,)</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(cat,)</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(like,)</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(a,)</th>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(this,)</th>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(dog,)</th>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              cat       dog      like         i      this        is         a  \\\n",
              "(is,)    0.100000  0.100000  0.200000  0.100000  0.100000  0.100000  0.100000   \n",
              "(i,)     0.100000  0.100000  0.200000  0.100000  0.100000  0.100000  0.100000   \n",
              "(<s>,)   0.090909  0.090909  0.090909  0.181818  0.181818  0.090909  0.090909   \n",
              "(cat,)   0.090909  0.090909  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
              "(like,)  0.090909  0.090909  0.090909  0.090909  0.090909  0.090909  0.272727   \n",
              "(a,)     0.272727  0.090909  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
              "(this,)  0.100000  0.200000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
              "(dog,)   0.100000  0.100000  0.100000  0.100000  0.100000  0.200000  0.100000   \n",
              "\n",
              "              <e>     <unk>  \n",
              "(is,)    0.100000  0.100000  \n",
              "(i,)     0.100000  0.100000  \n",
              "(<s>,)   0.090909  0.090909  \n",
              "(cat,)   0.272727  0.090909  \n",
              "(like,)  0.090909  0.090909  \n",
              "(a,)     0.090909  0.090909  \n",
              "(this,)  0.100000  0.100000  \n",
              "(dog,)   0.100000  0.100000  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "print(\"bigram probabilities\")\n",
        "display(make_probability_matrix(bigram_counts, unique_words, k=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perplexity\n",
        "\n",
        "In this section, you will generate the perplexity score to evaluate your model on the test set. \n",
        "- You will also use back-off when needed. \n",
        "- Perplexity is used as an evaluation metric of your language model. \n",
        "- To calculate the perplexity score of the test set on an n-gram model, use: \n",
        "\n",
        "$$ PP(W) =\\sqrt[N]{ \\prod_{t=n+1}^N \\frac{1}{P(w_t | w_{t-n} \\cdots w_{t-1})} } \\tag{4}$$\n",
        "\n",
        "- where $N$ is the length of the sentence.\n",
        "- $n$ is the number of words in the n-gram (e.g. 2 for a bigram).\n",
        "- In math, the numbering starts at one and not zero.\n",
        "\n",
        "In code, array indexing starts at zero, so the code will use ranges for $t$ according to this formula:\n",
        "\n",
        "$$ PP(W) =\\sqrt[N]{ \\prod_{t=n}^{N-1} \\frac{1}{P(w_t | w_{t-n} \\cdots w_{t-1})} } \\tag{4.1}$$\n",
        "\n",
        "The higher the probabilities are, the lower the perplexity will be. \n",
        "- The more the n-grams tell us about the sentence, the lower the perplexity score will be. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, start_token='<s>', end_token='<e>', k=1.0):\n",
        "    \"\"\"\n",
        "    Calculate perplexity for a list of sentences\n",
        "    \n",
        "    Args:\n",
        "        sentence: List of strings\n",
        "        n_gram_counts: Dictionary of counts of n-grams\n",
        "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
        "        vocabulary_size: number of unique words in the vocabulary\n",
        "        k: Positive smoothing constant\n",
        "    \n",
        "    Returns:\n",
        "        Perplexity score\n",
        "    \"\"\"\n",
        "    # length of previous words\n",
        "    n = len(list(n_gram_counts.keys())[0]) \n",
        "    \n",
        "    # prepend <s> and append <e>\n",
        "    sentence = [start_token] * n + sentence + [end_token]\n",
        "    \n",
        "    # Cast the sentence from a list to a tuple\n",
        "    sentence = tuple(sentence)\n",
        "    \n",
        "    # length of sentence (after adding <s> and <e> tokens)\n",
        "    N = len(sentence)\n",
        "    \n",
        "    # The variable p will hold the product\n",
        "    # that is calculated inside the n-root\n",
        "    # Update this in the code below\n",
        "    product_pi = 1.0\n",
        "    \n",
        "    # Index t ranges from n to N - 1, inclusive on both ends\n",
        "    for t in range(n, N):\n",
        "\n",
        "        # get the n-gram preceding the word at position t\n",
        "        n_gram = sentence[t - n:t]\n",
        "        \n",
        "        # get the word at position t\n",
        "        word = sentence[t]\n",
        "        \n",
        "        # Estimate the probability of the word given the n-gram\n",
        "        # using the n-gram counts, n-plus1-gram counts,\n",
        "        # vocabulary size, and smoothing constant\n",
        "        probability = estimate_probability(word, n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k)\n",
        "        \n",
        "        # Update the product of the probabilities\n",
        "        # This 'product_pi' is a cumulative product \n",
        "        # of the (1/P) factors that are calculated in the loop\n",
        "        product_pi *= 1 / probability\n",
        "\n",
        "    # Take the Nth root of the product\n",
        "    perplexity = (product_pi)**(1/N)\n",
        "    \n",
        "    return perplexity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexity for first train sample: 2.8040\n",
            "Perplexity for test sample: 3.9654\n"
          ]
        }
      ],
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "\n",
        "\n",
        "perplexity_train = calculate_perplexity(sentences[0],\n",
        "                                         unigram_counts, bigram_counts,\n",
        "                                         len(unique_words), k=1.0)\n",
        "print(f\"Perplexity for first train sample: {perplexity_train:.4f}\")\n",
        "\n",
        "test_sentence = ['i', 'like', 'a', 'dog']\n",
        "perplexity_test = calculate_perplexity(test_sentence,\n",
        "                                       unigram_counts, bigram_counts,\n",
        "                                       len(unique_words), k=1.0)\n",
        "print(f\"Perplexity for test sample: {perplexity_test:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**suggest_a_word**\n",
        "\n",
        "Compute probabilities for all possible next words and suggest the most likely one.\n",
        "- This function also take an optional argument `start_with`, which specifies the first few letters of the next words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, end_token='<e>', unknown_token=\"<unk>\", k=1.0, start_with=None):\n",
        "    \"\"\"\n",
        "    Get suggestion for the next word\n",
        "    \n",
        "    Args:\n",
        "        previous_tokens: The sentence you input where each token is a word. Must have length >= n \n",
        "        n_gram_counts: Dictionary of counts of n-grams\n",
        "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
        "        vocabulary: List of words\n",
        "        k: positive constant, smoothing parameter\n",
        "        start_with: If not None, specifies the first few letters of the next word\n",
        "        \n",
        "    Returns:\n",
        "        A tuple of \n",
        "          - string of the most likely next word\n",
        "          - corresponding probability\n",
        "    \"\"\"\n",
        "    \n",
        "    # Length of previous words\n",
        "    n = len(list(n_gram_counts.keys())[0])\n",
        "    \n",
        "    # Append \"<s>\" tokens to previous tokens\n",
        "    previous_tokens = ['<s>'] * n + previous_tokens\n",
        "    \n",
        "    # Get the most recent 'n' words as the previous n-gram\n",
        "    previous_n_gram = previous_tokens[-n:]\n",
        "\n",
        "    # Estimate the probabilities that each word in the vocabulary\n",
        "    # is the next word, given the previous n-gram, the dictionary of n-gram counts,\n",
        "    # the dictionary of n plus 1 gram counts, and the smoothing constant\n",
        "    probabilities = estimate_probabilities(previous_n_gram,\n",
        "                                           n_gram_counts, n_plus1_gram_counts,\n",
        "                                           vocabulary, k=k)\n",
        "    \n",
        "    # Initialize suggested word and its corresponding probability\n",
        "    suggestion = None\n",
        "    max_prob = 0\n",
        "    \n",
        "    # Iterate over each word and its probability in the probabilities dictionary\n",
        "    for word, prob in probabilities.items():\n",
        "        \n",
        "        # If the optional start_with string is set\n",
        "        if start_with is not None:\n",
        "            \n",
        "            # Check if the beginning of word does not match with the letters in 'start_with'\n",
        "            if not word.startswith(start_with):\n",
        "                \n",
        "                # If they don't match, skip this word (move onto the next word)\n",
        "                continue\n",
        "        \n",
        "        # Check if this word's probability is greater than the current maximum probability\n",
        "        if prob > max_prob:\n",
        "            \n",
        "            # If so, update the suggestion and maximum probability\n",
        "            suggestion = word\n",
        "            max_prob = prob\n",
        "\n",
        "    return suggestion, max_prob\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The previous words are 'i like',\n",
            "\tand the suggested word is `my` with a probability of 0.2727\n",
            "\n",
            "The previous words are 'i like', the suggestion must start with `c`\n",
            "\tand the suggested word is `cat` with a probability of 0.0909\n"
          ]
        }
      ],
      "source": [
        "sentences = [['i', 'like','my', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'my', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "\n",
        "previous_tokens = [\"i\", \"like\"]\n",
        "tmp_suggest1 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)\n",
        "print(f\"The previous words are 'i like',\\n\\tand the suggested word is `{tmp_suggest1[0]}` with a probability of {tmp_suggest1[1]:.4f}\")\n",
        "\n",
        "print()\n",
        "# test your code when setting the starts_with\n",
        "tmp_starts_with = 'c'\n",
        "tmp_suggest2 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0, start_with=tmp_starts_with)\n",
        "print(f\"The previous words are 'i like', the suggestion must start with `{tmp_starts_with}`\\n\\tand the suggested word is `{tmp_suggest2[0]}` with a probability of {tmp_suggest2[1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Get multiple suggestions**\n",
        "\n",
        "The function defined below loop over varioud n-gram models to get multiple suggestions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
        "    model_counts = len(n_gram_counts_list)\n",
        "    suggestions = []\n",
        "    for i in range(model_counts-1):\n",
        "        n_gram_counts = n_gram_counts_list[i]\n",
        "        n_plus1_gram_counts = n_gram_counts_list[i+1]\n",
        "        \n",
        "        suggestion = suggest_a_word(previous_tokens, n_gram_counts,\n",
        "                                    n_plus1_gram_counts, vocabulary,\n",
        "                                    k=k, start_with=start_with)\n",
        "        suggestions.append(suggestion)\n",
        "    return suggestions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The previous words are 'i like', the suggestions are:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('a', 0.2727272727272727), ('a', 0.2), ('a', 0.2), ('a', 0.2)]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "trigram_counts = count_n_grams(sentences, 3)\n",
        "quadgram_counts = count_n_grams(sentences, 4)\n",
        "qintgram_counts = count_n_grams(sentences, 5)\n",
        "\n",
        "n_gram_counts_list = [unigram_counts, bigram_counts, trigram_counts, quadgram_counts, qintgram_counts]\n",
        "previous_tokens = [\"i\", \"like\"]\n",
        "tmp_suggest3 = get_suggestions(previous_tokens, n_gram_counts_list, unique_words, k=1.0)\n",
        "\n",
        "print(f\"The previous words are 'i like', the suggestions are:\")\n",
        "display(tmp_suggest3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Suggest multiple words using n-grams of varying length**\n",
        "\n",
        "Let's see this with n-grams of varying lengths (unigrams, bigrams, trigrams, 4-grams...6-grams),In the traning data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing n-gram counts with n = 1 ...\n",
            "Computing n-gram counts with n = 2 ...\n",
            "Computing n-gram counts with n = 3 ...\n",
            "Computing n-gram counts with n = 4 ...\n",
            "Computing n-gram counts with n = 5 ...\n"
          ]
        }
      ],
      "source": [
        "n_gram_counts_list = []\n",
        "for n in range(1, 6):\n",
        "    print(\"Computing n-gram counts with n =\", n, \"...\")\n",
        "    n_model_counts = count_n_grams(train_data_processed, n)\n",
        "    n_gram_counts_list.append(n_model_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "38368"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_data_processed) # 38368 Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The previous words are ['Can', 'you', 'please'], the suggestions are:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('follow', 0.00451009869926139),\n",
              " ('follow', 0.000673536741429245),\n",
              " ('i', 6.745362563237774e-05),\n",
              " ('i', 6.745362563237774e-05)]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "previous_tokens = [\"Can\",\"you\",\"please\"]\n",
        "tmp_suggest4 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
        "\n",
        "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
        "display(tmp_suggest4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NLP with Probabilistic models : CBOW model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to\n",
            "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from utils import get_batches, compute_pca, get_dict\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tokens: 60976 \n",
            " ['o', 'for', 'a', 'muse', 'of', 'fire', '.', 'that', 'would', 'ascend', 'the', 'brightest', 'heaven', 'of', 'invention']\n"
          ]
        }
      ],
      "source": [
        "# Load, tokenize and process the data\n",
        "import re                                                           #  Load the Regex-modul\n",
        "with open(r\"data\\NLP with Probabilistic models  CBOW model\\shakespeare.txt\") as f:\n",
        "    data = f.read()                                                 #  Read in the data\n",
        "data = re.sub(r'[,!?;-]', '.',data)                                 #  Punktuations are replaced by .\n",
        "data = nltk.word_tokenize(data)                                     #  Tokenize string to words\n",
        "data = [ ch.lower() for ch in data if ch.isalpha() or ch == '.']    #  Lower case and drop non-alphabetical tokens\n",
        "print(\"Number of tokens:\", len(data),'\\n', data[:15])               #  print data sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of vocabulary:  5775\n",
            "Most frequent tokens:  [('.', 9630), ('the', 1521), ('and', 1394), ('i', 1257), ('to', 1159), ('of', 1093), ('my', 857), ('that', 781), ('in', 770), ('a', 752), ('you', 748), ('is', 630), ('not', 559), ('for', 467), ('it', 460), ('with', 441), ('his', 434), ('but', 417), ('me', 417), ('your', 397)]\n"
          ]
        }
      ],
      "source": [
        "# Compute the frequency distribution of the words in the dataset (vocabulary)\n",
        "fdist = nltk.FreqDist(word for word in data)\n",
        "print(\"Size of vocabulary: \",len(fdist) )\n",
        "print(\"Most frequent tokens: \",fdist.most_common(20) ) # print the 20 most frequent words and their freq."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of vocabulary:  5775\n"
          ]
        }
      ],
      "source": [
        "# get_dict creates two dictionaries, converting words to indices and viceversa.\n",
        "word2Ind, Ind2word = get_dict(data)\n",
        "V = len(word2Ind)\n",
        "print(\"Size of vocabulary: \", V)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index of the word 'king' :   2744\n",
            "Word which has index 2743:   kinds\n"
          ]
        }
      ],
      "source": [
        "# example of word to index mapping\n",
        "print(\"Index of the word 'king' :  \",word2Ind['king'] )\n",
        "print(\"Word which has index 2743:  \",Ind2word[2743] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initializing the Model\n",
        "ou will now initialize two matrices and two vectors. \n",
        "- The first matrix ($W_1$) is of dimension $N \\times V$, where $V$ is the number of words in your vocabulary and $N$ is the dimension of your word vector.\n",
        "- The second matrix ($W_2$) is of dimension $V \\times N$. \n",
        "- Vector $b_1$ has dimensions $N\\times 1$\n",
        "- Vector $b_2$ has dimensions  $V\\times 1$. \n",
        "- $b_1$ and $b_2$ are the bias vectors of the linear layers from matrices $W_1$ and $W_2$.\n",
        "\n",
        "The overall structure of the model will look as in Figure 1, but at this stage we are just initializing the parameters. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_model(N,V, random_seed=1):\n",
        "    '''\n",
        "    Inputs: \n",
        "        N:  dimension of hidden vector \n",
        "        V:  dimension of vocabulary\n",
        "        random_seed: random seed for consistent results in the unit tests\n",
        "     Outputs: \n",
        "        W1, W2, b1, b2: initialized weights and biases\n",
        "    '''\n",
        "    np.random.seed(random_seed)\n",
        "    # W1 has shape (N,V)\n",
        "    W1 = np.random.rand(N,V)\n",
        "    \n",
        "    # W2 has shape (V,N)\n",
        "    W2 = np.random.rand(V,N)\n",
        "    \n",
        "    # b1 has shape (N,1)\n",
        "    b1 = np.random.rand(N,1)\n",
        "    \n",
        "    # b2 has shape (V,1)\n",
        "    b2 = np.random.rand(V,1)\n",
        "    return W1, W2, b1, b2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tmp_W1.shape: (4, 10)\n",
            "tmp_W2.shape: (10, 4)\n",
            "tmp_b1.shape: (4, 1)\n",
            "tmp_b2.shape: (10, 1)\n"
          ]
        }
      ],
      "source": [
        "# Exemple\n",
        "tmp_N = 4\n",
        "tmp_V = 10\n",
        "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n",
        "assert tmp_W1.shape == ((tmp_N,tmp_V))\n",
        "assert tmp_W2.shape == ((tmp_V,tmp_N))\n",
        "print(f\"tmp_W1.shape: {tmp_W1.shape}\")\n",
        "print(f\"tmp_W2.shape: {tmp_W2.shape}\")\n",
        "print(f\"tmp_b1.shape: {tmp_b1.shape}\")\n",
        "print(f\"tmp_b2.shape: {tmp_b2.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def softmax(z):\n",
        "    '''\n",
        "    Inputs: \n",
        "        z: output scores from the hidden layer\n",
        "    Outputs: \n",
        "        yhat: prediction (estimate of y)\n",
        "    '''\n",
        "    # Calculate exponential of z\n",
        "    exp_z = np.exp(z)\n",
        "    \n",
        "    # Calculate sum of exponential of z for each example\n",
        "    sum_exp_z = np.sum(exp_z, axis=0, keepdims=True)\n",
        "    \n",
        "    # Calculate softmax\n",
        "    yhat = exp_z / sum_exp_z\n",
        "    \n",
        "    return yhat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.5       , 0.73105858, 0.88079708],\n",
              "       [0.5       , 0.26894142, 0.11920292]])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Example :\n",
        "tmp = np.array([[1,2,3],\n",
        "                [1,1,1]\n",
        "               ])\n",
        "tmp_sm = softmax(tmp)\n",
        "display(tmp_sm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## forward_prop\n",
        "\n",
        "\\begin{align}\n",
        " h &= W_1 \\  X + b_1  \\tag{1} \\\\\n",
        " h &= ReLU(h)  \\tag{2} \\\\\n",
        " z &= W_2 \\  h + b_2   \\tag{3} \\\\\n",
        "\\end{align}\n",
        "\n",
        "$$f(h)=\\max (0,h) \\tag{6}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def forward_prop(x, W1, W2, b1, b2):\n",
        "    '''\n",
        "    Inputs: \n",
        "        x: average one hot vector for the context \n",
        "        W1, W2, b1, b2: matrices and biases to be learned\n",
        "    Outputs: \n",
        "        z: output score vector\n",
        "    '''\n",
        "    \n",
        "    # Calculate h\n",
        "    h = np.dot(W1, x) + b1\n",
        "  \n",
        "    # Apply the relu on h, \n",
        "    # store the relu in h\n",
        "    h = np.maximum(0, h)\n",
        "    # Calculate z\n",
        "    z = np.dot(W2, h) + b2\n",
        "\n",
        "    return z, h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x has shape (3, 1)\n",
            "N is 2 and vocabulary size V is 3\n",
            "call forward_prop\n",
            "\n",
            "z has shape (3, 1)\n",
            "z has values:\n",
            "[[0.55379268]\n",
            " [1.58960774]\n",
            " [1.50722933]]\n",
            "h has shape (2, 1)\n",
            "h has values:\n",
            "[[0.92477674]\n",
            " [1.02487333]]\n"
          ]
        }
      ],
      "source": [
        "# Create some inputs\n",
        "tmp_N = 2\n",
        "tmp_V = 3\n",
        "tmp_x = np.array([[0,1,0]]).T\n",
        "#print(tmp_x)\n",
        "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(N=tmp_N,V=tmp_V, random_seed=1)\n",
        "\n",
        "print(f\"x has shape {tmp_x.shape}\")\n",
        "print(f\"N is {tmp_N} and vocabulary size V is {tmp_V}\")\n",
        "\n",
        "# call function\n",
        "tmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n",
        "print(\"call forward_prop\")\n",
        "print()\n",
        "# Look at output\n",
        "print(f\"z has shape {tmp_z.shape}\")\n",
        "print(\"z has values:\")\n",
        "print(tmp_z)\n",
        "\n",
        "print(f\"h has shape {tmp_h.shape}\")\n",
        "print(\"h has values:\")\n",
        "print(tmp_h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute_cost: cross-entropy cost function\n",
        "def compute_cost(y, yhat, batch_size):\n",
        "\n",
        "    # cost function \n",
        "    logprobs = np.multiply(np.log(yhat),y)\n",
        "    cost = - 1/batch_size * np.sum(logprobs)\n",
        "    cost = np.squeeze(cost)\n",
        "    return cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tmp_x.shape (5775, 4)\n",
            "tmp_y.shape (5775, 4)\n",
            "tmp_W1.shape (50, 5775)\n",
            "tmp_W2.shape (5775, 50)\n",
            "tmp_b1.shape (50, 1)\n",
            "tmp_b2.shape (5775, 1)\n",
            "tmp_z.shape: (5775, 4)\n",
            "tmp_h.shape: (50, 4)\n",
            "tmp_yhat.shape: (5775, 4)\n",
            "call compute_cost\n",
            "tmp_cost 10.3511\n"
          ]
        }
      ],
      "source": [
        "# Example\n",
        "# Test the function\n",
        "tmp_C = 2\n",
        "tmp_N = 50\n",
        "tmp_batch_size = 4\n",
        "tmp_word2Ind, tmp_Ind2word = get_dict(data)\n",
        "tmp_V = len(word2Ind)\n",
        "\n",
        "tmp_x, tmp_y = next(get_batches(data, tmp_word2Ind, tmp_V,tmp_C, tmp_batch_size))\n",
        "        \n",
        "print(f\"tmp_x.shape {tmp_x.shape}\")\n",
        "print(f\"tmp_y.shape {tmp_y.shape}\")\n",
        "\n",
        "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n",
        "\n",
        "print(f\"tmp_W1.shape {tmp_W1.shape}\")\n",
        "print(f\"tmp_W2.shape {tmp_W2.shape}\")\n",
        "print(f\"tmp_b1.shape {tmp_b1.shape}\")\n",
        "print(f\"tmp_b2.shape {tmp_b2.shape}\")\n",
        "\n",
        "tmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n",
        "print(f\"tmp_z.shape: {tmp_z.shape}\")\n",
        "print(f\"tmp_h.shape: {tmp_h.shape}\")\n",
        "\n",
        "tmp_yhat = softmax(tmp_z)\n",
        "print(f\"tmp_yhat.shape: {tmp_yhat.shape}\")\n",
        "\n",
        "tmp_cost = compute_cost(tmp_y, tmp_yhat, tmp_batch_size)\n",
        "print(\"call compute_cost\")\n",
        "print(f\"tmp_cost {tmp_cost:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backward_prop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size):\n",
        "    '''\n",
        "    Inputs: \n",
        "        x: average one hot vector for the context \n",
        "        yhat: prediction (estimate of y)\n",
        "        y: target vector\n",
        "        h: hidden vector (see eq. 1)\n",
        "        W1, W2, b1, b2: matrices and biases  \n",
        "        batch_size: batch size \n",
        "    Outputs: \n",
        "        grad_W1, grad_W2, grad_b1, grad_b2: gradients of matrices and biases   \n",
        "    '''\n",
        "    \n",
        "    # Compute z1 as \"W1⋅x + b1\"\n",
        "    z1 = np.dot(W1, x) + b1\n",
        "    \n",
        "    # Compute l1 as W2^T (Yhat - Y)\n",
        "    l1 = np.dot(W2.T, (yhat - y))\n",
        "    \n",
        "    # if z1 < 0, then l1 = 0\n",
        "    # otherwise l1 = l1\n",
        "    l1[z1 < 0] = 0\n",
        "    \n",
        "    # compute the gradient for W1\n",
        "    grad_W1 = np.dot(l1, x.T) / batch_size\n",
        "\n",
        "    # Compute gradient of W2\n",
        "    grad_W2 = np.dot(yhat - y, h.T) / batch_size\n",
        "    \n",
        "    # compute gradient for b1\n",
        "    grad_b1 = np.sum(l1, axis=1, keepdims=True) / batch_size\n",
        "\n",
        "    # compute gradient for b2\n",
        "    grad_b2 = np.sum(yhat - y, axis=1, keepdims=True) / batch_size\n",
        "    \n",
        "    return grad_W1, grad_W2, grad_b1, grad_b2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get a batch of data\n",
            "tmp_x.shape (5775, 4)\n",
            "tmp_y.shape (5775, 4)\n",
            "\n",
            "Initialize weights and biases\n",
            "tmp_W1.shape (50, 5775)\n",
            "tmp_W2.shape (5775, 50)\n",
            "tmp_b1.shape (50, 1)\n",
            "tmp_b2.shape (5775, 1)\n",
            "\n",
            "Forwad prop to get z and h\n",
            "tmp_z.shape: (5775, 4)\n",
            "tmp_h.shape: (50, 4)\n",
            "\n",
            "Get yhat by calling softmax\n",
            "tmp_yhat.shape: (5775, 4)\n",
            "\n",
            "call back_prop\n",
            "tmp_grad_W1.shape (50, 5775)\n",
            "tmp_grad_W2.shape (5775, 50)\n",
            "tmp_grad_b1.shape (50, 1)\n",
            "tmp_grad_b2.shape (5775, 1)\n"
          ]
        }
      ],
      "source": [
        "# Example\n",
        "tmp_C = 2\n",
        "tmp_N = 50\n",
        "tmp_batch_size = 4\n",
        "tmp_word2Ind, tmp_Ind2word = get_dict(data)\n",
        "tmp_V = len(word2Ind)\n",
        "\n",
        "\n",
        "# get a batch of data\n",
        "tmp_x, tmp_y = next(get_batches(data, tmp_word2Ind, tmp_V,tmp_C, tmp_batch_size))\n",
        "\n",
        "print(\"get a batch of data\")\n",
        "print(f\"tmp_x.shape {tmp_x.shape}\")\n",
        "print(f\"tmp_y.shape {tmp_y.shape}\")\n",
        "\n",
        "print()\n",
        "print(\"Initialize weights and biases\")\n",
        "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n",
        "\n",
        "print(f\"tmp_W1.shape {tmp_W1.shape}\")\n",
        "print(f\"tmp_W2.shape {tmp_W2.shape}\")\n",
        "print(f\"tmp_b1.shape {tmp_b1.shape}\")\n",
        "print(f\"tmp_b2.shape {tmp_b2.shape}\")\n",
        "\n",
        "print()\n",
        "print(\"Forwad prop to get z and h\")\n",
        "tmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n",
        "print(f\"tmp_z.shape: {tmp_z.shape}\")\n",
        "print(f\"tmp_h.shape: {tmp_h.shape}\")\n",
        "\n",
        "print()\n",
        "print(\"Get yhat by calling softmax\")\n",
        "tmp_yhat = softmax(tmp_z)\n",
        "print(f\"tmp_yhat.shape: {tmp_yhat.shape}\")\n",
        "\n",
        "tmp_m = (2*tmp_C)\n",
        "tmp_grad_W1, tmp_grad_W2, tmp_grad_b1, tmp_grad_b2 = back_prop(tmp_x, tmp_yhat, tmp_y, tmp_h, tmp_W1, tmp_W2, tmp_b1, tmp_b2, tmp_batch_size)\n",
        "\n",
        "print()\n",
        "print(\"call back_prop\")\n",
        "print(f\"tmp_grad_W1.shape {tmp_grad_W1.shape}\")\n",
        "print(f\"tmp_grad_W2.shape {tmp_grad_W2.shape}\")\n",
        "print(f\"tmp_grad_b1.shape {tmp_grad_b1.shape}\")\n",
        "print(f\"tmp_grad_b2.shape {tmp_grad_b2.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradient_descent(data, word2Ind, N, V, num_iters, alpha=0.03, \n",
        "                     random_seed=282, initialize_model=initialize_model, \n",
        "                     get_batches=get_batches, forward_prop=forward_prop, \n",
        "                     softmax=softmax, compute_cost=compute_cost, \n",
        "                     back_prop=back_prop):\n",
        "    \n",
        "    '''\n",
        "    This is the gradient_descent function\n",
        "    \n",
        "      Inputs: \n",
        "        data: text\n",
        "        word2Ind: words to Indices\n",
        "        N: dimension of hidden vector  \n",
        "        V: dimension of vocabulary \n",
        "        num_iters: number of iterations  \n",
        "        random_seed: random seed to initialize the model's matrices and vectors\n",
        "        initialize_model: your implementation of the function to initialize the model\n",
        "        get_batches: function to get the data in batches\n",
        "        forward_prop: your implementation of the function to perform forward propagation\n",
        "        softmax: your implementation of the softmax function\n",
        "        compute_cost: cost function (Cross entropy)\n",
        "        back_prop: your implementation of the function to perform backward propagation\n",
        "     Outputs: \n",
        "        W1, W2, b1, b2: updated matrices and biases after num_iters iterations\n",
        "\n",
        "    '''\n",
        "    W1, W2, b1, b2 = initialize_model(N,V, random_seed=random_seed) # W1=(N,V) and W2=(V,N)\n",
        "\n",
        "    batch_size = 128\n",
        "    iters = 0\n",
        "    C = 2 \n",
        "    \n",
        "    for x, y in get_batches(data, word2Ind, V, C, batch_size):\n",
        "        # get z and h\n",
        "        z, h = forward_prop(x, W1, W2, b1, b2)\n",
        "                \n",
        "        # get yhat\n",
        "        yhat = softmax(z)\n",
        "        \n",
        "        # get cost\n",
        "        cost = compute_cost(y, yhat, batch_size)\n",
        "        if ( (iters+1) % 10 == 0):\n",
        "            print(f\"iters: {iters + 1} cost: {cost:.6f}\")\n",
        "            \n",
        "        # get gradients\n",
        "        grad_W1, grad_W2, grad_b1, grad_b2 = back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size)\n",
        "        \n",
        "        # update weights and biases\n",
        "        W1 -= alpha * grad_W1\n",
        "        W2 -= alpha * grad_W2\n",
        "        b1 -= alpha * grad_b1\n",
        "        b2 -= alpha * grad_b2\n",
        "\n",
        "        iters +=1 \n",
        "        if iters == num_iters: \n",
        "            break\n",
        "        if iters % 100 == 0:\n",
        "            alpha *= 0.66\n",
        "            \n",
        "    return W1, W2, b1, b2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Call gradient_descent\n",
            "iters: 10 cost: 9.513211\n",
            "iters: 20 cost: 9.476105\n",
            "iters: 30 cost: 9.543887\n",
            "iters: 40 cost: 9.370119\n",
            "iters: 50 cost: 8.752134\n",
            "iters: 60 cost: 8.645475\n",
            "iters: 70 cost: 8.777353\n",
            "iters: 80 cost: 8.683993\n",
            "iters: 90 cost: 8.785594\n",
            "iters: 100 cost: 8.286367\n",
            "iters: 110 cost: 8.527792\n",
            "iters: 120 cost: 8.414913\n",
            "iters: 130 cost: 8.099485\n",
            "iters: 140 cost: 8.282044\n",
            "iters: 150 cost: 8.239267\n"
          ]
        }
      ],
      "source": [
        "C = 2\n",
        "N = 50\n",
        "word2Ind, Ind2word = get_dict(data)\n",
        "V = len(word2Ind)\n",
        "num_iters = 150\n",
        "print(\"Call gradient_descent\")\n",
        "W1, W2, b1, b2 = gradient_descent(data, word2Ind, N, V, num_iters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing the Word Vectors :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(16, 50) [2744, 3949, 2960, 3022, 5672, 1452, 5671, 4189, 2315, 4276, 2974, 4153, 407, 740, 1452, 2058]\n"
          ]
        }
      ],
      "source": [
        "# visualizing the word vectors here\n",
        "from matplotlib import pyplot\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "words = ['king', 'queen','lord','man', 'woman','dog','wolf',\n",
        "         'rich','happy','sad','love','respect','beautiful','cat','dog','fox']\n",
        "\n",
        "embs = (W1.T + W2)/2.0\n",
        " \n",
        "# given a list of words and the embeddings, it returns a matrix with all the embeddings\n",
        "idx = [word2Ind[word] for word in words]\n",
        "X = embs[idx, :]\n",
        "print(X.shape, idx)  # X.shape:  Number of words of dimension N each "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/svg+xml": [
              "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
              "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
              "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
              "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"413.901335pt\" height=\"297.190125pt\" viewBox=\"0 0 413.901335 297.190125\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
              " <metadata>\n",
              "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
              "   <cc:Work>\n",
              "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
              "    <dc:date>2024-04-28T00:49:42.235560</dc:date>\n",
              "    <dc:format>image/svg+xml</dc:format>\n",
              "    <dc:creator>\n",
              "     <cc:Agent>\n",
              "      <dc:title>Matplotlib v3.8.2, https://matplotlib.org/</dc:title>\n",
              "     </cc:Agent>\n",
              "    </dc:creator>\n",
              "   </cc:Work>\n",
              "  </rdf:RDF>\n",
              " </metadata>\n",
              " <defs>\n",
              "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
              " </defs>\n",
              " <g id=\"figure_1\">\n",
              "  <g id=\"patch_1\">\n",
              "   <path d=\"M 0 297.190125 \n",
              "L 413.901335 297.190125 \n",
              "L 413.901335 0 \n",
              "L 0 0 \n",
              "z\n",
              "\" style=\"fill: #ffffff\"/>\n",
              "  </g>\n",
              "  <g id=\"axes_1\">\n",
              "   <g id=\"patch_2\">\n",
              "    <path d=\"M 44.845313 273.312 \n",
              "L 401.965312 273.312 \n",
              "L 401.965312 7.2 \n",
              "L 44.845313 7.2 \n",
              "z\n",
              "\" style=\"fill: #ffffff\"/>\n",
              "   </g>\n",
              "   <g id=\"PathCollection_1\">\n",
              "    <defs>\n",
              "     <path id=\"m0e8670d786\" d=\"M 0 3 \n",
              "C 0.795609 3 1.55874 2.683901 2.12132 2.12132 \n",
              "C 2.683901 1.55874 3 0.795609 3 0 \n",
              "C 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \n",
              "C 1.55874 -2.683901 0.795609 -3 0 -3 \n",
              "C -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \n",
              "C -2.683901 -1.55874 -3 -0.795609 -3 0 \n",
              "C -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \n",
              "C -1.55874 2.683901 -0.795609 3 0 3 \n",
              "z\n",
              "\" style=\"stroke: #1f77b4\"/>\n",
              "    </defs>\n",
              "    <g clip-path=\"url(#pfeca47dad1)\">\n",
              "     <use xlink:href=\"#m0e8670d786\" x=\"348.506191\" y=\"98.66082\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
              "     <use xlink:href=\"#m0e8670d786\" x=\"294.588439\" y=\"161.414304\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
              "     <use xlink:href=\"#m0e8670d786\" x=\"314.312158\" y=\"182.567749\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
              "     <use xlink:href=\"#m0e8670d786\" x=\"238.633449\" y=\"82.162517\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
              "     <use xlink:href=\"#m0e8670d786\" x=\"279.493219\" y=\"55.220861\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
              "     <use xlink:href=\"#m0e8670d786\" x=\"61.07804\" y=\"151.84084\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
              "     <use xlink:href=\"#m0e8670d786\" x=\"253.767607\" y=\"63.811527\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
              "     <use xlink:href=\"#m0e8670d786\" x=\"314.851688\" y=\"44.097533\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
              "     <use xlink:href=\"#m0e8670d786\" x=\"286.822223\" y=\"203.362065\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
              "     <use xlink:href=\"#m0e8670d786\" x=\"338.244757\" y=\"208.14271\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
              "     <use xlink:href=\"#m0e8670d786\" x=\"385.732585\" y=\"47.569035\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
              "     <use xlink:href=\"#m0e8670d786\" x=\"327.360768\" y=\"95.812971\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
              "     <use xlink:href=\"#m0e8670d786\" x=\"349.917043\" y=\"155.279821\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
              "     <use xlink:href=\"#m0e8670d786\" x=\"344.488457\" y=\"261.216\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
              "     <use xlink:href=\"#m0e8670d786\" x=\"61.07804\" y=\"151.84084\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
              "     <use xlink:href=\"#m0e8670d786\" x=\"244.158454\" y=\"19.296\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
              "    </g>\n",
              "   </g>\n",
              "   <g id=\"matplotlib.axis_1\">\n",
              "    <g id=\"xtick_1\">\n",
              "     <g id=\"line2d_1\">\n",
              "      <defs>\n",
              "       <path id=\"mc574b5a415\" d=\"M 0 0 \n",
              "L 0 3.5 \n",
              "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </defs>\n",
              "      <g>\n",
              "       <use xlink:href=\"#mc574b5a415\" x=\"64.352625\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_1\">\n",
              "      <!-- −1.25 -->\n",
              "      <g transform=\"translate(49.029969 287.910437) scale(0.1 -0.1)\">\n",
              "       <defs>\n",
              "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
              "L 4684 2272 \n",
              "L 4684 1741 \n",
              "L 678 1741 \n",
              "L 678 2272 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
              "L 1825 531 \n",
              "L 1825 4091 \n",
              "L 703 3866 \n",
              "L 703 4441 \n",
              "L 1819 4666 \n",
              "L 2450 4666 \n",
              "L 2450 531 \n",
              "L 3481 531 \n",
              "L 3481 0 \n",
              "L 794 0 \n",
              "L 794 531 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
              "L 1344 794 \n",
              "L 1344 0 \n",
              "L 684 0 \n",
              "L 684 794 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
              "L 3431 531 \n",
              "L 3431 0 \n",
              "L 469 0 \n",
              "L 469 531 \n",
              "Q 828 903 1448 1529 \n",
              "Q 2069 2156 2228 2338 \n",
              "Q 2531 2678 2651 2914 \n",
              "Q 2772 3150 2772 3378 \n",
              "Q 2772 3750 2511 3984 \n",
              "Q 2250 4219 1831 4219 \n",
              "Q 1534 4219 1204 4116 \n",
              "Q 875 4013 500 3803 \n",
              "L 500 4441 \n",
              "Q 881 4594 1212 4672 \n",
              "Q 1544 4750 1819 4750 \n",
              "Q 2544 4750 2975 4387 \n",
              "Q 3406 4025 3406 3419 \n",
              "Q 3406 3131 3298 2873 \n",
              "Q 3191 2616 2906 2266 \n",
              "Q 2828 2175 2409 1742 \n",
              "Q 1991 1309 1228 531 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
              "L 3169 4666 \n",
              "L 3169 4134 \n",
              "L 1269 4134 \n",
              "L 1269 2991 \n",
              "Q 1406 3038 1543 3061 \n",
              "Q 1681 3084 1819 3084 \n",
              "Q 2600 3084 3056 2656 \n",
              "Q 3513 2228 3513 1497 \n",
              "Q 3513 744 3044 326 \n",
              "Q 2575 -91 1722 -91 \n",
              "Q 1428 -91 1123 -41 \n",
              "Q 819 9 494 109 \n",
              "L 494 744 \n",
              "Q 775 591 1075 516 \n",
              "Q 1375 441 1709 441 \n",
              "Q 2250 441 2565 725 \n",
              "Q 2881 1009 2881 1497 \n",
              "Q 2881 1984 2565 2268 \n",
              "Q 2250 2553 1709 2553 \n",
              "Q 1456 2553 1204 2497 \n",
              "Q 953 2441 691 2322 \n",
              "L 691 4666 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       </defs>\n",
              "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-32\" x=\"179.199219\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-35\" x=\"242.822266\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"xtick_2\">\n",
              "     <g id=\"line2d_2\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#mc574b5a415\" x=\"107.020014\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_2\">\n",
              "      <!-- −1.00 -->\n",
              "      <g transform=\"translate(91.697358 287.910437) scale(0.1 -0.1)\">\n",
              "       <defs>\n",
              "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
              "Q 1547 4250 1301 3770 \n",
              "Q 1056 3291 1056 2328 \n",
              "Q 1056 1369 1301 889 \n",
              "Q 1547 409 2034 409 \n",
              "Q 2525 409 2770 889 \n",
              "Q 3016 1369 3016 2328 \n",
              "Q 3016 3291 2770 3770 \n",
              "Q 2525 4250 2034 4250 \n",
              "z\n",
              "M 2034 4750 \n",
              "Q 2819 4750 3233 4129 \n",
              "Q 3647 3509 3647 2328 \n",
              "Q 3647 1150 3233 529 \n",
              "Q 2819 -91 2034 -91 \n",
              "Q 1250 -91 836 529 \n",
              "Q 422 1150 422 2328 \n",
              "Q 422 3509 836 4129 \n",
              "Q 1250 4750 2034 4750 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       </defs>\n",
              "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"179.199219\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"242.822266\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"xtick_3\">\n",
              "     <g id=\"line2d_3\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#mc574b5a415\" x=\"149.687403\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_3\">\n",
              "      <!-- −0.75 -->\n",
              "      <g transform=\"translate(134.364747 287.910437) scale(0.1 -0.1)\">\n",
              "       <defs>\n",
              "        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \n",
              "L 3525 4666 \n",
              "L 3525 4397 \n",
              "L 1831 0 \n",
              "L 1172 0 \n",
              "L 2766 4134 \n",
              "L 525 4134 \n",
              "L 525 4666 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       </defs>\n",
              "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-37\" x=\"179.199219\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-35\" x=\"242.822266\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"xtick_4\">\n",
              "     <g id=\"line2d_4\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#mc574b5a415\" x=\"192.354792\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_4\">\n",
              "      <!-- −0.50 -->\n",
              "      <g transform=\"translate(177.032136 287.910437) scale(0.1 -0.1)\">\n",
              "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"242.822266\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"xtick_5\">\n",
              "     <g id=\"line2d_5\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#mc574b5a415\" x=\"235.022181\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_5\">\n",
              "      <!-- −0.25 -->\n",
              "      <g transform=\"translate(219.699525 287.910437) scale(0.1 -0.1)\">\n",
              "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-32\" x=\"179.199219\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-35\" x=\"242.822266\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"xtick_6\">\n",
              "     <g id=\"line2d_6\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#mc574b5a415\" x=\"277.68957\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_6\">\n",
              "      <!-- 0.00 -->\n",
              "      <g transform=\"translate(266.556757 287.910437) scale(0.1 -0.1)\">\n",
              "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"xtick_7\">\n",
              "     <g id=\"line2d_7\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#mc574b5a415\" x=\"320.356959\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_7\">\n",
              "      <!-- 0.25 -->\n",
              "      <g transform=\"translate(309.224146 287.910437) scale(0.1 -0.1)\">\n",
              "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"xtick_8\">\n",
              "     <g id=\"line2d_8\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#mc574b5a415\" x=\"363.024348\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_8\">\n",
              "      <!-- 0.50 -->\n",
              "      <g transform=\"translate(351.891535 287.910437) scale(0.1 -0.1)\">\n",
              "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "   </g>\n",
              "   <g id=\"matplotlib.axis_2\">\n",
              "    <g id=\"ytick_1\">\n",
              "     <g id=\"line2d_9\">\n",
              "      <defs>\n",
              "       <path id=\"m5cece3109a\" d=\"M 0 0 \n",
              "L -3.5 0 \n",
              "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </defs>\n",
              "      <g>\n",
              "       <use xlink:href=\"#m5cece3109a\" x=\"44.845313\" y=\"266.086003\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_9\">\n",
              "      <!-- −1.00 -->\n",
              "      <g transform=\"translate(7.2 269.885222) scale(0.1 -0.1)\">\n",
              "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"179.199219\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"242.822266\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"ytick_2\">\n",
              "     <g id=\"line2d_10\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#m5cece3109a\" x=\"44.845313\" y=\"230.537871\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_10\">\n",
              "      <!-- −0.75 -->\n",
              "      <g transform=\"translate(7.2 234.33709) scale(0.1 -0.1)\">\n",
              "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-37\" x=\"179.199219\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-35\" x=\"242.822266\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"ytick_3\">\n",
              "     <g id=\"line2d_11\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#m5cece3109a\" x=\"44.845313\" y=\"194.989739\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_11\">\n",
              "      <!-- −0.50 -->\n",
              "      <g transform=\"translate(7.2 198.788958) scale(0.1 -0.1)\">\n",
              "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"242.822266\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"ytick_4\">\n",
              "     <g id=\"line2d_12\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#m5cece3109a\" x=\"44.845313\" y=\"159.441607\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_12\">\n",
              "      <!-- −0.25 -->\n",
              "      <g transform=\"translate(7.2 163.240825) scale(0.1 -0.1)\">\n",
              "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-32\" x=\"179.199219\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-35\" x=\"242.822266\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"ytick_5\">\n",
              "     <g id=\"line2d_13\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#m5cece3109a\" x=\"44.845313\" y=\"123.893475\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_13\">\n",
              "      <!-- 0.00 -->\n",
              "      <g transform=\"translate(15.579688 127.692693) scale(0.1 -0.1)\">\n",
              "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"ytick_6\">\n",
              "     <g id=\"line2d_14\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#m5cece3109a\" x=\"44.845313\" y=\"88.345342\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_14\">\n",
              "      <!-- 0.25 -->\n",
              "      <g transform=\"translate(15.579688 92.144561) scale(0.1 -0.1)\">\n",
              "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"ytick_7\">\n",
              "     <g id=\"line2d_15\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#m5cece3109a\" x=\"44.845313\" y=\"52.79721\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_15\">\n",
              "      <!-- 0.50 -->\n",
              "      <g transform=\"translate(15.579688 56.596429) scale(0.1 -0.1)\">\n",
              "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"ytick_8\">\n",
              "     <g id=\"line2d_16\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#m5cece3109a\" x=\"44.845313\" y=\"17.249078\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_16\">\n",
              "      <!-- 0.75 -->\n",
              "      <g transform=\"translate(15.579688 21.048297) scale(0.1 -0.1)\">\n",
              "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-37\" x=\"95.410156\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "   </g>\n",
              "   <g id=\"patch_3\">\n",
              "    <path d=\"M 44.845313 273.312 \n",
              "L 44.845313 7.2 \n",
              "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
              "   </g>\n",
              "   <g id=\"patch_4\">\n",
              "    <path d=\"M 401.965312 273.312 \n",
              "L 401.965312 7.2 \n",
              "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
              "   </g>\n",
              "   <g id=\"patch_5\">\n",
              "    <path d=\"M 44.845312 273.312 \n",
              "L 401.965312 273.312 \n",
              "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
              "   </g>\n",
              "   <g id=\"patch_6\">\n",
              "    <path d=\"M 44.845312 7.2 \n",
              "L 401.965312 7.2 \n",
              "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
              "   </g>\n",
              "   <g id=\"text_17\">\n",
              "    <!-- king -->\n",
              "    <g transform=\"translate(348.506191 98.66082) scale(0.1 -0.1)\">\n",
              "     <defs>\n",
              "      <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \n",
              "L 1159 4863 \n",
              "L 1159 1991 \n",
              "L 2875 3500 \n",
              "L 3609 3500 \n",
              "L 1753 1863 \n",
              "L 3688 0 \n",
              "L 2938 0 \n",
              "L 1159 1709 \n",
              "L 1159 0 \n",
              "L 581 0 \n",
              "L 581 4863 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "      <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
              "L 1178 3500 \n",
              "L 1178 0 \n",
              "L 603 0 \n",
              "L 603 3500 \n",
              "z\n",
              "M 603 4863 \n",
              "L 1178 4863 \n",
              "L 1178 4134 \n",
              "L 603 4134 \n",
              "L 603 4863 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "      <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
              "L 3513 0 \n",
              "L 2938 0 \n",
              "L 2938 2094 \n",
              "Q 2938 2591 2744 2837 \n",
              "Q 2550 3084 2163 3084 \n",
              "Q 1697 3084 1428 2787 \n",
              "Q 1159 2491 1159 1978 \n",
              "L 1159 0 \n",
              "L 581 0 \n",
              "L 581 3500 \n",
              "L 1159 3500 \n",
              "L 1159 2956 \n",
              "Q 1366 3272 1645 3428 \n",
              "Q 1925 3584 2291 3584 \n",
              "Q 2894 3584 3203 3211 \n",
              "Q 3513 2838 3513 2113 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "      <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \n",
              "Q 2906 2416 2648 2759 \n",
              "Q 2391 3103 1925 3103 \n",
              "Q 1463 3103 1205 2759 \n",
              "Q 947 2416 947 1791 \n",
              "Q 947 1169 1205 825 \n",
              "Q 1463 481 1925 481 \n",
              "Q 2391 481 2648 825 \n",
              "Q 2906 1169 2906 1791 \n",
              "z\n",
              "M 3481 434 \n",
              "Q 3481 -459 3084 -895 \n",
              "Q 2688 -1331 1869 -1331 \n",
              "Q 1566 -1331 1297 -1286 \n",
              "Q 1028 -1241 775 -1147 \n",
              "L 775 -588 \n",
              "Q 1028 -725 1275 -790 \n",
              "Q 1522 -856 1778 -856 \n",
              "Q 2344 -856 2625 -561 \n",
              "Q 2906 -266 2906 331 \n",
              "L 2906 616 \n",
              "Q 2728 306 2450 153 \n",
              "Q 2172 0 1784 0 \n",
              "Q 1141 0 747 490 \n",
              "Q 353 981 353 1791 \n",
              "Q 353 2603 747 3093 \n",
              "Q 1141 3584 1784 3584 \n",
              "Q 2172 3584 2450 3431 \n",
              "Q 2728 3278 2906 2969 \n",
              "L 2906 3500 \n",
              "L 3481 3500 \n",
              "L 3481 434 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "     </defs>\n",
              "     <use xlink:href=\"#DejaVuSans-6b\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-69\" x=\"57.910156\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-6e\" x=\"85.693359\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-67\" x=\"149.072266\"/>\n",
              "    </g>\n",
              "   </g>\n",
              "   <g id=\"text_18\">\n",
              "    <!-- queen -->\n",
              "    <g transform=\"translate(294.588439 161.414304) scale(0.1 -0.1)\">\n",
              "     <defs>\n",
              "      <path id=\"DejaVuSans-71\" d=\"M 947 1747 \n",
              "Q 947 1113 1208 752 \n",
              "Q 1469 391 1925 391 \n",
              "Q 2381 391 2643 752 \n",
              "Q 2906 1113 2906 1747 \n",
              "Q 2906 2381 2643 2742 \n",
              "Q 2381 3103 1925 3103 \n",
              "Q 1469 3103 1208 2742 \n",
              "Q 947 2381 947 1747 \n",
              "z\n",
              "M 2906 525 \n",
              "Q 2725 213 2448 61 \n",
              "Q 2172 -91 1784 -91 \n",
              "Q 1150 -91 751 415 \n",
              "Q 353 922 353 1747 \n",
              "Q 353 2572 751 3078 \n",
              "Q 1150 3584 1784 3584 \n",
              "Q 2172 3584 2448 3432 \n",
              "Q 2725 3281 2906 2969 \n",
              "L 2906 3500 \n",
              "L 3481 3500 \n",
              "L 3481 -1331 \n",
              "L 2906 -1331 \n",
              "L 2906 525 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "      <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
              "L 544 3500 \n",
              "L 1119 3500 \n",
              "L 1119 1403 \n",
              "Q 1119 906 1312 657 \n",
              "Q 1506 409 1894 409 \n",
              "Q 2359 409 2629 706 \n",
              "Q 2900 1003 2900 1516 \n",
              "L 2900 3500 \n",
              "L 3475 3500 \n",
              "L 3475 0 \n",
              "L 2900 0 \n",
              "L 2900 538 \n",
              "Q 2691 219 2414 64 \n",
              "Q 2138 -91 1772 -91 \n",
              "Q 1169 -91 856 284 \n",
              "Q 544 659 544 1381 \n",
              "z\n",
              "M 1991 3584 \n",
              "L 1991 3584 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "      <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
              "L 3597 1613 \n",
              "L 953 1613 \n",
              "Q 991 1019 1311 708 \n",
              "Q 1631 397 2203 397 \n",
              "Q 2534 397 2845 478 \n",
              "Q 3156 559 3463 722 \n",
              "L 3463 178 \n",
              "Q 3153 47 2828 -22 \n",
              "Q 2503 -91 2169 -91 \n",
              "Q 1331 -91 842 396 \n",
              "Q 353 884 353 1716 \n",
              "Q 353 2575 817 3079 \n",
              "Q 1281 3584 2069 3584 \n",
              "Q 2775 3584 3186 3129 \n",
              "Q 3597 2675 3597 1894 \n",
              "z\n",
              "M 3022 2063 \n",
              "Q 3016 2534 2758 2815 \n",
              "Q 2500 3097 2075 3097 \n",
              "Q 1594 3097 1305 2825 \n",
              "Q 1016 2553 972 2059 \n",
              "L 3022 2063 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "     </defs>\n",
              "     <use xlink:href=\"#DejaVuSans-71\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-75\" x=\"63.476562\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-65\" x=\"126.855469\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-65\" x=\"188.378906\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-6e\" x=\"249.902344\"/>\n",
              "    </g>\n",
              "   </g>\n",
              "   <g id=\"text_19\">\n",
              "    <!-- lord -->\n",
              "    <g transform=\"translate(314.312158 182.567749) scale(0.1 -0.1)\">\n",
              "     <defs>\n",
              "      <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
              "L 1178 4863 \n",
              "L 1178 0 \n",
              "L 603 0 \n",
              "L 603 4863 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "      <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
              "Q 1497 3097 1228 2736 \n",
              "Q 959 2375 959 1747 \n",
              "Q 959 1119 1226 758 \n",
              "Q 1494 397 1959 397 \n",
              "Q 2419 397 2687 759 \n",
              "Q 2956 1122 2956 1747 \n",
              "Q 2956 2369 2687 2733 \n",
              "Q 2419 3097 1959 3097 \n",
              "z\n",
              "M 1959 3584 \n",
              "Q 2709 3584 3137 3096 \n",
              "Q 3566 2609 3566 1747 \n",
              "Q 3566 888 3137 398 \n",
              "Q 2709 -91 1959 -91 \n",
              "Q 1206 -91 779 398 \n",
              "Q 353 888 353 1747 \n",
              "Q 353 2609 779 3096 \n",
              "Q 1206 3584 1959 3584 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "      <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
              "Q 2534 3019 2420 3045 \n",
              "Q 2306 3072 2169 3072 \n",
              "Q 1681 3072 1420 2755 \n",
              "Q 1159 2438 1159 1844 \n",
              "L 1159 0 \n",
              "L 581 0 \n",
              "L 581 3500 \n",
              "L 1159 3500 \n",
              "L 1159 2956 \n",
              "Q 1341 3275 1631 3429 \n",
              "Q 1922 3584 2338 3584 \n",
              "Q 2397 3584 2469 3576 \n",
              "Q 2541 3569 2628 3553 \n",
              "L 2631 2963 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "      <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \n",
              "L 2906 4863 \n",
              "L 3481 4863 \n",
              "L 3481 0 \n",
              "L 2906 0 \n",
              "L 2906 525 \n",
              "Q 2725 213 2448 61 \n",
              "Q 2172 -91 1784 -91 \n",
              "Q 1150 -91 751 415 \n",
              "Q 353 922 353 1747 \n",
              "Q 353 2572 751 3078 \n",
              "Q 1150 3584 1784 3584 \n",
              "Q 2172 3584 2448 3432 \n",
              "Q 2725 3281 2906 2969 \n",
              "z\n",
              "M 947 1747 \n",
              "Q 947 1113 1208 752 \n",
              "Q 1469 391 1925 391 \n",
              "Q 2381 391 2643 752 \n",
              "Q 2906 1113 2906 1747 \n",
              "Q 2906 2381 2643 2742 \n",
              "Q 2381 3103 1925 3103 \n",
              "Q 1469 3103 1208 2742 \n",
              "Q 947 2381 947 1747 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "     </defs>\n",
              "     <use xlink:href=\"#DejaVuSans-6c\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-72\" x=\"88.964844\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-64\" x=\"128.328125\"/>\n",
              "    </g>\n",
              "   </g>\n",
              "   <g id=\"text_20\">\n",
              "    <!-- man -->\n",
              "    <g transform=\"translate(238.633449 82.162517) scale(0.1 -0.1)\">\n",
              "     <defs>\n",
              "      <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \n",
              "Q 3544 3216 3844 3400 \n",
              "Q 4144 3584 4550 3584 \n",
              "Q 5097 3584 5394 3201 \n",
              "Q 5691 2819 5691 2113 \n",
              "L 5691 0 \n",
              "L 5113 0 \n",
              "L 5113 2094 \n",
              "Q 5113 2597 4934 2840 \n",
              "Q 4756 3084 4391 3084 \n",
              "Q 3944 3084 3684 2787 \n",
              "Q 3425 2491 3425 1978 \n",
              "L 3425 0 \n",
              "L 2847 0 \n",
              "L 2847 2094 \n",
              "Q 2847 2600 2669 2842 \n",
              "Q 2491 3084 2119 3084 \n",
              "Q 1678 3084 1418 2786 \n",
              "Q 1159 2488 1159 1978 \n",
              "L 1159 0 \n",
              "L 581 0 \n",
              "L 581 3500 \n",
              "L 1159 3500 \n",
              "L 1159 2956 \n",
              "Q 1356 3278 1631 3431 \n",
              "Q 1906 3584 2284 3584 \n",
              "Q 2666 3584 2933 3390 \n",
              "Q 3200 3197 3328 2828 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "      <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
              "Q 1497 1759 1228 1600 \n",
              "Q 959 1441 959 1056 \n",
              "Q 959 750 1161 570 \n",
              "Q 1363 391 1709 391 \n",
              "Q 2188 391 2477 730 \n",
              "Q 2766 1069 2766 1631 \n",
              "L 2766 1759 \n",
              "L 2194 1759 \n",
              "z\n",
              "M 3341 1997 \n",
              "L 3341 0 \n",
              "L 2766 0 \n",
              "L 2766 531 \n",
              "Q 2569 213 2275 61 \n",
              "Q 1981 -91 1556 -91 \n",
              "Q 1019 -91 701 211 \n",
              "Q 384 513 384 1019 \n",
              "Q 384 1609 779 1909 \n",
              "Q 1175 2209 1959 2209 \n",
              "L 2766 2209 \n",
              "L 2766 2266 \n",
              "Q 2766 2663 2505 2880 \n",
              "Q 2244 3097 1772 3097 \n",
              "Q 1472 3097 1187 3025 \n",
              "Q 903 2953 641 2809 \n",
              "L 641 3341 \n",
              "Q 956 3463 1253 3523 \n",
              "Q 1550 3584 1831 3584 \n",
              "Q 2591 3584 2966 3190 \n",
              "Q 3341 2797 3341 1997 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "     </defs>\n",
              "     <use xlink:href=\"#DejaVuSans-6d\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-61\" x=\"97.412109\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-6e\" x=\"158.691406\"/>\n",
              "    </g>\n",
              "   </g>\n",
              "   <g id=\"text_21\">\n",
              "    <!-- woman -->\n",
              "    <g transform=\"translate(279.493219 55.220861) scale(0.1 -0.1)\">\n",
              "     <defs>\n",
              "      <path id=\"DejaVuSans-77\" d=\"M 269 3500 \n",
              "L 844 3500 \n",
              "L 1563 769 \n",
              "L 2278 3500 \n",
              "L 2956 3500 \n",
              "L 3675 769 \n",
              "L 4391 3500 \n",
              "L 4966 3500 \n",
              "L 4050 0 \n",
              "L 3372 0 \n",
              "L 2619 2869 \n",
              "L 1863 0 \n",
              "L 1184 0 \n",
              "L 269 3500 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "     </defs>\n",
              "     <use xlink:href=\"#DejaVuSans-77\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-6f\" x=\"81.787109\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-6d\" x=\"142.96875\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-61\" x=\"240.380859\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-6e\" x=\"301.660156\"/>\n",
              "    </g>\n",
              "   </g>\n",
              "   <g id=\"text_22\">\n",
              "    <!-- dog -->\n",
              "    <g transform=\"translate(61.07804 151.84084) scale(0.1 -0.1)\">\n",
              "     <use xlink:href=\"#DejaVuSans-64\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-6f\" x=\"63.476562\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-67\" x=\"124.658203\"/>\n",
              "    </g>\n",
              "   </g>\n",
              "   <g id=\"text_23\">\n",
              "    <!-- wolf -->\n",
              "    <g transform=\"translate(253.767607 63.811527) scale(0.1 -0.1)\">\n",
              "     <defs>\n",
              "      <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \n",
              "L 2375 4384 \n",
              "L 1825 4384 \n",
              "Q 1516 4384 1395 4259 \n",
              "Q 1275 4134 1275 3809 \n",
              "L 1275 3500 \n",
              "L 2222 3500 \n",
              "L 2222 3053 \n",
              "L 1275 3053 \n",
              "L 1275 0 \n",
              "L 697 0 \n",
              "L 697 3053 \n",
              "L 147 3053 \n",
              "L 147 3500 \n",
              "L 697 3500 \n",
              "L 697 3744 \n",
              "Q 697 4328 969 4595 \n",
              "Q 1241 4863 1831 4863 \n",
              "L 2375 4863 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "     </defs>\n",
              "     <use xlink:href=\"#DejaVuSans-77\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-6f\" x=\"81.787109\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-6c\" x=\"142.96875\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-66\" x=\"170.751953\"/>\n",
              "    </g>\n",
              "   </g>\n",
              "   <g id=\"text_24\">\n",
              "    <!-- rich -->\n",
              "    <g transform=\"translate(314.851688 44.097533) scale(0.1 -0.1)\">\n",
              "     <defs>\n",
              "      <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
              "L 3122 2828 \n",
              "Q 2878 2963 2633 3030 \n",
              "Q 2388 3097 2138 3097 \n",
              "Q 1578 3097 1268 2742 \n",
              "Q 959 2388 959 1747 \n",
              "Q 959 1106 1268 751 \n",
              "Q 1578 397 2138 397 \n",
              "Q 2388 397 2633 464 \n",
              "Q 2878 531 3122 666 \n",
              "L 3122 134 \n",
              "Q 2881 22 2623 -34 \n",
              "Q 2366 -91 2075 -91 \n",
              "Q 1284 -91 818 406 \n",
              "Q 353 903 353 1747 \n",
              "Q 353 2603 823 3093 \n",
              "Q 1294 3584 2113 3584 \n",
              "Q 2378 3584 2631 3529 \n",
              "Q 2884 3475 3122 3366 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "      <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
              "L 3513 0 \n",
              "L 2938 0 \n",
              "L 2938 2094 \n",
              "Q 2938 2591 2744 2837 \n",
              "Q 2550 3084 2163 3084 \n",
              "Q 1697 3084 1428 2787 \n",
              "Q 1159 2491 1159 1978 \n",
              "L 1159 0 \n",
              "L 581 0 \n",
              "L 581 4863 \n",
              "L 1159 4863 \n",
              "L 1159 2956 \n",
              "Q 1366 3272 1645 3428 \n",
              "Q 1925 3584 2291 3584 \n",
              "Q 2894 3584 3203 3211 \n",
              "Q 3513 2838 3513 2113 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "     </defs>\n",
              "     <use xlink:href=\"#DejaVuSans-72\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-69\" x=\"41.113281\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-63\" x=\"68.896484\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-68\" x=\"123.876953\"/>\n",
              "    </g>\n",
              "   </g>\n",
              "   <g id=\"text_25\">\n",
              "    <!-- happy -->\n",
              "    <g transform=\"translate(286.822223 203.362065) scale(0.1 -0.1)\">\n",
              "     <defs>\n",
              "      <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
              "L 1159 -1331 \n",
              "L 581 -1331 \n",
              "L 581 3500 \n",
              "L 1159 3500 \n",
              "L 1159 2969 \n",
              "Q 1341 3281 1617 3432 \n",
              "Q 1894 3584 2278 3584 \n",
              "Q 2916 3584 3314 3078 \n",
              "Q 3713 2572 3713 1747 \n",
              "Q 3713 922 3314 415 \n",
              "Q 2916 -91 2278 -91 \n",
              "Q 1894 -91 1617 61 \n",
              "Q 1341 213 1159 525 \n",
              "z\n",
              "M 3116 1747 \n",
              "Q 3116 2381 2855 2742 \n",
              "Q 2594 3103 2138 3103 \n",
              "Q 1681 3103 1420 2742 \n",
              "Q 1159 2381 1159 1747 \n",
              "Q 1159 1113 1420 752 \n",
              "Q 1681 391 2138 391 \n",
              "Q 2594 391 2855 752 \n",
              "Q 3116 1113 3116 1747 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "      <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
              "Q 1816 -950 1584 -1140 \n",
              "Q 1353 -1331 966 -1331 \n",
              "L 506 -1331 \n",
              "L 506 -850 \n",
              "L 844 -850 \n",
              "Q 1081 -850 1212 -737 \n",
              "Q 1344 -625 1503 -206 \n",
              "L 1606 56 \n",
              "L 191 3500 \n",
              "L 800 3500 \n",
              "L 1894 763 \n",
              "L 2988 3500 \n",
              "L 3597 3500 \n",
              "L 2059 -325 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "     </defs>\n",
              "     <use xlink:href=\"#DejaVuSans-68\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-61\" x=\"63.378906\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-70\" x=\"124.658203\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-70\" x=\"188.134766\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-79\" x=\"251.611328\"/>\n",
              "    </g>\n",
              "   </g>\n",
              "   <g id=\"text_26\">\n",
              "    <!-- sad -->\n",
              "    <g transform=\"translate(338.244757 208.14271) scale(0.1 -0.1)\">\n",
              "     <defs>\n",
              "      <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
              "L 2834 2853 \n",
              "Q 2591 2978 2328 3040 \n",
              "Q 2066 3103 1784 3103 \n",
              "Q 1356 3103 1142 2972 \n",
              "Q 928 2841 928 2578 \n",
              "Q 928 2378 1081 2264 \n",
              "Q 1234 2150 1697 2047 \n",
              "L 1894 2003 \n",
              "Q 2506 1872 2764 1633 \n",
              "Q 3022 1394 3022 966 \n",
              "Q 3022 478 2636 193 \n",
              "Q 2250 -91 1575 -91 \n",
              "Q 1294 -91 989 -36 \n",
              "Q 684 19 347 128 \n",
              "L 347 722 \n",
              "Q 666 556 975 473 \n",
              "Q 1284 391 1588 391 \n",
              "Q 1994 391 2212 530 \n",
              "Q 2431 669 2431 922 \n",
              "Q 2431 1156 2273 1281 \n",
              "Q 2116 1406 1581 1522 \n",
              "L 1381 1569 \n",
              "Q 847 1681 609 1914 \n",
              "Q 372 2147 372 2553 \n",
              "Q 372 3047 722 3315 \n",
              "Q 1072 3584 1716 3584 \n",
              "Q 2034 3584 2315 3537 \n",
              "Q 2597 3491 2834 3397 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "     </defs>\n",
              "     <use xlink:href=\"#DejaVuSans-73\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-61\" x=\"52.099609\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-64\" x=\"113.378906\"/>\n",
              "    </g>\n",
              "   </g>\n",
              "   <g id=\"text_27\">\n",
              "    <!-- love -->\n",
              "    <g transform=\"translate(385.732585 47.569035) scale(0.1 -0.1)\">\n",
              "     <defs>\n",
              "      <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
              "L 800 3500 \n",
              "L 1894 563 \n",
              "L 2988 3500 \n",
              "L 3597 3500 \n",
              "L 2284 0 \n",
              "L 1503 0 \n",
              "L 191 3500 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "     </defs>\n",
              "     <use xlink:href=\"#DejaVuSans-6c\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-76\" x=\"88.964844\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-65\" x=\"148.144531\"/>\n",
              "    </g>\n",
              "   </g>\n",
              "   <g id=\"text_28\">\n",
              "    <!-- respect -->\n",
              "    <g transform=\"translate(327.360768 95.812971) scale(0.1 -0.1)\">\n",
              "     <defs>\n",
              "      <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
              "L 1172 3500 \n",
              "L 2356 3500 \n",
              "L 2356 3053 \n",
              "L 1172 3053 \n",
              "L 1172 1153 \n",
              "Q 1172 725 1289 603 \n",
              "Q 1406 481 1766 481 \n",
              "L 2356 481 \n",
              "L 2356 0 \n",
              "L 1766 0 \n",
              "Q 1100 0 847 248 \n",
              "Q 594 497 594 1153 \n",
              "L 594 3053 \n",
              "L 172 3053 \n",
              "L 172 3500 \n",
              "L 594 3500 \n",
              "L 594 4494 \n",
              "L 1172 4494 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "     </defs>\n",
              "     <use xlink:href=\"#DejaVuSans-72\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-65\" x=\"38.863281\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-73\" x=\"100.386719\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-70\" x=\"152.486328\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-65\" x=\"215.962891\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-63\" x=\"277.486328\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-74\" x=\"332.466797\"/>\n",
              "    </g>\n",
              "   </g>\n",
              "   <g id=\"text_29\">\n",
              "    <!-- beautiful -->\n",
              "    <g transform=\"translate(349.917043 155.279821) scale(0.1 -0.1)\">\n",
              "     <defs>\n",
              "      <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \n",
              "Q 3116 2381 2855 2742 \n",
              "Q 2594 3103 2138 3103 \n",
              "Q 1681 3103 1420 2742 \n",
              "Q 1159 2381 1159 1747 \n",
              "Q 1159 1113 1420 752 \n",
              "Q 1681 391 2138 391 \n",
              "Q 2594 391 2855 752 \n",
              "Q 3116 1113 3116 1747 \n",
              "z\n",
              "M 1159 2969 \n",
              "Q 1341 3281 1617 3432 \n",
              "Q 1894 3584 2278 3584 \n",
              "Q 2916 3584 3314 3078 \n",
              "Q 3713 2572 3713 1747 \n",
              "Q 3713 922 3314 415 \n",
              "Q 2916 -91 2278 -91 \n",
              "Q 1894 -91 1617 61 \n",
              "Q 1341 213 1159 525 \n",
              "L 1159 0 \n",
              "L 581 0 \n",
              "L 581 4863 \n",
              "L 1159 4863 \n",
              "L 1159 2969 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "     </defs>\n",
              "     <use xlink:href=\"#DejaVuSans-62\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-65\" x=\"63.476562\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-61\" x=\"125\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-75\" x=\"186.279297\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-74\" x=\"249.658203\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-69\" x=\"288.867188\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-66\" x=\"316.650391\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-75\" x=\"351.855469\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-6c\" x=\"415.234375\"/>\n",
              "    </g>\n",
              "   </g>\n",
              "   <g id=\"text_30\">\n",
              "    <!-- cat -->\n",
              "    <g transform=\"translate(344.488457 261.216) scale(0.1 -0.1)\">\n",
              "     <use xlink:href=\"#DejaVuSans-63\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-61\" x=\"54.980469\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-74\" x=\"116.259766\"/>\n",
              "    </g>\n",
              "   </g>\n",
              "   <g id=\"text_31\">\n",
              "    <!-- dog -->\n",
              "    <g transform=\"translate(61.07804 151.84084) scale(0.1 -0.1)\">\n",
              "     <use xlink:href=\"#DejaVuSans-64\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-6f\" x=\"63.476562\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-67\" x=\"124.658203\"/>\n",
              "    </g>\n",
              "   </g>\n",
              "   <g id=\"text_32\">\n",
              "    <!-- fox -->\n",
              "    <g transform=\"translate(244.158454 19.296) scale(0.1 -0.1)\">\n",
              "     <defs>\n",
              "      <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \n",
              "L 2247 1797 \n",
              "L 3578 0 \n",
              "L 2900 0 \n",
              "L 1881 1375 \n",
              "L 863 0 \n",
              "L 184 0 \n",
              "L 1544 1831 \n",
              "L 300 3500 \n",
              "L 978 3500 \n",
              "L 1906 2253 \n",
              "L 2834 3500 \n",
              "L 3513 3500 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "     </defs>\n",
              "     <use xlink:href=\"#DejaVuSans-66\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-6f\" x=\"35.205078\"/>\n",
              "     <use xlink:href=\"#DejaVuSans-78\" x=\"93.261719\"/>\n",
              "    </g>\n",
              "   </g>\n",
              "  </g>\n",
              " </g>\n",
              " <defs>\n",
              "  <clipPath id=\"pfeca47dad1\">\n",
              "   <rect x=\"44.845313\" y=\"7.2\" width=\"357.12\" height=\"266.112\"/>\n",
              "  </clipPath>\n",
              " </defs>\n",
              "</svg>\n"
            ],
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "result= compute_pca(X, 2)\n",
        "pyplot.scatter(result[:, 0], result[:, 1])\n",
        "for i, word in enumerate(words):\n",
        "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NLP with Sequencial models : Sentimenet Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Libraries :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from utils import load_tweets, process_tweet_2\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load and split the Data\n",
        "\n",
        "- Import the positive and negative tweets\n",
        "- Have a look at some examples of the tweets\n",
        "- Split the data into the training and validation sets\n",
        "- Create labels for the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of positive tweets: 5000\n",
            "The number of negative tweets: 5000\n"
          ]
        }
      ],
      "source": [
        "# Load positive and negative tweets\n",
        "all_positive_tweets, all_negative_tweets = load_tweets()\n",
        "\n",
        "# View the total number of positive and negative tweets.\n",
        "print(f\"The number of positive tweets: {len(all_positive_tweets)}\")\n",
        "print(f\"The number of negative tweets: {len(all_negative_tweets)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Positive tweet example:\n",
            "yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days\n",
            "\n",
            "Negative tweet example:\n",
            "Dang starting next week I have \"work\" :(\n"
          ]
        }
      ],
      "source": [
        "# Change the tweet number to any number between 0 and 4999 to see a different pair of tweets.\n",
        "tweet_number = 4\n",
        "print('Positive tweet example:')\n",
        "print(all_positive_tweets[tweet_number])\n",
        "print('\\nNegative tweet example:')\n",
        "print(all_negative_tweets[tweet_number])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here you will process the tweets. This part of the code has been implemented for you.  The processing includes:\n",
        "\n",
        "- tokenizing the sentence (splitting to words)\n",
        "- removing stock market tickers like $GE\n",
        "- removing old style retweet text \"RT\"\n",
        "- removing hyperlinks\n",
        "- removing hashtags\n",
        "- lowercasing\n",
        "- removing stopwords and punctuation\n",
        "- stemming\n",
        "\n",
        "Some of these things are general steps you would do when processing any text, some others are very \"tweet-specific\". The details of the process_tweet function are available in utils.py file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process all the tweets: tokenize the string, remove tickers, handles, punctuation and stopwords, stem the words\n",
        "all_positive_tweets_processed = [process_tweet_2(tweet) for tweet in all_positive_tweets]\n",
        "all_negative_tweets_processed = [process_tweet_2(tweet) for tweet in all_negative_tweets]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Positive processed tweet example:\n",
            "['yeaaah', 'yipppy', 'accnt', 'verify', 'rqst', 'succeed', 'get', 'blue', 'tick', 'mark', 'fb', 'profile', ':)', '15', 'day']\n",
            "\n",
            "Negative processed tweet example:\n",
            "['dang', 'start', 'next', 'week', 'work', ':(']\n"
          ]
        }
      ],
      "source": [
        "# Change the tweet number to any number between 0 and 4999 to see a different pair of tweets.\n",
        "tweet_number = 4\n",
        "print('Positive processed tweet example:')\n",
        "print(all_positive_tweets_processed[tweet_number])\n",
        "print('\\nNegative processed tweet example:')\n",
        "print(all_negative_tweets_processed[tweet_number])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 8000 sentences for training.\n",
            "There are 8000 labels for training.\n",
            "\n",
            "There are 2000 sentences for validation.\n",
            "There are 2000 labels for validation.\n"
          ]
        }
      ],
      "source": [
        "# Split positive set into validation and training\n",
        "val_pos = all_positive_tweets_processed[4000:]\n",
        "train_pos = all_positive_tweets_processed[:4000]\n",
        "# Split negative set into validation and training\n",
        "val_neg = all_negative_tweets_processed[4000:]\n",
        "train_neg = all_negative_tweets_processed[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg \n",
        "val_x  = val_pos + val_neg\n",
        "\n",
        "# Set the labels for the training and validation set (1 for positive, 0 for negative)\n",
        "train_y = [[1] for _ in train_pos] + [[0] for _ in train_neg]\n",
        "val_y  = [[1] for _ in val_pos] + [[0] for _ in val_neg]\n",
        "\n",
        "print(f\"There are {len(train_x)} sentences for training.\")\n",
        "print(f\"There are {len(train_y)} labels for training.\\n\")\n",
        "print(f\"There are {len(val_x)} sentences for validation.\")\n",
        "print(f\"There are {len(val_y)} labels for validation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Vocabulary\n",
        "\n",
        "The vocabulary will also include some special tokens\n",
        "- `''`: padding\n",
        "- `'[UNK]'`: a token representing any word that is not in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary contains 9535 words\n",
            "\n",
            "{'': 0, '[UNK]': 1, 'followfriday': 2, 'top': 3, 'engage': 4, 'member': 5, 'community': 6, 'week': 7, ':)': 8, 'hey': 9, 'james': 10, 'odd': 11, ':/': 12, 'please': 13, 'call': 14, 'contact': 15, 'centre': 16, '02392441234': 17, 'able': 18, 'assist': 19, 'many': 20, 'thanks': 21, 'listen': 22, 'last': 23, 'night': 24, 'bleed': 25, 'amazing': 26, 'track': 27, 'scotland': 28, 'congrats': 29, 'yeaaah': 30, 'yipppy': 31, 'accnt': 32, 'verify': 33, 'rqst': 34, 'succeed': 35, 'get': 36, 'blue': 37, 'tick': 38, 'mark': 39, 'fb': 40, 'profile': 41, '15': 42, 'day': 43, 'one': 44, 'irresistible': 45, 'flipkartfashionfriday': 46, 'like': 47, 'keep': 48, 'lovely': 49, 'customer': 50, 'wait': 51, 'long': 52, 'hope': 53, 'enjoy': 54, 'happy': 55, 'friday': 56, 'lwwf': 57, 'second': 58, 'thought': 59, '’': 60, 'enough': 61, 'time': 62, 'dd': 63, 'new': 64, 'short': 65, 'enter': 66, 'system': 67, 'sheep': 68, 'must': 69, 'buy': 70, 'jgh': 71, 'go': 72, 'bayan': 73, ':D': 74, 'bye': 75, 'act': 76, 'mischievousness': 77, 'etl': 78, 'layer': 79, 'in-house': 80, 'warehouse': 81, 'app': 82, 'katamari': 83, 'well': 84, '…': 85, 'name': 86, 'imply': 87, ':p': 88, 'influencers': 89, 'love': 90, 'big': 91, '...': 92, 'juicy': 93, 'selfies': 94, 'follow': 95, 'perfect': 96, 'already': 97, 'know': 98, \"what's\": 99, 'great': 100, 'opportunity': 101, 'junior': 102, 'triathletes': 103, 'age': 104, '12': 105, '13': 106, 'gatorade': 107, 'series': 108, 'entry': 109, 'lay': 110, 'greeting': 111, 'card': 112, 'range': 113, 'print': 114, 'today': 115, 'job': 116, ':-)': 117, \"friend's\": 118, 'lunch': 119, 'yummm': 120, 'nostalgia': 121, 'tb': 122, 'ku': 123, 'id': 124, 'conflict': 125, 'help': 126, \"here's\": 127, 'screenshot': 128, 'work': 129, 'hi': 130, 'liv': 131, 'hello': 132, 'need': 133, 'something': 134, 'u': 135, 'fm': 136, 'twitter': 137, '—': 138, 'sure': 139, 'thing': 140, 'dm': 141, 'x': 142, 'follower': 143, \"i've\": 144, 'hear': 145, 'four': 146, 'season': 147, 'pretty': 148, 'dope': 149, 'penthouse': 150, 'obvs': 151, 'gobigorgohome': 152, 'fun': 153, \"y'all\": 154, 'yeah': 155, 'suppose': 156, 'lol': 157, 'chat': 158, 'bit': 159, 'youth': 160, '💅🏽': 161, '💋': 162, 'see': 163, 'year': 164, 'thank': 165, 'rest': 166, 'quickly': 167, 'bed': 168, 'music': 169, 'fix': 170, 'dream': 171, 'spiritual': 172, 'ritual': 173, 'festival': 174, 'népal': 175, 'beginning': 176, 'line-up': 177, 'leave': 178, 'sarah': 179, 'send': 180, 'email': 181, 'bitsy@bitdefender.com': 182, \"we'll\": 183, 'asap': 184, 'lols': 185, 'kik': 186, 'hatessuce': 187, '32429': 188, 'kikme': 189, 'lgbt': 190, 'tinder': 191, 'nsfw': 192, 'akua': 193, 'cumshot': 194, 'come': 195, 'house': 196, 'nsn_supplements': 197, 'effective': 198, 'press': 199, 'release': 200, 'distribution': 201, 'result': 202, 'link': 203, 'remove': 204, 'pressrelease': 205, 'newsdistribution': 206, 'bam': 207, 'bestfriend': 208, 'lot': 209, 'warsaw': 210, '<3': 211, 'x46': 212, 'everyone': 213, 'watch': 214, 'documentary': 215, 'earthling': 216, 'youtube': 217, 'support': 218, 'buuut': 219, 'oh': 220, 'look': 221, 'forward': 222, 'visit': 223, 'next': 224, 'letsgetmessy': 225, 'jo': 226, 'make': 227, 'feel': 228, 'never': 229, 'anyone': 230, 'kpop': 231, 'flesh': 232, 'good': 233, 'girl': 234, 'best': 235, 'wish': 236, 'reason': 237, 'epic': 238, 'soundtrack': 239, 'shout': 240, 'add': 241, 'video': 242, 'playlist': 243, 'would': 244, 'dear': 245, 'jordan': 246, 'okay': 247, 'fake': 248, 'gameplays': 249, ';)': 250, 'haha': 251, 'im': 252, 'kidding': 253, 'stuff': 254, 'exactly': 255, 'product': 256, 'line': 257, 'etsy': 258, 'shop': 259, 'check': 260, 'vacation': 261, 'rechargeable': 262, 'normally': 263, 'charger': 264, 'asleep': 265, 'talk': 266, 'sooo': 267, 'someone': 268, 'text': 269, 'yes': 270, 'bet': 271, \"he'll\": 272, 'fit': 273, 'speech': 274, 'pity': 275, 'green': 276, 'garden': 277, 'midnight': 278, 'sun': 279, 'beautiful': 280, 'canal': 281, 'dasvidaniya': 282, 'till': 283, 'scout': 284, 'sg': 285, 'future': 286, 'wlan': 287, 'pro': 288, 'conference': 289, 'asia': 290, 'change': 291, 'lollipop': 292, '🍭': 293, 'nez': 294, 'agnezmo': 295, 'oley': 296, 'mama': 297, 'stand': 298, 'strong': 299, 'god': 300, 'misty': 301, 'baby': 302, 'cute': 303, 'woohoo': 304, \"can't\": 305, 'sign': 306, 'yet': 307, 'still': 308, 'think': 309, 'mka': 310, 'liam': 311, 'access': 312, 'welcome': 313, 'stats': 314, 'arrive': 315, '1': 316, 'unfollowers': 317, 'via': 318, 'surprise': 319, 'figure': 320, 'happybirthdayemilybett': 321, 'sweet': 322, 'talented': 323, 'amaze': 324, '2': 325, 'plan': 326, 'drain': 327, 'gotta': 328, 'timezones': 329, 'parent': 330, 'proud': 331, 'least': 332, 'maybe': 333, 'sometimes': 334, 'grade': 335, 'al': 336, 'grande': 337, 'manila_bro': 338, 'choose': 339, 'let': 340, 'around': 341, '..': 342, 'side': 343, 'world': 344, 'eh': 345, 'take': 346, 'care': 347, 'finally': 348, 'fuck': 349, 'weekend': 350, 'real': 351, 'x45': 352, 'join': 353, 'hushedcallwithfraydoe': 354, 'gift': 355, 'yeahhh': 356, 'hushedpinwithsammy': 357, 'event': 358, 'might': 359, 'luv': 360, 'really': 361, 'appreciate': 362, 'share': 363, 'wow': 364, 'tom': 365, 'gym': 366, 'monday': 367, 'invite': 368, 'scope': 369, 'influencer': 370, 'friend': 371, 'nude': 372, 'sleep': 373, 'birthday': 374, 'want': 375, 't-shirt': 376, 'cool': 377, 'haw': 378, 'phela': 379, 'mom': 380, 'obviously': 381, 'prince': 382, 'charm': 383, 'stage': 384, 'luck': 385, 'tyler': 386, 'hipster': 387, 'glass': 388, 'marty': 389, 'glad': 390, 'do': 391, 'afternoon': 392, 'read': 393, 'kahfi': 394, 'finish': 395, 'ohmyg': 396, 'yaya': 397, 'dub': 398, 'stalk': 399, 'ig': 400, 'gondooo': 401, 'moo': 402, 'tologooo': 403, 'become': 404, 'detail': 405, 'zzz': 406, 'xx': 407, 'physiotherapy': 408, 'hashtag': 409, 'custom': 410, '💪': 411, 'monica': 412, 'miss': 413, 'sound': 414, 'morning': 415, \"that's\": 416, 'x43': 417, 'definitely': 418, 'try': 419, 'tonight': 420, 'advice': 421, 'treviso': 422, 'concert': 423, 'city': 424, 'country': 425, \"i'll\": 426, 'start': 427, 'fine': 428, 'gorgeous': 429, 'xo': 430, 'oven': 431, 'roast': 432, 'garlic': 433, 'olive': 434, 'oil': 435, 'dry': 436, 'tomato': 437, 'dried': 438, 'basil': 439, 'century': 440, 'tuna': 441, 'right': 442, 'back': 443, 'atchya': 444, 'even': 445, 'almost': 446, 'chance': 447, 'cheer': 448, 'po': 449, 'ice': 450, 'cream': 451, 'agree': 452, '100': 453, 'hehehehe': 454, 'thats': 455, 'point': 456, 'stay': 457, 'home': 458, 'soon': 459, 'promise': 460, 'web': 461, 'whatsapp': 462, 'volta': 463, 'funcionar': 464, 'com': 465, 'iphone': 466, 'jailbroken': 467, 'later': 468, '34': 469, 'min': 470, 'leia': 471, 'appear': 472, 'hologram': 473, 'r2d2': 474, 'w': 475, 'message': 476, 'obi': 477, 'wan': 478, 'sit': 479, 'luke': 480, 'inter': 481, '3': 482, 'ucl': 483, 'arsenal': 484, 'small': 485, 'team': 486, 'passing': 487, '🚂': 488, 'dewsbury': 489, 'railway': 490, 'station': 491, 'dew': 492, 'west': 493, 'yorkshire': 494, '430': 495, 'smh': 496, '9:25': 497, 'live': 498, 'strange': 499, 'imagine': 500, 'megan': 501, 'masaantoday': 502, 'a4': 503, 'shweta': 504, 'tripathi': 505, '5': 506, '20': 507, 'kurta': 508, 'half': 509, 'number': 510, 'wsalelove': 511, 'ah': 512, 'larry': 513, 'anyway': 514, 'kinda': 515, 'goood': 516, 'life': 517, 'enn': 518, 'surely': 519, 'could': 520, 'warmup': 521, '15th': 522, 'bath': 523, 'dum': 524, 'andar': 525, 'ram': 526, 'sampath': 527, 'sona': 528, 'mohapatra': 529, 'samantha': 530, 'edward': 531, 'mein': 532, 'tulane': 533, 'razi': 534, 'wah': 535, 'josh': 536, 'always': 537, 'smile': 538, 'picture': 539, '16.20': 540, 'timing': 541, 'giveitup': 542, 'give': 543, 'gas': 544, 'subsidy': 545, 'initiative': 546, 'propose': 547, 'feeling': 548, 'delighted': 549, 'yesterday': 550, 'x42': 551, 'lmaoo': 552, 'song': 553, 'ever': 554, 'shall': 555, 'little': 556, 'throwback': 557, 'outlying': 558, 'island': 559, 'cheung': 560, 'chau': 561, 'mui': 562, 'wo': 563, 'totally': 564, 'different': 565, 'kfckitchentours': 566, 'kitchen': 567, 'clean': 568, \"i'm\": 569, 'amazed': 570, 'cusp': 571, 'test': 572, 'water': 573, 'reward': 574, 'arummzz': 575, \"let's\": 576, 'drive': 577, 'travel': 578, 'traveler': 579, 'yogyakarta': 580, 'jeep': 581, 'indonesia': 582, 'instamood': 583, 'wanna': 584, 'skype': 585, 'may': 586, 'nice': 587, 'friendly': 588, 'pretend': 589, 'film': 590, 'congratulation': 591, 'winner': 592, 'cheesydelights': 593, 'contest': 594, 'address': 595, 'guy': 596, 'marketing': 597, '24/7': 598, '14': 599, 'hour': 600, 'without': 601, 'delay': 602, 'actually': 603, 'easy': 604, 'guess': 605, 'train': 606, 'wd': 607, 'shift': 608, 'engine': 609, 'etc': 610, 'sunburn': 611, 'peel': 612, 'blog': 613, 'huge': 614, 'warm': 615, '☆': 616, 'complete': 617, 'triangle': 618, 'northern': 619, 'ireland': 620, 'sight': 621, 'smthng': 622, 'fr': 623, 'hug': 624, 'xoxo': 625, 'uu': 626, 'jaann': 627, 'topnewfollowers': 628, 'connect': 629, 'wonderful': 630, 'fluffy': 631, 'inside': 632, 'pirouette': 633, 'moose': 634, 'trip': 635, 'philly': 636, 'december': 637, \"i'd\": 638, 'dude': 639, 'x41': 640, 'question': 641, 'flaw': 642, 'pain': 643, 'negate': 644, 'strength': 645, 'solo': 646, 'move': 647, 'fav': 648, 'nirvana': 649, 'smell': 650, 'teen': 651, 'spirit': 652, 'rip': 653, 'amy': 654, 'winehouse': 655, 'couple': 656, 'tomhiddleston': 657, 'elizabetholsen': 658, 'yaytheylookgreat': 659, 'goodnight': 660, 'vid': 661, 'wake': 662, 'gonna': 663, 'shoot': 664, 'itty': 665, 'bitty': 666, 'teenie': 667, 'bikini': 668, 'much': 669, '4th': 670, 'together': 671, 'end': 672, 'xfiles': 673, 'content': 674, 'rain': 675, 'fabulous': 676, 'fantastic': 677, '♡': 678, 'jb': 679, 'forever': 680, 'belieber': 681, 'nighty': 682, 'bug': 683, 'bite': 684, 'bracelet': 685, 'idea': 686, 'foundry': 687, 'game': 688, 'sense': 689, 'pic': 690, 'eff': 691, 'phone': 692, 'woot': 693, 'derek': 694, 'use': 695, 'parkshare': 696, 'gloucestershire': 697, 'aaaahhh': 698, 'man': 699, 'traffic': 700, 'stress': 701, 'reliever': 702, \"how're\": 703, 'arbeloa': 704, 'turn': 705, '17': 706, 'omg': 707, 'difference': 708, 'say': 709, 'europe': 710, 'rise': 711, 'find': 712, 'hard': 713, 'believe': 714, 'uncountable': 715, 'coz': 716, 'unlimited': 717, 'course': 718, 'teampositive': 719, 'aldub': 720, '☕': 721, 'rita': 722, 'info': 723, \"we'd\": 724, 'way': 725, 'boy': 726, 'x40': 727, 'true': 728, 'sethi': 729, 'high': 730, 'exe': 731, 'skeem': 732, 'saam': 733, 'people': 734, 'polite': 735, 'izzat': 736, 'wese': 737, 'trust': 738, 'khawateen': 739, 'k': 740, 'sath': 741, 'mana': 742, 'kar': 743, 'deya': 744, 'evening': 745, 'sort': 746, 'smart': 747, 'hair': 748, 'tbh': 749, 'jacob': 750, 'g': 751, 'upgrade': 752, 'tee': 753, 'family': 754, 'reading': 755, 'person': 756, 'two': 757, 'conversation': 758, 'online': 759, 'mclaren': 760, 'fridayfeeling': 761, 'tgif': 762, 'square': 763, 'enix': 764, 'bissmillah': 765, 'ya': 766, 'allah': 767, \"we're\": 768, 'training': 769, 'socent': 770, 'startup': 771, 'drop': 772, 'youre': 773, 'arnd': 774, 'town': 775, 'basically': 776, 'piss': 777, 'cup': 778, 'also': 779, 'terrible': 780, 'complicated': 781, 'discussion': 782, 'snapchat': 783, 'lynettelowe': 784, 'kikmenow': 785, 'snapme': 786, 'hot': 787, 'amazon': 788, 'kikmeguys': 789, 'definately': 790, 'grow': 791, 'sport': 792, 'rt': 793, 'rakyat': 794, 'writing': 795, 'since': 796, 'mention': 797, 'fly': 798, 'fish': 799, 'promoted': 800, 'post': 801, 'cyber': 802, 'ourdaughtersourpride': 803, 'mypapamypride': 804, 'papa': 805, 'coach': 806, 'positive': 807, 'kha': 808, 'atleast': 809, 'x39': 810, 'mango': 811, \"lassi's\": 812, \"monty's\": 813, 'marvellous': 814, 'though': 815, 'suspect': 816, 'mean': 817, '24': 818, 'hr': 819, 'touch': 820, 'kepler': 821, '452b': 822, 'chalna': 823, 'hai': 824, 'thankyou': 825, 'hazel': 826, 'food': 827, 'market': 828, 'brooklyn': 829, 'pta': 830, 'awake': 831, 'okayy': 832, 'awww': 833, 'ha': 834, 'doc': 835, 'splendid': 836, 'spam': 837, 'folder': 838, 'amount': 839, 'nigeria': 840, 'claim': 841, 'rted': 842, 'legs': 843, 'hurt': 844, 'bad': 845, 'mine': 846, 'saturday': 847, 'thaaanks': 848, 'puhon': 849, 'happiness': 850, 'tnc': 851, 'prior': 852, 'notification': 853, 'fat': 854, 'co': 855, 'probably': 856, 'eat': 857, 'yuna': 858, 'tameside': 859, '´': 860, 'google': 861, 'account': 862, 'scouser': 863, 'everything': 864, 'zoe': 865, 'mate': 866, 'literally': 867, \"they're\": 868, 'sameee': 869, 'edgar': 870, 'update': 871, 'log': 872, 'bring': 873, 'abes': 874, 'meet': 875, 'x38': 876, 'sigh': 877, 'dreamily': 878, 'pout': 879, 'eye': 880, 'quacketyquack': 881, 'funny': 882, 'happen': 883, 'phil': 884, 'em': 885, 'del': 886, 'rodders': 887, 'else': 888, 'play': 889, 'gamejam': 890, 'irish': 891, 'literature': 892, 'inaccessible': 893, \"kareena's\": 894, 'fan': 895, 'brain': 896, 'dot': 897, 'braindots': 898, 'fair': 899, 'rush': 900, 'either': 901, 'brandi': 902, '18': 903, 'selfie': 904, 'carnival': 905, 'men': 906, 'put': 907, 'mask': 908, 'xavier': 909, 'forneret': 910, 'jennifer': 911, 'site': 912, 'free': 913, '50.000': 914, '8': 915, 'ball': 916, 'pool': 917, 'coin': 918, 'edit': 919, 'trish': 920, '♥': 921, 'gratefulness': 922, 'three': 923, 'grateful': 924, 'comment': 925, 'wakeup': 926, 'beside': 927, 'dirty': 928, 'sex': 929, 'lmaooo': 930, '😤': 931, 'louis': 932, \"he's\": 933, 'throw': 934, 'cause': 935, 'inspire': 936, 'ff': 937, 'twoofs': 938, 'gr8': 939, 'wkend': 940, 'kind': 941, 'exhausted': 942, 'word': 943, 'cheltenham': 944, 'area': 945, 'kale': 946, 'crisp': 947, 'ruin': 948, 'x37': 949, 'open': 950, 'worldwide': 951, 'outta': 952, 'sfvbeta': 953, 'vantastic': 954, 'xcylin': 955, 'bundle': 956, 'show': 957, 'internet': 958, 'price': 959, 'realisticly': 960, 'pay': 961, 'net': 962, 'education': 963, 'powerful': 964, 'weapon': 965, 'nelson': 966, 'mandela': 967, 'recent': 968, 'j': 969, 'chenab': 970, 'flow': 971, 'pakistan': 972, 'incredibleindia': 973, 'teenchoice': 974, 'choiceinternationalartist': 975, 'superjunior': 976, 'caught': 977, 'first': 978, 'salmon': 979, 'super-blend': 980, 'project': 981, 'youth@bipolaruk.org.uk': 982, 'awesome': 983, 'stream': 984, 'alma': 985, 'mater': 986, 'highschooldays': 987, 'clientvisit': 988, 'faith': 989, 'christian': 990, 'school': 991, 'lizaminnelli': 992, 'upcoming': 993, 'uk': 994, 'appearance': 995, '😄': 996, 'single': 997, 'hill': 998, 'every': 999, 'beat': 1000, 'wrong': 1001, 'ready': 1002, 'natural': 1003, 'pefumery': 1004, 'workshop': 1005, 'neals': 1006, 'yard': 1007, 'covent': 1008, 'tomorrow': 1009, 'fback': 1010, 'indo': 1011, 'harmos': 1012, 'americano': 1013, 'remember': 1014, 'aww': 1015, 'head': 1016, 'saw': 1017, 'dark': 1018, 'handshome': 1019, 'juga': 1020, 'hurray': 1021, 'meeting': 1022, 'hate': 1023, 'cant': 1024, 'decide': 1025, 'save': 1026, 'list': 1027, 'hiya': 1028, 'exec': 1029, 'loryn.good@lincs-chamber.co.uk': 1030, 'photo': 1031, 'thx': 1032, '4': 1033, 'china': 1034, 'homosexual': 1035, 'hyungbot': 1036, 'fam': 1037, 'mind': 1038, 'timetunnel': 1039, '1982': 1040, 'quite': 1041, 'radio': 1042, 'set': 1043, 'heart': 1044, 'hiii': 1045, 'jack': 1046, 'ily': 1047, '✨': 1048, 'domino': 1049, 'pub': 1050, 'heat': 1051, 'prob': 1052, 'sorry': 1053, 'hastily': 1054, 'type': 1055, 'screenshotting': 1056, 'pakistani': 1057, 'x36': 1058, '3points': 1059, 'dreamteam': 1060, 'gooo': 1061, 'bailey': 1062, 'pbb': 1063, '737gold': 1064, 'drink': 1065, 'old': 1066, '1/2': 1067, 'welsh': 1068, 'wale': 1069, 'yippee': 1070, '💟': 1071, 'bro': 1072, 'lord': 1073, 'michael': 1074, \"u're\": 1075, 'ure': 1076, 'bigot': 1077, 'usually': 1078, 'front': 1079, 'squat': 1080, 'dobar': 1081, 'dan': 1082, 'brand': 1083, 'heavy': 1084, 'musicology': 1085, '2015': 1086, 'spend': 1087, 'marathon': 1088, 'iflix': 1089, 'officially': 1090, 'graduate': 1091, 'cry': 1092, '__': 1093, 'yep': 1094, 'expert': 1095, 'bisexuality': 1096, 'minal': 1097, 'aidzin': 1098, 'yo': 1099, 'pi': 1100, 'cook': 1101, 'book': 1102, 'dinner': 1103, 'tough': 1104, 'choice': 1105, 'others': 1106, 'chill': 1107, 'smu': 1108, 'oval': 1109, 'basketball': 1110, 'player': 1111, 'whahahaha': 1112, 'soamazing': 1113, 'moment': 1114, 'onto': 1115, 'a5': 1116, 'wardrobe': 1117, 'user': 1118, 'teamred': 1119, 'apparently': 1120, 'hopefully': 1121, 'depends': 1122, 'greatly': 1123, 'design': 1124, 'ahhh': 1125, '7th': 1126, 'cinepambata': 1127, 'mechanic': 1128, 'official': 1129, 'form': 1130, 'download': 1131, 'ur': 1132, 'swishers': 1133, 'cop': 1134, 'ducktails': 1135, 'surreal': 1136, 'exposure': 1137, 'sotw': 1138, 'halesowen': 1139, 'blackcountryfair': 1140, 'street': 1141, 'assessment': 1142, 'mental': 1143, 'body': 1144, 'ooze': 1145, 'appeal': 1146, 'amassiveoverdoseofships': 1147, 'late': 1148, 'isi': 1149, 'chan': 1150, 'c': 1151, 'note': 1152, 'pkwalasawaal': 1153, 'gemma': 1154, 'orleans': 1155, 'fever': 1156, 'catch': 1157, 'geskenya': 1158, 'obamainkenya': 1159, 'magicalkenya': 1160, 'greatkenya': 1161, 'allgoodthingske': 1162, 'anime': 1163, 'umaru': 1164, 'singer': 1165, 'ship': 1166, 'order': 1167, 'room': 1168, 'car': 1169, 'hahaha': 1170, 'story': 1171, 'relate': 1172, 'label': 1173, 'batch': 1174, 'principal': 1175, 'due': 1176, 'march': 1177, 'wooftastic': 1178, 'receive': 1179, 'necessary': 1180, 'regret': 1181, 'rn': 1182, 'whatever': 1183, 'hat': 1184, 'success': 1185, 'abstinence': 1186, 'wtf': 1187, \"there's\": 1188, 'thrown': 1189, 'middle': 1190, 'repeat': 1191, 'relentlessly': 1192, 'approximately': 1193, 'oldschool': 1194, 'runescape': 1195, 'daaay': 1196, 'jumma_mubarik': 1197, 'frnds': 1198, 'stay_blessed': 1199, 'bless': 1200, 'pussycats': 1201, 'main': 1202, 'launch': 1203, 'pretoria': 1204, 'fahrinahmad': 1205, 'tengkuaaronshah': 1206, 'eksperimencinta': 1207, 'tykkäsin': 1208, 'videosta': 1209, 'month': 1210, 'hoodie': 1211, 'eeep': 1212, 'yay': 1213, 'sohappyrightnow': 1214, 'mmm': 1215, 'azz-sets': 1216, 'babe': 1217, 'feedback': 1218, 'gain': 1219, 'value': 1220, 'peaceful': 1221, 'refresh': 1222, 'manthan': 1223, 'tune': 1224, 'freshness': 1225, 'mother': 1226, 'determination': 1227, 'maxfreshmove': 1228, 'lonely': 1229, 'tattoo': 1230, 'friday.and': 1231, 'magnificent': 1232, 'e': 1233, 'achieve': 1234, 'rashmi': 1235, 'dedication': 1236, 'inspiration': 1237, 'happyfriday': 1238, 'nearly': 1239, 'retweeted': 1240, 'alert': 1241, 'da': 1242, 'dang': 1243, 'rad': 1244, 'fanart': 1245, 'massive': 1246, 'niamh': 1247, 'fennell': 1248, 'journalism': 1249, 'land': 1250, 'copying': 1251, 'paste': 1252, 'tweet': 1253, 'ariana': 1254, 'selena': 1255, 'gomez': 1256, 'tomlinson': 1257, 'payne': 1258, 'caradelevingne': 1259, '🌷': 1260, 'trade': 1261, 'tire': 1262, 'nope': 1263, 'apply': 1264, 'iamca': 1265, 'aftie': 1266, 'goodmorning': 1267, 'prokabaddi': 1268, 'koel': 1269, 'mallick': 1270, 'recite': 1271, 'national': 1272, 'anthem': 1273, '6': 1274, 'yournaturalleaders': 1275, 'youngnaturalleaders': 1276, 'mon': 1277, '27july': 1278, 'cumbria': 1279, 'flockstars': 1280, 'thur': 1281, '30july': 1282, 'itv': 1283, 'sleeptight': 1284, 'haveagoodday': 1285, 'leg': 1286, 'september': 1287, 'perhaps': 1288, 'bb': 1289, 'promote': 1290, 'full': 1291, 'album': 1292, 'fully': 1293, 'intend': 1294, 'write': 1295, 'possible': 1296, 'attack': 1297, '>:D': 1298, 'bird': 1299, 'teamadmicro': 1300, 'fridaydownpour': 1301, 'clear': 1302, 'rohit': 1303, 'queen': 1304, 'otwolgrandtrailer': 1305, 'sheer': 1306, 'fact': 1307, 'obama': 1308, 'innumerable': 1309, 'odds': 1310, 'president': 1311, 'ni': 1312, 'shauri': 1313, 'yako': 1314, 'memotohaters': 1315, 'sunday': 1316, 'pamper': 1317, \"t'was\": 1318, 'cabincrew': 1319, 'interview': 1320, 'langkawi': 1321, '1st': 1322, 'august': 1323, 'fulfil': 1324, 'fantasy': 1325, '👉': 1326, 'thinking': 1327, 'ex-twelebs': 1328, 'friends': 1329, 'apartment': 1330, 'makeover': 1331, 'brilliantly': 1332, 'happyyy': 1333, 'birthdaaayyy': 1334, 'kill': 1335, 'interested': 1336, 'internship': 1337, 'program': 1338, 'sadly': 1339, 'career': 1340, 'page': 1341, 'issue': 1342, 'sad': 1343, 'overwhelmingly': 1344, 'aha': 1345, 'beauts': 1346, '♬': 1347, 'win': 1348, 'deo': 1349, 'faaabulous': 1350, 'freebiefriday': 1351, 'aluminiumfree': 1352, 'stayfresh': 1353, 'john': 1354, 'worry': 1355, 'navigate': 1356, 'thnks': 1357, 'progrmr': 1358, '9pm': 1359, '9am': 1360, 'quit': 1361, 'hardly': 1362, 'surprising': 1363, 'roses': 1364, 'emotive': 1365, 'poetry': 1366, 'frequentflyer': 1367, 'break': 1368, 'apologize': 1369, 'kb': 1370, 'londondairy': 1371, 'icecream': 1372, 'experience': 1373, 'past': 1374, 'cover': 1375, 'sin': 1376, 'excited': 1377, \":')\": 1378, 'xxx': 1379, 'jim': 1380, 'chuckle': 1381, 'shopping': 1382, 'cake': 1383, 'doh': 1384, '500': 1385, 'subscriber': 1386, 'reach': 1387, 'scorch': 1388, 'summer': 1389, 'young': 1390, 'woman': 1391, 'stamen': 1392, 'expect': 1393, 'anything': 1394, 'less': 1395, 'tweeties': 1396, 'fab': 1397, 'dont': 1398, '-->': 1399, '10': 1400, 'loner': 1401, 'introduce': 1402, 'v': 1403, 'alter': 1404, 'understanding': 1405, 'spread': 1406, 'problem': 1407, 'supa': 1408, 'dupa': 1409, 'near': 1410, 'dartmoor': 1411, 'gold': 1412, 'colour': 1413, 'ok': 1414, 'someday': 1415, 'r': 1416, 'dii': 1417, 'n': 1418, 'forget': 1419, 'si': 1420, 'smf': 1421, 'ft': 1422, 'japanese': 1423, 'import': 1424, 'kitty': 1425, 'matching': 1426, 'stationary': 1427, 'draw': 1428, 'close': 1429, 'specialise': 1430, 'thermal': 1431, 'image': 1432, 'survey': 1433, '–': 1434, 'south': 1435, 'korea': 1436, 'scamper': 1437, 'alarm': 1438, \"ain't\": 1439, 'mad': 1440, 'chweina': 1441, 'xd': 1442, 'jotzh': 1443, 'waste': 1444, 'place': 1445, 'completely': 1446, 'worth': 1447, 'coat': 1448, 'beforehand': 1449, 'tho': 1450, 'foh': 1451, 'outside': 1452, 'holiday': 1453, 'menace': 1454, 'jojo': 1455, 'ta': 1456, 'accepted': 1457, 'guys': 1458, 'admin': 1459, 'lukris': 1460, '😘': 1461, 'momma': 1462, 'bear': 1463, '❤': 1464, '️': 1465, 'redo': 1466, '8th': 1467, 'v.ball': 1468, 'atm': 1469, 'retweets': 1470, 'build': 1471, 'pack': 1472, 'suitcase': 1473, 'hang-copying': 1474, 'translation': 1475, \"dostoevsky's\": 1476, 'voucher': 1477, 'bugatti': 1478, 'bra': 1479, 'مطعم_هاشم': 1480, 'yummy': 1481, 'a7la': 1482, 'bdayt': 1483, 'mnwreeen': 1484, 'jazz': 1485, 'truck': 1486, 'x34': 1487, 'speak': 1488, 'pbevent': 1489, 'hq': 1490, 'yoona': 1491, 'hairpin': 1492, 'otp': 1493, 'collection': 1494, 'mastership': 1495, 'honey': 1496, 'paindo': 1497, 'await': 1498, 'report': 1499, 'manny': 1500, 'asshole': 1501, 'brijresidency': 1502, 'structure': 1503, '156': 1504, 'unit': 1505, 'encompass': 1506, 'bhk': 1507, 'flat': 1508, '91': 1509, '975-580-': 1510, '444': 1511, 'honor': 1512, 'curry': 1513, 'clash': 1514, 'milano': 1515, '👌': 1516, 'followback': 1517, ':-D': 1518, 'legit': 1519, 'loser': 1520, 'dead': 1521, 'starsquad': 1522, '⭐': 1523, 'news': 1524, 'utc': 1525, 'flume': 1526, 'kaytranada': 1527, 'alunageorge': 1528, 'ticket': 1529, 'kms': 1530, 'certainty': 1531, 'solve': 1532, 'faster': 1533, '👊': 1534, 'hurry': 1535, 'totem': 1536, 'somewhere': 1537, 'alice': 1538, 'dog': 1539, 'cat': 1540, 'goodwynsgoodies': 1541, 'ugh': 1542, 'fade': 1543, 'moan': 1544, 'leeds': 1545, 'jozi': 1546, 'wasnt': 1547, 'fifth': 1548, 'available': 1549, 'tix': 1550, 'pa': 1551, 'ba': 1552, 'ng': 1553, 'atl': 1554, 'coldplay': 1555, 'favorite': 1556, 'scientist': 1557, 'yellow': 1558, 'atlas': 1559, 'yein': 1560, 'selos': 1561, 'jabongatpumaurbanstampede': 1562, 'an': 1563, '7': 1564, 'timely': 1565, 'arrival': 1566, 'waiter': 1567, 'bill': 1568, 'sir': 1569, 'title': 1570, 'pocket': 1571, 'wripped': 1572, 'jean': 1573, 'connie': 1574, 'crew': 1575, 'staff': 1576, 'sweetan': 1577, 'ask': 1578, 'filming': 1579, 'mum': 1580, 'beg': 1581, 'soprano': 1582, 'ukraine': 1583, 'x33': 1584, 'olly': 1585, 'disney.arts': 1586, 'elmoprinssi': 1587, 'tired': 1588, 'salsa': 1589, 'dance': 1590, 'tell': 1591, 'truth': 1592, 'pls': 1593, '4-6': 1594, 'interest': 1595, '2nd': 1596, 'blogiversary': 1597, 'review': 1598, 'cutie': 1599, 'bohol': 1600, 'briliant': 1601, 'key': 1602, 'annual': 1603, 'productive': 1604, 'far': 1605, 'spin': 1606, 'voice': 1607, '\\U000fe334': 1608, 'yeheyy': 1609, 'pinya': 1610, 'whoooah': 1611, 'trance': 1612, 'lover': 1613, 'subject': 1614, 'physic': 1615, 'stop': 1616, 'ब': 1617, 'matter': 1618, 'jungle': 1619, 'accommodate': 1620, 'secret': 1621, 'behind': 1622, 'sandroforceo': 1623, 'ceo': 1624, '1month': 1625, 'swag': 1626, 'mia': 1627, 'workinprogress': 1628, 'finnigan': 1629, 'loyal': 1630, 'royal': 1631, 'fotoset': 1632, 'reusful': 1633, 'seem': 1634, 'somebody': 1635, 'sell': 1636, 'understand': 1637, 'muntu': 1638, 'another': 1639, 'gem': 1640, 'falcos': 1641, 'supersmash': 1642, 'hotnsexy': 1643, 'friskyfriday': 1644, 'beach': 1645, 'movie': 1646, 'crop': 1647, 'nash': 1648, 'tissue': 1649, 'chocolate': 1650, 'tea': 1651, 'hannibal': 1652, 'episode': 1653, 'hotbed': 1654, 'bush': 1655, 'classicassures': 1656, 'thrill': 1657, 'international': 1658, 'assignment': 1659, 'aerial': 1660, 'camera': 1661, 'operator': 1662, 'wales': 1663, 'boom': 1664, 'hong': 1665, 'kong': 1666, 'ferry': 1667, 'central': 1668, 'girlfriend': 1669, 'after-work': 1670, 'dj': 1671, 'resto': 1672, 'drinkt': 1673, 'koffie': 1674, 'a6': 1675, 'stargate': 1676, 'atlantis': 1677, 'muaahhh': 1678, 'ohh': 1679, 'hii': 1680, '🙈': 1681, 'di': 1682, 'nagsend': 1683, 'yung': 1684, 'ko': 1685, '</3': 1686, 'ulit': 1687, '🎉': 1688, '🎈': 1689, 'ugly': 1690, 'leggete': 1691, 'qui': 1692, 'per': 1693, 'la': 1694, 'mar': 1695, 'encourage': 1696, 'employer': 1697, 'board': 1698, 'sticker': 1699, 'sponsor': 1700, 'prize': 1701, '(:': 1702, 'milo': 1703, 'aurini': 1704, 'juicebro': 1705, 'fucking': 1706, 'pillar': 1707, 'respective': 1708, 'boii': 1709, 'smashingbook': 1710, 'bible': 1711, 'ill': 1712, 'sick': 1713, 'lamo': 1714, 'fangirl': 1715, 'platonic': 1716, 'science': 1717, 'resident': 1718, 'servicewithasmile': 1719, 'fams': 1720, 'bloodline': 1721, 'husky': 1722, 'obituary': 1723, 'advert': 1724, 'goofingaround': 1725, 'madness': 1726, 'bollywood': 1727, 'giveaway': 1728, 'dah': 1729, 'nothing': 1730, 'bitterness': 1731, 'anger': 1732, 'hatred': 1733, 'towards': 1734, 'pure': 1735, 'indifference': 1736, 'suite': 1737, 'zach': 1738, 'cody': 1739, 'deliver': 1740, 'ac': 1741, 'excellence': 1742, 'producer': 1743, 'boggling': 1744, 'fatigue': 1745, 'baareeq': 1746, 'gamedev': 1747, 'hobby': 1748, 'tweenie_fox': 1749, 'click': 1750, 'accessory': 1751, 'tamang': 1752, 'hinala': 1753, 'niam': 1754, 'selfieee': 1755, 'especially': 1756, 'lass': 1757, 'aling': 1758, 'swim': 1759, 'perfection': 1760, 'bout': 1761, 'goodbye': 1762, 'feminist': 1763, 'fight': 1764, 'snobby': 1765, 'bitch': 1766, 'caroline': 1767, 'mighty': 1768, '🔥': 1769, 'hbd': 1770, 'follback': 1771, 'jog': 1772, 'remote': 1773, 'newly': 1774, 'ebay': 1775, 'store': 1776, 'disneyinfinity': 1777, 'starwars': 1778, 'character': 1779, 'preorder': 1780, 'starter': 1781, 'hit': 1782, 'snap': 1783, 'homies': 1784, 'skin': 1785, 'bday': 1786, 'chant': 1787, 'jai': 1788, 'italy': 1789, 'fast': 1790, 'heeeyyy': 1791, 'woah': 1792, '★': 1793, '😊': 1794, 'whenever': 1795, 'ang': 1796, 'kiss': 1797, 'philippine': 1798, 'package': 1799, 'bruise': 1800, 'rib': 1801, '😀': 1802, '😁': 1803, '😂': 1804, '😃': 1805, '😅': 1806, '😉': 1807, 'tombraider': 1808, 'hype': 1809, 'thejuiceinthemix': 1810, 'rela': 1811, 'building': 1812, 'low': 1813, 'priority': 1814, 'match': 1815, 'harry': 1816, 'bc': 1817, 'opportune': 1818, 'collapse': 1819, 'chaotic': 1820, 'cosas': 1821, '<---': 1822, 'alliteration': 1823, 'oppayaa': 1824, \"how's\": 1825, 'natgeo': 1826, 'lick': 1827, 'elbow': 1828, '. .': 1829, 'interesting': 1830, '“': 1831, 'emu': 1832, 'stoke': 1833, \"people's\": 1834, 'approval': 1835, \"god's\": 1836, 'jisung': 1837, 'kid': 1838, 'sunshine': 1839, 'mm': 1840, 'nicola': 1841, 'brighten': 1842, 'helen': 1843, 'brian': 1844, '2-3': 1845, 'australia': 1846, 'ol': 1847, 'bone': 1848, 'creaking': 1849, 'abuti': 1850, 'tweetland': 1851, 'android': 1852, 'xmas': 1853, 'skyblock': 1854, 'standing': 1855, 'bcause': 1856, '2009': 1857, 'die': 1858, 'twitch': 1859, 'sympathy': 1860, 'laugh': 1861, 'unnieee': 1862, 'nuka': 1863, 'penacova': 1864, 'djset': 1865, 'edm': 1866, 'kizomba': 1867, 'latinhouse': 1868, 'housemusic': 1869, 'portugal': 1870, 'wild': 1871, 'ride': 1872, 'anytime': 1873, 'taste': 1874, 'yer': 1875, 'mtn': 1876, 'maganda': 1877, 'mistress': 1878, 'saphire': 1879, 'busy': 1880, '4000': 1881, 'instagram': 1882, 'among': 1883, 'coconut': 1884, 'sambal': 1885, 'mussel': 1886, 'recipe': 1887, 'kalin': 1888, 'mixcloud': 1889, 'sarcasm': 1890, 'chelsea': 1891, 'he': 1892, 'useless': 1893, 'thursday': 1894, 'hang': 1895, 'hehe': 1896, 'benson': 1897, 'facebook': 1898, 'solid': 1899, '16/17': 1900, '30': 1901, '°': 1902, '😜': 1903, 'maryhicks': 1904, 'kikmeboys': 1905, 'photooftheday': 1906, 'musicbiz': 1907, 'sheskindahot': 1908, 'fleekile': 1909, 'mbalula': 1910, 'africa': 1911, 'mexican': 1912, 'scar': 1913, 'office': 1914, 'donut': 1915, 'foiegras': 1916, 'despite': 1917, 'weather': 1918, 'wedding': 1919, 'tony': 1920, 'stark': 1921, 'incredible': 1922, 'poem': 1923, 'bubble': 1924, 'dale': 1925, 'billion': 1926, 'magical': 1927, 'op': 1928, 'cast': 1929, 'vote': 1930, 'election': 1931, 'jcreport': 1932, 'piggin': 1933, 'peace': 1934, 'botanical': 1935, 'soap': 1936, 'upload': 1937, 'freshly': 1938, '3weeks': 1939, 'heal': 1940, 'exciting': 1941, 'tobi-bro': 1942, 'isp': 1943, 'steel': 1944, 'wednesday': 1945, 'swear': 1946, 'earlier': 1947, 'cam': 1948, '😭': 1949, 'except': 1950, \"masha'allah\": 1951, 'french': 1952, 'wwat': 1953, 'france': 1954, 'yaaay': 1955, 'beiruting': 1956, 'coffee': 1957, 'panda': 1958, 'eonnie': 1959, 'favourite': 1960, 'soda': 1961, 'fuller': 1962, 'shit': 1963, 'healthy': 1964, '💓': 1965, 'rettweet': 1966, 'mvg': 1967, 'valuable': 1968, 'madrid': 1969, 'sore': 1970, 'bergerac': 1971, 'u21': 1972, 'individual': 1973, 'excellent': 1974, 'adam': 1975, \"beach's\": 1976, 'suicide': 1977, 'squad': 1978, 'fond': 1979, 'christopher': 1980, 'initially': 1981, 'cocky': 1982, 'prove': 1983, \"attitude's\": 1984, 'improve': 1985, 'suggest': 1986, 'date': 1987, 'indeed': 1988, 'happys': 1989, 'intelligent': 1990, 'cs': 1991, 'certain': 1992, 'exam': 1993, 'forgot': 1994, 'home-based': 1995, 'knee': 1996, 'sale': 1997, 'fleur': 1998, 'dress': 1999, 'readystock_hijabmart': 2000, 'idr': 2001, '325.000': 2002, '200.000': 2003, 'tompolo': 2004, 'aim': 2005, 'cannot': 2006, 'buyer': 2007, 'disappoint': 2008, 'paper': 2009, 'slacking': 2010, 'crack': 2011, 'particularly': 2012, 'striking': 2013, '31': 2014, 'mam': 2015, 'feytyaz': 2016, 'instant': 2017, 'stiffening': 2018, 'ricky_febs': 2019, 'grindea': 2020, 'courier': 2021, 'crypt': 2022, 'possibly': 2023, 'arma': 2024, 'record': 2025, 'gosh': 2026, 'limbo': 2027, 'retweeting': 2028, 'orchard': 2029, 'art': 2030, 'super': 2031, 'karachi': 2032, 'ka': 2033, 'venice': 2034, 'several': 2035, 'part': 2036, 'witness': 2037, 'accumulate': 2038, 'maroon': 2039, 'cocktail': 2040, '0-100': 2041, 'quick': 2042, '1100d': 2043, 'auto-focus': 2044, 'manual': 2045, 'vein': 2046, 'crackle': 2047, 'glaze': 2048, 'layout': 2049, 'bomb': 2050, 'social': 2051, 'website': 2052, 'pake': 2053, 'joim': 2054, 'fee': 2055, 'troop': 2056, 'beauty': 2057, 'mail': 2058, 'ladolcevitainluxembourg@hotmail.com': 2059, 'prrequest': 2060, 'journorequest': 2061, 'the_madstork': 2062, 'shaun': 2063, 'bot': 2064, 'chloe': 2065, 'actress': 2066, 'away': 2067, 'wicked': 2068, 'hola': 2069, 'juan': 2070, 'sending': 2071, 'houston': 2072, 'tx': 2073, 'jenni': 2074, \"year's\": 2075, 'stumble': 2076, 'upon': 2077, 'prob.nice': 2078, 'choker': 2079, 'btw': 2080, 'seouljins': 2081, 'photoset': 2082, 'sadomasochistsparadise': 2083, 'wynter': 2084, 'bottom': 2085, 'outtake': 2086, 'sadomasochist': 2087, 'paradise': 2088, 'cuties': 2089, 'ty': 2090, 'bby': 2091, 'clip': 2092, 'lose': 2093, 'cypher': 2094, 'amen': 2095, 'x32': 2096, 'plant': 2097, 'allow': 2098, 'corner': 2099, 'addict': 2100, 'gurl': 2101, 'suck': 2102, 'special': 2103, 'owe': 2104, 'daniel': 2105, 'ape': 2106, 'saar': 2107, 'ahead': 2108, 'verse': 2109, 'butterfly': 2110, 'bonus': 2111, 'fill': 2112, 'tear': 2113, 'laughter': 2114, '5sos': 2115, 'yummmyyy': 2116, 'dosa': 2117, 'unless': 2118, 'achi': 2119, 'youuu': 2120, 'bawi': 2121, 'ako': 2122, 'queenesther': 2123, 'sharp': 2124, 'wonder': 2125, 'poldi': 2126, 'cimbom': 2127, 'buddy': 2128, 'bruhhh': 2129, 'daddy': 2130, '”': 2131, 'communal': 2132, 'knowledge': 2133, 'attention': 2134, '1tb': 2135, 'bank': 2136, 'credit': 2137, 'department': 2138, 'anz': 2139, 'extreme': 2140, 'offshoring': 2141, 'absolutely': 2142, 'classic': 2143, 'gottolovebanks': 2144, 'yup': 2145, 'in-shaa-allah': 2146, 'dua': 2147, 'thru': 2148, 'aameen': 2149, '4/5': 2150, 'coca': 2151, 'cola': 2152, 'fanta': 2153, 'pepsi': 2154, 'sprite': 2155, 'alls': 2156, 'sweeety': 2157, ';-)': 2158, 'welcometweet': 2159, 'psygustokita': 2160, 'setup': 2161, 'wet': 2162, 'foot': 2163, 'carpet': 2164, 'judgmental': 2165, 'hypocritical': 2166, 'narcissist': 2167, 'jumpsuit': 2168, 'bt': 2169, 'denim': 2170, 'verge': 2171, 'owl': 2172, 'constant': 2173, 'run': 2174, 'sia': 2175, 'count': 2176, 'brilliant': 2177, 'teacher': 2178, 'comparative': 2179, 'religion': 2180, 'rant': 2181, 'student': 2182, 'benchers': 2183, '1/5': 2184, 'porsche': 2185, 'paddock': 2186, 'budapestgp': 2187, 'johnyherbert': 2188, 'roll': 2189, 'porschesupercup': 2190, 'koyal': 2191, 'melody': 2192, 'unexpected': 2193, 'create': 2194, 'memory': 2195, '35': 2196, 'eps': 2197, 'wirh': 2198, 'arc': 2199, 'x31': 2200, 'wolf': 2201, 'fulfill': 2202, 'desire': 2203, 'ameen': 2204, 'kca': 2205, 'votejkt': 2206, '48id': 2207, 'helpinggroupdms': 2208, 'quote': 2209, 'weird': 2210, 'dp': 2211, 'wife': 2212, 'poor': 2213, 'chick': 2214, 'guide': 2215, 'zonzofox': 2216, 'bhaiya': 2217, 'brother': 2218, 'lucky': 2219, 'patty': 2220, 'elaborate': 2221, 'kuching': 2222, 'rate': 2223, 'merdeka': 2224, 'palace': 2225, 'hotel': 2226, 'plusmiles': 2227, 'service': 2228, 'hahahaa': 2229, 'nex': 2230, 'safe': 2231, 'gwd': 2232, 'shes': 2233, 'okok': 2234, '33': 2235, 'idiot': 2236, 'chaerin': 2237, 'unnie': 2238, 'viable': 2239, 'alternative': 2240, 'nowadays': 2241, 'pass': 2242, 'ip': 2243, 'tombow': 2244, 'abt': 2245, 'friyay': 2246, 'smug': 2247, 'marrickville': 2248, 'public': 2249, 'ten': 2250, 'ago': 2251, 'eighteen': 2252, 'auvssscr': 2253, 'ncaaseason': 2254, 'slow': 2255, 'popsicle': 2256, 'soft': 2257, 'melt': 2258, 'mouth': 2259, 'thankyouuu': 2260, 'dianna': 2261, 'ngga': 2262, 'usah': 2263, 'dipikirin': 2264, 'elah': 2265, 'easily': 2266, \"who's\": 2267, 'entp': 2268, 'killin': 2269, 'meme': 2270, 'worthy': 2271, 'shot': 2272, 'emon': 2273, 'decent': 2274, 'outdoor': 2275, 'rave': 2276, 'dv': 2277, 'aku': 2278, 'bakal': 2279, 'liat': 2280, 'kak': 2281, 'merry': 2282, 'tv': 2283, 'outfit': 2284, '--->': 2285, 'fashionfriday': 2286, 'angle.nelson': 2287, 'cheap': 2288, 'mymonsoonstory': 2289, 'tree': 2290, 'lotion': 2291, 'moisturize': 2292, 'important': 2293, 'monsoon': 2294, 'whoop': 2295, 'romantic': 2296, 'valencia': 2297, 'daaru': 2298, 'party': 2299, 'chaddi': 2300, 'bros': 2301, 'wonderful.great': 2302, 'closely': 2303, 'trim': 2304, 'pubes': 2305, 'mi': 2306, 'tio': 2307, 'sinaloa': 2308, 'arre': 2309, 'stylish': 2310, 'trendy': 2311, 'kim': 2312, 'fabfriday': 2313, 'facetime': 2314, 'calum': 2315, 'constantly': 2316, 'announce': 2317, 'filbarbarian': 2318, 'beer': 2319, 'broken': 2320, 'arm': 2321, 'testicle': 2322, 'light': 2323, 'katerina': 2324, 'maniataki': 2325, 'ahh': 2326, 'alright': 2327, 'worthwhile': 2328, 'judging': 2329, 'tech': 2330, 'window': 2331, 'stupid': 2332, 'plugin': 2333, 'bass': 2334, 'slap': 2335, '6pm': 2336, 'door': 2337, 'vip': 2338, 'general': 2339, 'seat': 2340, 'early': 2341, 'london': 2342, 'toptravelcentar': 2343, 'ttctop': 2344, 'lux': 2345, 'luxurytravel': 2346, 'beograd': 2347, 'srbija': 2348, 'putovanja': 2349, 'wendy': 2350, 'provide': 2351, 'fresh': 2352, 'drainage': 2353, 'homebound': 2354, 'hahahays': 2355, 'yeeeah': 2356, 'moar': 2357, 'kittehs': 2358, 'incoming': 2359, 'tower': 2360, 'yippeee': 2361, 'scrummy': 2362, 'bio': 2363, 'mcpe': 2364, '->': 2365, 'vainglory': 2366, 'driver': 2367, '6:01': 2368, 'lilydale': 2369, 'f': 2370, 'raise': 2371, 'magicalmysterytour': 2372, 'chek': 2373, 'rule': 2374, 'weebly': 2375, 'donetsk': 2376, 'earth': 2377, 'personalise': 2378, 'wrap': 2379, 'business': 2380, 'stationery': 2381, 'adrian': 2382, 'parcel': 2383, 'tuesday': 2384, 'pris': 2385, '80': 2386, 'wz': 2387, 'pattern': 2388, 'cut': 2389, 'buttonhole': 2390, 'finishing': 2391, '4my': 2392, 'designer': 2393, 'famous': 2394, 'client': 2395, 'p': 2396, 'alive': 2397, 'trial': 2398, 'spm': 2399, 'dinooo': 2400, 'cardio': 2401, 'steak': 2402, 'cue': 2403, 'laptop': 2404, 'excite': 2405, 'guinea': 2406, 'pig': 2407, 'bestfriends': 2408, 'salamat': 2409, 'sa': 2410, 'mga': 2411, 'nag.greet': 2412, 'appreciated': 2413, 'guise': 2414, 'godbless': 2415, 'crush': 2416, 'apple': 2417, 'ga': 2418, 'deserve': 2419, 'charles': 2420, 'workhard': 2421, 'model': 2422, 'forrit': 2423, 'bread': 2424, 'bacon': 2425, 'butter': 2426, 'afang': 2427, 'soup': 2428, 'semo': 2429, 'brb': 2430, 'force': 2431, 'doesnt': 2432, 'tato': 2433, 'bulat': 2434, 'discuss': 2435, 'suggestion': 2436, 'concerned': 2437, 'snake': 2438, 'perform': 2439, 'con': 2440, 'todayyy': 2441, 'max': 2442, 'gaza': 2443, 'retweet': 2444, 'bbb': 2445, 'peacefully': 2446, 'pc': 2447, '22': 2448, 'legal': 2449, 'ditch': 2450, 'tory': 2451, 'bajrangibhaijaanhighestweek': 2452, \"s'okay\": 2453, 'andy': 2454, 'you-and': 2455, 'return': 2456, 'tuitutil': 2457, 'bud': 2458, 'learn': 2459, 'takeaway': 2460, 'slept': 2461, 'instead': 2462, '1hr': 2463, 'genial': 2464, 'competition': 2465, 'yosh': 2466, 'procrastinate': 2467, 'plus': 2468, 'sorting': 2469, 'kfc': 2470, 'itunes': 2471, 'dedicatedfan': 2472, '💜': 2473, 'daft': 2474, 'teethe': 2475, 'trouble': 2476, 'huxley': 2477, 'basket': 2478, 'ben': 2479, 'gamer': 2480, 'active': 2481, '120': 2482, 'distance': 2483, 'suitable': 2484, 'final': 2485, 'stockholm': 2486, 'zack': 2487, 'destroy': 2488, 'heel': 2489, 'claw': 2490, 'q': 2491, 'blonde': 2492, 'box': 2493, 'cheerio': 2494, 'seed': 2495, 'cutest': 2496, 'ffback': 2497, 'spotify': 2498, \"we've\": 2499, 'vc': 2500, 'tgp': 2501, 'race': 2502, 'average': 2503, \"joe's\": 2504, 'bluejays': 2505, 'vinylbear': 2506, 'pal': 2507, 'furbaby': 2508, 'luff': 2509, 'mega': 2510, 'retail': 2511, 'boot': 2512, 'whsmith': 2513, 'ps3': 2514, 'shannon': 2515, 'na': 2516, 'redecorate': 2517, 'bob': 2518, 'ellie': 2519, 'mairi': 2520, 'workout': 2521, 'impair': 2522, 'uggghhh': 2523, 'dam': 2524, 'dun': 2525, 'eczema': 2526, 'sufferer': 2527, 'ndee': 2528, 'pleasure': 2529, 'publilius': 2530, 'syrus': 2531, 'fear': 2532, 'death': 2533, 'dread': 2534, 'fell': 2535, 'fuk': 2536, 'unblock': 2537, 'manually': 2538, 'tweak': 2539, 'php': 2540, 'fall': 2541, 'oomf': 2542, 'pippa': 2543, 'hschool': 2544, 'bus': 2545, 'cardi': 2546, 'everyday': 2547, 'everytime': 2548, 'hk': 2549, \"why'd\": 2550, 'acorn': 2551, 'originally': 2552, 'c64': 2553, 'apart': 2554, 'cpu': 2555, 'considerably': 2556, 'advanced': 2557, 'onair': 2558, 'bay': 2559, 'hold': 2560, 'river': 2561, '0878 0388': 2562, '1033': 2563, '0272 3306': 2564, '70': 2565, 'rescue': 2566, 'mutt': 2567, 'confirm': 2568, 'delivery': 2569, 'switch': 2570, 'lap': 2571, 'optimize': 2572, 'lu': 2573, ':|': 2574, 'tweetofthedecade': 2575, ':P': 2576, 'class': 2577, 'happiest': 2578, 'bbmme': 2579, 'pin': 2580, '7df9e60a': 2581, 'bbm': 2582, 'bbmpin': 2583, 'addmeonbbm': 2584, 'addme': 2585, \"today's\": 2586, 'normal': 2587, 'menu': 2588, 'marry': 2589, 'glenn': 2590, 'whats': 2591, 'height': 2592, \"sculptor's\": 2593, 'ti5': 2594, 'dota': 2595, 'nudge': 2596, 'spot': 2597, 'tasty': 2598, 'hilly': 2599, 'cycle': 2600, 'england': 2601, 'scotlandismassive': 2602, 'gen': 2603, 'vikk': 2604, 'fna': 2605, 'mombasa': 2606, 'tukutanemombasa': 2607, '100reasonstovisitmombasa': 2608, 'karibumombasa': 2609, 'hanbin': 2610, 'certainly': 2611, 'goosnight': 2612, 'kindly': 2613, 'familiar': 2614, 'jealous': 2615, 'tent': 2616, 'yea': 2617, 'cozy': 2618, 'phenomenal': 2619, 'collab': 2620, 'birth': 2621, 'behave': 2622, 'monster': 2623, 'spree': 2624, '000': 2625, 'tank': 2626, 'outstanding': 2627, 'donation': 2628, 'h': 2629, 'contestkiduniya': 2630, 'mfundo': 2631, 'oche': 2632, 'hun': 2633, 'inner': 2634, 'nerd': 2635, 'tame': 2636, 'insidious': 2637, 'logic': 2638, 'math': 2639, 'channel': 2640, 'continue': 2641, 'doubt': 2642, '300': 2643, 'sub': 2644, '200': 2645, 'subs': 2646, 'forgiven': 2647, 'wonderfuls': 2648, 'mannerfuls': 2649, 'yhooo': 2650, 'ngi': 2651, 'mood': 2652, 'push': 2653, 'limit': 2654, 'obakeng': 2655, 'goat': 2656, 'alhamdullilah': 2657, 'pebble': 2658, 'engross': 2659, 'bing': 2660, 'scream': 2661, 'whole': 2662, 'wide': 2663, '🌎': 2664, '😧': 2665, 'wat': 2666, 'muahhh': 2667, 'pausetime': 2668, 'drift': 2669, 'loose': 2670, 'campaign': 2671, 'kickstarter': 2672, 'article': 2673, 'absolute': 2674, 'jenna': 2675, 'bellybutton': 2676, 'innie': 2677, 'outie': 2678, 'havent': 2679, 'delish': 2680, 'joselito': 2681, 'freya': 2682, 'nth': 2683, 'latepost': 2684, 'lupet': 2685, 'mo': 2686, 'eric': 2687, 'askaman': 2688, 'helpful': 2689, 'alternatively': 2690, '150': 2691, '0345': 2692, '454': 2693, '111': 2694, 'webz': 2695, 'oops': 2696, \"they'll\": 2697, 'realise': 2698, 'anymore': 2699, 'carmel': 2700, 'decision': 2701, 'matt': 2702, 'probs': 2703, '@commonculture': 2704, '@connorfranta': 2705, 'honestly': 2706, 'explain': 2707, 'relationship': 2708, 'pick': 2709, 'tessnzach': 2710, 'paperboy': 2711, 'honest': 2712, 'reassure': 2713, 'personal': 2714, 'mubank': 2715, \"dongwoo's\": 2716, 'bright': 2717, 'tommorow': 2718, 'newyork': 2719, 'magic': 2720, 'lolll': 2721, 'twinx': 2722, '16': 2723, 'path': 2724, 'firmansyahbl': 2725, 'usual': 2726, 'procedure': 2727, 'grim': 2728, 'fandango': 2729, 'ordinary': 2730, 'extraordinary': 2731, 'bos': 2732, 'birmingham': 2733, 'oracle': 2734, 'samosa': 2735, 'fireball': 2736, 'shoe': 2737, 'serve': 2738, 'sushi': 2739, 'shoeshi': 2740, '�': 2741, 'lymond': 2742, 'philippa': 2743, 'novel': 2744, 'tara': 2745, '. . .': 2746, 'aur': 2747, 'han': 2748, 'imran': 2749, 'khan': 2750, '63': 2751, 'agaaain': 2752, 'doli': 2753, 'siregar': 2754, 'ninh': 2755, 'size': 2756, 'geekiest': 2757, 'geek': 2758, 'wallet': 2759, 'das': 2760, 'request': 2761, 'medium': 2762, 'rally': 2763, 'rotate': 2764, 'direction': 2765, 'eek': 2766, 'red': 2767, 'beijing': 2768, 'meni': 2769, 'tebrik': 2770, 'etdi': 2771, '700': 2772, '💗': 2773, 'rod': 2774, 'embrace': 2775, 'actor': 2776, 'aplomb': 2777, 'foreveralone': 2778, 'mysummer': 2779, '01482': 2780, '333505': 2781, 'hahahaha': 2782, 'wear': 2783, 'uniform': 2784, 'evil': 2785, 'owww': 2786, 'choo': 2787, 'chweet': 2788, 'shorthaired': 2789, 'oscar': 2790, 'realize': 2791, 'harmony': 2792, 'judge': 2793, 'denerivery': 2794, '506': 2795, 'kiksexting': 2796, 'kikkomansabor': 2797, 'killer': 2798, 'henessydiaries': 2799, 'journey': 2800, 'band': 2801, 'plz': 2802, 'convo': 2803, '11': 2804, 'vault': 2805, 'expand': 2806, 'vinny': 2807, 'money': 2808, 'hahahahaha': 2809, '50cents': 2810, 'repay': 2811, 'debt': 2812, 'smiling': 2813, 'evet': 2814, 'wifi': 2815, 'lifestyle': 2816, 'qatarday': 2817, '. ..': 2818, '🌞': 2819, 'girly': 2820, 'india': 2821, 'innovate': 2822, 'volunteer': 2823, 'saran': 2824, 'drama': 2825, 'genre': 2826, 'romance': 2827, 'comedy': 2828, 'leanneriner': 2829, '19': 2830, 'porno': 2831, 'l4l': 2832, 'weloveyounamjoon': 2833, 'homey': 2834, 'kenya': 2835, 'emotional': 2836, 'roller': 2837, 'coaster': 2838, 'aspect': 2839, 'najam': 2840, 'confession': 2841, 'ad': 2842, 'pricelessantique': 2843, 'takesonetoknowone': 2844, 'extra': 2845, 'ucount': 2846, 'ji': 2847, 'turkish': 2848, 'crap': 2849, 'burn': 2850, '80x': 2851, 'airline': 2852, 'sexy': 2853, 'yello': 2854, 'gail': 2855, 'yael': 2856, 'lesson': 2857, 'en': 2858, 'manos': 2859, 'hand': 2860, 'manager': 2861, 'reader': 2862, 'dnt': 2863, 'ideal': 2864, 'weekly': 2865, 'idol': 2866, 'pose': 2867, 'shortlist': 2868, 'dominion': 2869, 'picnic': 2870, 'tmrw': 2871, 'nobody': 2872, 'jummamubarak': 2873, 'shower': 2874, 'shalwarkameez': 2875, 'itter': 2876, 'offer': 2877, 'jummaprayer': 2878, 'af': 2879, 'display': 2880, 'enable': 2881, 'company': 2882, 'peep': 2883, 'tweeps': 2884, 'folow': 2885, '2k': 2886, 'ohhh': 2887, 'teaser': 2888, 'airecs': 2889, '009': 2890, 'acid': 2891, 'mouse': 2892, 'ep': 2893, '31st': 2894, 'include': 2895, 'robin': 2896, 'rough': 2897, 'control': 2898, 'remixes': 2899, 'rts': 2900, 'faves': 2901, 'toss': 2902, 'lady': 2903, '🐑': 2904, 'library': 2905, 'mr2': 2906, 'climb': 2907, 'cuddle': 2908, 'jilla': 2909, 'headline': 2910, '2017': 2911, 'jumma': 2912, 'mubarik': 2913, 'total': 2914, 'congratz': 2915, 'contribution': 2916, '2.0': 2917, 'yuppiieee': 2918, 'alienthought': 2919, 'happyalien': 2920, 'crowd': 2921, 'loud': 2922, 'gary': 2923, 'particular': 2924, 'attraction': 2925, 'supprt': 2926, 'savage': 2927, 'cleanse': 2928, 'scam': 2929, 'ridden': 2930, 'vyapam': 2931, 'rename': 2932, 'wave': 2933, 'couch': 2934, 'dodge': 2935, 'explanation': 2936, 'bag': 2937, 'sanza': 2938, 'yaa': 2939, 'slr': 2940, 'som': 2941, 'honour': 2942, 'hehehe': 2943, 'view': 2944, 'explorer': 2945, 'wayanadan': 2946, 'forest': 2947, 'wayanad': 2948, 'srijith': 2949, 'whisper': 2950, 'lie': 2951, 'pokemon': 2952, 'dazzle': 2953, 'urself': 2954, 'double': 2955, 'flare': 2956, 'black': 2957, '9': 2958, '51': 2959, 'browse': 2960, 'bore': 2961, 'female': 2962, 'tour': 2963, 'delve': 2964, 'muchhh': 2965, 'tmr': 2966, 'breakfast': 2967, 'gl': 2968, \"tonight's\": 2969, '):': 2970, 'litey': 2971, 'manuella': 2972, 'maine': 2973, 'abhi': 2974, 'tak': 2975, 'ye': 2976, 'nhi': 2977, 'dekhi': 2978, 'promos': 2979, 'se': 2980, 'welcomed': 2981, 'xpax': 2982, 'lisa': 2983, 'aboard': 2984, 'institution': 2985, 'nc': 2986, 'cheese': 2987, 'overload': 2988, 'pizza': 2989, '•': 2990, 'mcfloat': 2991, 'fudge': 2992, 'sandae': 2993, 'munchkins': 2994, \"d'd\": 2995, 'granny': 2996, 'baller': 2997, 'lil': 2998, 'chain': 2999, 'everybody': 3000, 'ought': 3001, 'jay': 3002, 'events@breastcancernow.org': 3003, '79x': 3004, 'champion': 3005, 'letter': 3006, 'approve': 3007, 'unique': 3008, 'affaraid': 3009, 'dearslim': 3010, 'role': 3011, 'billy': 3012, 'labs': 3013, 'ovh': 3014, 'maxi': 3015, 'bunch': 3016, 'acc': 3017, 'sprit': 3018, 'yous': 3019, 'til': 3020, 'severe': 3021, 'hammies': 3022, 'freedom': 3023, 'pistol': 3024, 'unlock': 3025, 'bemeapp': 3026, 'thumb': 3027, 'beme': 3028, 'bemecode': 3029, 'proudtobeme': 3030, 'round': 3031, 'calm': 3032, 'kepo': 3033, 'luckily': 3034, 'clearly': 3035, 'دعمم': 3036, 'للعودة': 3037, 'للحياة': 3038, 'heiyo': 3039, 'dudaftie': 3040, 'breaktym': 3041, 'fatal': 3042, 'dangerous': 3043, 'term': 3044, 'health': 3045, 'outraged': 3046, '645k': 3047, 'muna': 3048, 'magstart': 3049, 'salute': 3050, '→': 3051, 'thq': 3052, 'continous': 3053, 'thalaivar': 3054, '£': 3055, 'heiya': 3056, 'grab': 3057, '30.000': 3058, 'av': 3059, 'gd': 3060, 'wknd': 3061, 'ear': 3062, 'yesss': 3063, \"y'day\": 3064, 'hxh': 3065, 'besides': 3066, 'vids': 3067, 'badass': 3068, 'killua': 3069, 'scene': 3070, 'suffering': 3071, 'feed': 3072, '78x': 3073, 'unappreciated': 3074, 'gracious': 3075, 'nailedit': 3076, 'ourdisneyinfinity': 3077, 'mary': 3078, 'jillmill': 3079, 'webcam': 3080, 'elfindelmundo': 3081, 'sexi': 3082, 'mainly': 3083, 'favour': 3084, 'dancetastic': 3085, 'satyajit': 3086, \"ray's\": 3087, 'porosh': 3088, 'pathor': 3089, 'situation': 3090, 'goldbugs': 3091, 'wine': 3092, 'bottle': 3093, 'spill': 3094, 'jazmin': 3095, 'bonilla': 3096, '15000': 3097, 'star': 3098, 'hollywood': 3099, 'rofl': 3100, 'shade': 3101, 'grey': 3102, 'netsec': 3103, 'edition': 3104, 'ate': 3105, 'kev': 3106, 'apology': 3107, 'fangirled': 3108, 'sister': 3109, 'unlisted': 3110, 'hickey': 3111, 'dad': 3112, 'hock': 3113, 'mamma': 3114, 'human': 3115, 'being': 3116, 'mere': 3117, 'holistic': 3118, 'cosmovision': 3119, 'narrow-minded': 3120, 'charge': 3121, 'ce': 3122, 'alix': 3123, 'quan': 3124, 'tip': 3125, 'naaahhh': 3126, 'duh': 3127, 'emesh': 3128, 'hilarious': 3129, 'kath': 3130, 'kia': 3131, '@vauk': 3132, 'tango': 3133, 'tracerequest': 3134, 'homie': 3135, 'dassy': 3136, 'fwm': 3137, 'selamat': 3138, 'nichola': 3139, 'found': 3140, 'malta': 3141, 'gto': 3142, 'tomorrowland': 3143, 'incall': 3144, 'shobs': 3145, 'incomplete': 3146, 'barkada': 3147, 'silverstone': 3148, 'pull': 3149, 'bookstore': 3150, 'lately': 3151, 'ganna': 3152, 'hillary': 3153, 'clinton': 3154, 'court': 3155, 'notice': 3156, 'slice': 3157, 'life-so': 3158, 'hide': 3159, 'untapped': 3160, 'mca': 3161, 'gettin': 3162, 'hella': 3163, 'wana': 3164, 'bandz': 3165, 'hell': 3166, 'donington': 3167, 'park': 3168, '24/25': 3169, 'hop': 3170, 'x30': 3171, 'merci': 3172, 'bien': 3173, 'amie': 3174, 'pitbull': 3175, '777x': 3176, 'fri': 3177, 'annyeong': 3178, 'oppa': 3179, 'indonesian': 3180, 'elf': 3181, 'flight': 3182, 'bf': 3183, 'jennyjean': 3184, 'kikchat': 3185, 'sabadodeganarseguidores': 3186, 'sexysasunday': 3187, 'marseille': 3188, 'ganda': 3189, 'fnaf': 3190, 'steam': 3191, 'assure': 3192, 'current': 3193, 'goin': 3194, 'sweety': 3195, \"spot's\": 3196, 'barnstaple': 3197, 'bideford': 3198, 'abit': 3199, 'road': 3200, 'rocro': 3201, '13glodyysbro': 3202, 'hire': 3203, '2ne1': 3204, 'aspetti': 3205, 'chicken': 3206, 'chip': 3207, 'cupboard': 3208, 'empty': 3209, 'jamie': 3210, 'ian': 3211, 'latin': 3212, 'asian': 3213, 'version': 3214, 'fave': 3215, 'vaing': 3216, '642': 3217, 'kikgirl': 3218, 'orgasm': 3219, 'phonesex': 3220, 'spacers': 3221, 'felicity': 3222, 'smoak': 3223, '👓': 3224, '💘': 3225, 'child': 3226, 'psychopaths': 3227, 'spoile': 3228, 'dimple': 3229, 'contemplate': 3230, 'indie': 3231, 'route': 3232, 'jsl': 3233, '76x': 3234, 'gotcha': 3235, 'kina': 3236, 'donna': 3237, 'reachability': 3238, 'jk': 3239, 'bitter': 3240, 's02e04': 3241, 'air': 3242, 'naggy': 3243, 'anal': 3244, 'vidcon': 3245, 'anxious': 3246, 'shake': 3247, '10:30': 3248, 'smoke': 3249, 'white': 3250, 'grandpa': 3251, 'prolly': 3252, 'stash': 3253, 'closer-chasing': 3254, 'spec': 3255, 'league': 3256, 'chase': 3257, 'wall': 3258, 'angel': 3259, 'mochamichelle': 3260, 'iph': 3261, '0ne': 3262, 'simply': 3263, 'bi0': 3264, 'x29': 3265, 'there': 3266, 'background': 3267, 'maggie': 3268, 'afraid': 3269, 'mull': 3270, 'nil': 3271, 'glasgow': 3272, 'netball': 3273, 'thistle': 3274, 'thistlelove': 3275, 'effect': 3276, 'minecraft': 3277, 'boring': 3278, 'drew': 3279, 'delicious': 3280, 'muddle': 3281, 'racket': 3282, 'isolate': 3283, 'fa': 3284, 'participate': 3285, 'icecreammaster': 3286, 'group': 3287, 'huhu': 3288, 'shet': 3289, 'desk': 3290, 'o_o': 3291, 'orz': 3292, 'problemmm': 3293, '75x': 3294, 'english': 3295, 'yeeaayy': 3296, 'alhamdulillah': 3297, 'amin': 3298, 'weed': 3299, 'definition': 3300, 'crowdfunding': 3301, 'goal': 3302, 'walk': 3303, 'hellooo': 3304, 'selection': 3305, 'lynne': 3306, 'buffer': 3307, 'button': 3308, 'composer': 3309, 'fridayfun': 3310, 'non-filipina': 3311, 'ejayster': 3312, 'united': 3313, 'state': 3314, 'le': 3315, 'stan': 3316, 'lee': 3317, 'discovery': 3318, 'cousin': 3319, '1400': 3320, 'yrs': 3321, 'teleportation': 3322, 'shahid': 3323, 'afridi': 3324, 'tou': 3325, 'mahnor': 3326, 'baloch': 3327, 'nikki': 3328, 'flower': 3329, 'blackfly': 3330, 'courgette': 3331, 'wont': 3332, 'affect': 3333, 'fruit': 3334, 'italian': 3335, 'netfilx': 3336, 'unmarried': 3337, 'finger': 3338, 'rock': 3339, 'wiellys': 3340, 'paul': 3341, 'barcode': 3342, 'charlotte': 3343, 'thtas': 3344, 'trailblazerhonors': 3345, 'labour': 3346, 'leader': 3347, 'alot': 3348, 'agayhippiehippy': 3349, 'exercise': 3350, 'better': 3351, 'ginger': 3352, 'x28': 3353, 'teach': 3354, 'awareness': 3355, '::': 3356, 'portsmouth': 3357, 'sonal': 3358, 'hungry': 3359, 'hmmm': 3360, 'pedant': 3361, '98': 3362, 'kit': 3363, 'ack': 3364, 'hih': 3365, 'choir': 3366, 'rosidbinr': 3367, 'duke': 3368, 'earl': 3369, 'tau': 3370, 'awak': 3371, 'orayt': 3372, 'knw': 3373, 'block': 3374, 'dikha': 3375, 'reh': 3376, 'adolf': 3377, 'hitler': 3378, 'obstacle': 3379, 'exist': 3380, 'surrender': 3381, 'terrific': 3382, 'advaddict': 3383, '_15': 3384, 'jimin': 3385, 'notanapology': 3386, 'map': 3387, 'informed': 3388, '0.7': 3389, 'dependency': 3390, 'motherfucking': 3391, \"david's\": 3392, 'damn': 3393, 'college': 3394, '24th': 3395, 'steroid': 3396, 'made': 3397, 'alansmithpart': 3398, 'publication': 3399, 'servus': 3400, 'bonasio': 3401, \"doido's\": 3402, 'task': 3403, 'delegate': 3404, 'aaahhh': 3405, 'jen': 3406, 'information': 3407, 'virgin': 3408, 'non-mapbox': 3409, 'restrict': 3410, 'mapbox': 3411, 'basemaps': 3412, 'contractually': 3413, 'researcher': 3414, 'seafood': 3415, 'weltum': 3416, 'teh': 3417, 'dety': 3418, 'huh': 3419, '=D': 3420, 'annoy': 3421, 'katmtan': 3422, 'swan': 3423, 'fandom': 3424, 'blurry': 3425, 'besok': 3426, 'b': 3427, 'urgently': 3428, 'within': 3429, 'currently': 3430, 'dorset': 3431, 'goddess': 3432, 'blast': 3433, 'shitfaced': 3434, 'soul': 3435, 'donate': 3436, 'sing': 3437, 'disney': 3438, 'doug': 3439, '28': 3440, 'bnte': 3441, 'hain': 3442, ';p': 3443, 'shiiitt': 3444, 'case': 3445, 'rm35': 3446, 'negooo': 3447, 'male': 3448, 'madeline': 3449, 'nun': 3450, 'mornin': 3451, 'yapsters': 3452, 'ply': 3453, 'copy': 3454, 'icon': 3455, 'alchemist': 3456, 'x27': 3457, 'dayz': 3458, 'preview': 3459, 'thug': 3460, 'lmao': 3461, 'sharethelove': 3462, 'highvalue': 3463, 'halsey': 3464, '30th': 3465, 'wed': 3466, 'anniversary': 3467, 'folk': 3468, 'bae': 3469, 'reply': 3470, 'complain': 3471, 'rude': 3472, 'bond': 3473, 'niggs': 3474, 'readingres': 3475, 'wordoftheweek': 3476, 'wotw': 3477, '4:18': 3478, 'est': 3479, 'earn': 3480, 'whatevs': 3481, 'jess': 3482, 'surry': 3483, 'botany': 3484, 'gel': 3485, 'alison': 3486, 'lsa': 3487, 'response': 3488, 'fron': 3489, 'debbie': 3490, 'carol': 3491, 'patient': 3492, 'discharge': 3493, 'lounge': 3494, 'walmart': 3495, 'balance': 3496, 'study': 3497, 'hayley': 3498, 'shoulder': 3499, 'pad': 3500, 'mount': 3501, 'inquisitor': 3502, 'cosplay': 3503, 'cosplayprogress': 3504, 'mike': 3505, 'dunno': 3506, 'housing': 3507, 'insecurity': 3508, 'nh': 3509, 'devolution': 3510, 'patriotism': 3511, 'halla': 3512, 'ark': 3513, \"jiyeon's\": 3514, 'buzz': 3515, 'burnt': 3516, 'mist': 3517, 'opi': 3518, 'avoplex': 3519, 'nail': 3520, 'cuticle': 3521, 'replenish': 3522, '15ml': 3523, 'serious': 3524, 'submission': 3525, 'lb': 3526, 'cherish': 3527, 'flip': 3528, 'backflip': 3529, 'jumpgiants': 3530, 'foampit': 3531, 'usa': 3532, 'pamer': 3533, 'thks': 3534, 'actuallythough': 3535, 'craft': 3536, 'session': 3537, 'mehtab': 3538, 'aunty': 3539, 'gc': 3540, 'yeeew': 3541, 'pre': 3542, 'lan': 3543, 'yeey': 3544, 'strangely': 3545, 'arrange': 3546, 'doodle': 3547, 'comic': 3548, 'summoner': 3549, 'none': 3550, '🙅': 3551, 'lycra': 3552, 'vincent': 3553, 'couldnt': 3554, 'roy': 3555, 'bg': 3556, 'img': 3557, 'circle': 3558, 'font': 3559, 'deathofgrass': 3560, 'loan': 3561, 'lawnmower': 3562, 'popular': 3563, 'charismatic': 3564, 'man.he': 3565, 'thrive': 3566, 'economy': 3567, 'burst': 3568, 'georgie': 3569, 'x26': 3570, 'million': 3571, 'fl': 3572, 'sometime': 3573, 'iceland': 3574, 'crazy': 3575, 'landscape': 3576, 'yok': 3577, 'lah': 3578, 'concordia': 3579, 'reunite': 3580, 'xxxibmchll': 3581, 'sea': 3582, 'imitatia': 3583, 'oe': 3584, 'michelle': 3585, 'comeback': 3586, 'gross': 3587, 'treat': 3588, 'equal': 3589, 'injustice': 3590, 'feminism': 3591, 'ineedfeminismbecause': 3592, 'jam': 3593, 'stuck': 3594, 'recommend': 3595, 'redhead': 3596, 'wacky': 3597, 'rather': 3598, 'worst': 3599, 'waytoliveahappylife': 3600, 'hoxton': 3601, 'holborn': 3602, 'karen': 3603, 'wag': 3604, 'bum': 3605, 'wwooo': 3606, 'nite': 3607, 'drawing': 3608, 'laiten': 3609, 'arond': 3610, '1:30': 3611, 'consider': 3612, 'exhaust': 3613, 'mature': 3614, 'journeyps': 3615, 'foam': 3616, \"lady's\": 3617, 'mob': 3618, 'false': 3619, 'bulletin': 3620, 'spring': 3621, 'fiesta': 3622, 'noise': 3623, 'awuuu': 3624, 'aich': 3625, 'sept': 3626, 'rudramadevi': 3627, 'anushka': 3628, 'gunashekar': 3629, 'harryxhood': 3630, 'upset': 3631, 'ooh': 3632, 'humanist': 3633, 'magazine': 3634, 'username': 3635, 'rape': 3636, 'csrracing': 3637, 'lack': 3638, 'hygiene': 3639, 'tose': 3640, 'clothes': 3641, 'temperature': 3642, 'planet': 3643, 'brave': 3644, 'ge': 3645, '2015kenya': 3646, 'ryan': 3647, 'tidy': 3648, 'hagergang': 3649, 'chanhun': 3650, 'photoshoot': 3651, 'afterall': 3652, 'sadkaay': 3653, 'tharkness': 3654, 'peak': 3655, 'heatwave': 3656, 'lower': 3657, 'standard': 3658, 'x25': 3659, 'exams': 3660, 'recruit': 3661, 'doom': 3662, 'nasty': 3663, 'affiliate': 3664, '>:)': 3665, 'situate': 3666, '64': 3667, '74': 3668, '40': 3669, '00': 3670, 'hall': 3671, 'ted': 3672, 'pixgram': 3673, 'creative': 3674, 'slideshow': 3675, 'tentatively': 3676, 'nibble': 3677, 'ivy': 3678, 'sho': 3679, 'superpower': 3680, 'obsess': 3681, 'oth': 3682, 'third': 3683, 'ngarepfollbackdarinabilahjkt': 3684, '48': 3685, 'sunglasses': 3686, 'jackie': 3687, 'sunnies': 3688, 'style': 3689, 'jlo': 3690, 'jlovers': 3691, 'turkey': 3692, 'goodafternoon': 3693, 'collage': 3694, 'furry': 3695, 'bruce': 3696, 'kunoriforceo': 3697, 'aayegi': 3698, 'timming': 3699, 'wiw': 3700, 'bips': 3701, 'zareen': 3702, 'daisy': 3703, \"b'coz\": 3704, 'karte': 3705, 'mak': 3706, '∗': 3707, 'lega': 3708, 'branding': 3709, 'spag': 3710, 'boat': 3711, 'outboarding': 3712, 'spell': 3713, 'reboarding': 3714, 'fire': 3715, 'offboarding': 3716, 'sn16': 3717, '9dg': 3718, 'following': 3719, 'bnf': 3720, '50': 3721, 'jason': 3722, 'rob': 3723, 'feb': 3724, 'victoriasecret': 3725, 'finland': 3726, 'helsinki': 3727, 'airport': 3728, 'plane': 3729, 'beyond': 3730, 'onting': 3731, 'tiis': 3732, 'lng': 3733, 'yan': 3734, \"u'll\": 3735, 'steve': 3736, 'bell': 3737, 'prescott': 3738, 'leadership': 3739, 'cartoon': 3740, 'upside': 3741, 'statement': 3742, 'selamathariraya': 3743, 'lovesummertime': 3744, 'dumont': 3745, 'jax': 3746, 'jones': 3747, 'awesomeee': 3748, 'x24': 3749, 'geoff': 3750, 'packing': 3751, 'stick': 3752, 'amazingly': 3753, 'talanted': 3754, 'vsco': 3755, 'thankies': 3756, 'hash': 3757, 'tag': 3758, 'ifimeetanalien': 3759, 'bff': 3760, 'section': 3761, 'follbaaack': 3762, 'az': 3763, 'cauliflower': 3764, 'attempt': 3765, 'prinsesa': 3766, 'yaaah': 3767, 'law': 3768, 'toy': 3769, 'sonaaa': 3770, 'beautifull': 3771, \"josephine's\": 3772, 'mirror': 3773, 'cretaperfect': 3774, '4me': 3775, 'cretaperfectsuv': 3776, 'creta': 3777, 'load': 3778, 'telecom': 3779, 'judy': 3780, 'superb': 3781, 'slightly': 3782, 'rakna': 3783, 'ew': 3784, 'whose': 3785, 'fifa': 3786, 'lineup': 3787, 'survive': 3788, 'p90x': 3789, 'p90': 3790, 'dishoom': 3791, 'rajnigandha': 3792, 'minju': 3793, 'rapper': 3794, 'lead': 3795, 'vocal': 3796, 'yujin': 3797, 'visual': 3798, 'maknae': 3799, 'jane': 3800, 'hah': 3801, 'hawk': 3802, 'history': 3803, 'along': 3804, 'talkback': 3805, 'process': 3806, 'feature': 3807, 'mostly': 3808, \"cinema's\": 3809, 'defend': 3810, 'fashion': 3811, 'atrocity': 3812, 'pandimensional': 3813, 'manifestation': 3814, 'argos': 3815, 'ring': 3816, '640': 3817, 'nad': 3818, 'plezzz': 3819, 'asthma': 3820, 'inhaler': 3821, 'breathe': 3822, 'goodluck': 3823, 'hunger': 3824, 'mockingjay': 3825, 'thehungergames': 3826, 'adore': 3827, 'x23': 3828, 'reina': 3829, 'felt': 3830, 'blogged': 3831, 'excuse': 3832, 'attender': 3833, 'whn': 3834, 'andre': 3835, 'mamayang': 3836, '11pm': 3837, '1d': 3838, '89.9': 3839, 'powys': 3840, 'shropshire': 3841, 'border': 3842, \"school's\": 3843, 'san': 3844, 'diego': 3845, 'jump': 3846, 'source': 3847, 'appeasement': 3848, '¦': 3849, 'aj': 3850, 'action': 3851, 'grunt': 3852, 'sc': 3853, 'anti-christ': 3854, 'm8': 3855, 'ju': 3856, 'halfway': 3857, 'ex': 3858, 'postive': 3859, 'opinion': 3860, 'avi': 3861, 'dare': 3862, 'corridor': 3863, '👯': 3864, 'neither': 3865, 'rundown': 3866, 'yah': 3867, 'leviboard': 3868, 'kleper': 3869, ':(': 3870, 'impeccable': 3871, 'setokido': 3872, 'shoulda': 3873, 'hippo': 3874, 'materialistic': 3875, 'showpo': 3876, 'cough': 3877, '@artofsleepingin': 3878, 'x22': 3879, '☺': 3880, 'makesme': 3881, 'santorini': 3882, 'escape': 3883, 'beatport': 3884, '👊🏻': 3885, 'trmdhesitant': 3886, 'manuel': 3887, 'valls': 3888, 'king': 3889, 'seven': 3890, 'kingdom': 3891, 'andals': 3892, 'privacy': 3893, 'wise': 3894, 'natsuki': 3895, 'often': 3896, 'catchy': 3897, 'neil': 3898, 'emirate': 3899, 'brill': 3900, 'urquhart': 3901, 'castle': 3902, 'simple': 3903, 'generally': 3904, 'shatter': 3905, 'contrast': 3906, 'educampakl': 3907, 'rotorua': 3908, 'pehly': 3909, 'phir': 3910, 'somi': 3911, 'burfday': 3912, 'university': 3913, 'santo': 3914, 'tomas': 3915, 'norhing': 3916, 'dialogue': 3917, 'chainsaw': 3918, 'amusement': 3919, 'awe': 3920, 'protect': 3921, 'pop': 3922, '2ish': 3923, 'fahad': 3924, 'bhai': 3925, 'iqrar': 3926, 'waseem': 3927, 'abroad': 3928, 'rotation': 3929, 'moviee': 3930, 'chef': 3931, 'grogol': 3932, 'long-distance': 3933, 'rhys': 3934, 'pwrfl': 3935, 'benefit': 3936, 'b2b': 3937, 'b2c': 3938, \"else's\": 3939, 'soo': 3940, 'enterprison': 3941, 'schoolsoutforsummer': 3942, 'fellow': 3943, 'juggle': 3944, 'purrthos': 3945, 'cathos': 3946, 'catamis': 3947, 'fourfiveseconds': 3948, 'deaf': 3949, 'drug': 3950, 'alcohol': 3951, 'apexis': 3952, 'crystal': 3953, 'meth': 3954, 'champagne': 3955, 'fc': 3956, 'streamer': 3957, 'juice': 3958, 'correct': 3959, 'portrait': 3960, 'izumi': 3961, 'fugiwara': 3962, 'clonmel': 3963, 'refreshing': 3964, 'vibrant': 3965, 'estimate': 3966, 'server': 3967, 'quiet': 3968, 'yey': 3969, \"insha'allah\": 3970, 'wil': 3971, 'pleased': 3972, 'x21': 3973, 'trend': 3974, 'akshaymostlovedsuperstarever': 3975, 'indirecting': 3976, 'askurban': 3977, 'lyka': 3978, 'sits': 3979, 'nap': 3980, 'aff': 3981, 'uname': 3982, 'jonginuh': 3983, 'billie': 3984, 'forecast': 3985, '10am': 3986, '5am': 3987, 'soothe': 3988, 'vii': 3989, 'sweetheart': 3990, 'freak': 3991, 'original': 3992, 'zayn': 3993, 'fucker': 3994, 'pet': 3995, 'illustration': 3996, 'wohoo': 3997, 'gleam': 3998, 'painting': 3999, 'deal': 4000, 'prime': 4001, 'minister': 4002, 'sunjam': 4003, 'industry': 4004, 'present': 4005, 'practicing': 4006, 'proactive': 4007, 'environment': 4008, 'unreal': 4009, 'zaine': 4010, 'zac': 4011, 'isaac': 4012, 'os': 4013, 'frank': 4014, 'iero': 4015, 'phase': 4016, 'david': 4017, 'beginner': 4018, 'shin': 4019, 'sunflower': 4020, 'sunny': 4021, 'favourites': 4022, 'tommarow': 4023, 'yall': 4024, 'rank': 4025, 'birthdaymonth': 4026, 'vianey': 4027, 'bffs': 4028, 'july': 4029, 'birthdaygirl': 4030, \"town's\": 4031, 'andrew': 4032, 'checkout': 4033, 'otwol': 4034, 'awhile': 4035, 'x20': 4036, 'all-time': 4037, 'julia': 4038, 'robert': 4039, 'awwhh': 4040, 'bulldog': 4041, 'unfortunate': 4042, '02079': 4043, '490': 4044, '132': 4045, 'caring': 4046, 'fightstickfriday': 4047, 'extravagant': 4048, 'tearout': 4049, 'selektion': 4050, 'yoot': 4051, 'cross': 4052, 'deserved': 4053, 'gudday': 4054, 'dave': 4055, 'haileyhelps': 4056, 'eid': 4057, 'mubarak': 4058, 'brotheeerrr': 4059, 'adventure': 4060, 'tokyo': 4061, 'kansai': 4062, 'l': 4063, 'uppe': 4064, 'om': 4065, '60': 4066, 'minuter': 4067, 'detailed': 4068, 'data': 4069, 'jesus': 4070, 'amsterdam': 4071, '3rd': 4072, 'nextweek': 4073, 'sends': 4074, 'booty': 4075, 'bcuz': 4076, 'step': 4077, 'option': 4078, 'stable': 4079, 'sturdy': 4080, 'lukkkee': 4081, 'again.ensoi': 4082, 'tc': 4083, 'madam': 4084, 'siddi': 4085, 'unknown': 4086, 'roomie': 4087, 'gn': 4088, 'gf': 4089, 'consent': 4090, 'mister': 4091, 'supportive': 4092, 'vine': 4093, 'peyton': 4094, 'nagato': 4095, 'yuki-chan': 4096, 'shoushitsu': 4097, 'archdbanterbury': 4098, 'experttradesmen': 4099, 'banter': 4100, 'quiz': 4101, 'tradetalk': 4102, 'floofs': 4103, 'face': 4104, 'muahah': 4105, 'x19': 4106, 'anticipation': 4107, 'jds': 4108, 'laro': 4109, 'tayo': 4110, 'answer': 4111, 'ht': 4112, 'angelica': 4113, 'anghel': 4114, 'aa': 4115, 'kkk': 4116, 'macbook': 4117, 'rehearse': 4118, 'youthcelebrate': 4119, 'mute': 4120, '29th': 4121, 'gohf': 4122, 'invited': 4123, 'vegetarian': 4124, \"she'll\": 4125, 'gooday': 4126, '101': 4127, '12000': 4128, 'oshieer': 4129, 'realreviews': 4130, 'happycustomers': 4131, 'realoshi': 4132, 'dealsuthaonotebachao': 4133, 'dime': 4134, 'uhuh': 4135, '🎵': 4136, 'code': 4137, 'pleasant': 4138, 'on-board': 4139, 'raheel': 4140, 'flyhigh': 4141, 'bother': 4142, 'everette': 4143, 'taylor': 4144, 'ha-ha': 4145, 'peachyloans': 4146, 'fridayfreebie': 4147, 'noe': 4148, 'yi': 4149, 'bindingofissac': 4150, 'xboxone': 4151, 'console': 4152, 'justin': 4153, 'gladly': 4154, 'son': 4155, 'morocco': 4156, 'peru': 4157, 'nxt': 4158, 'bps': 4159, 'resort': 4160, 'x18': 4161, 'havuuuloveyou': 4162, 'uuu': 4163, 'possitve': 4164, 'hopeyou': 4165, 'sweetie': 4166, 'throwbackfriday': 4167, 'christen': 4168, 'ki': 4169, 'yaad': 4170, 'gayi': 4171, 'opossum': 4172, 'running': 4173, 'belated': 4174, 'yeahh': 4175, 'kuffar': 4176, 'computer': 4177, 'cell': 4178, 'diarrhea': 4179, 'immigrant': 4180, 'louse': 4181, 'goictived': 4182, '70685': 4183, 'tagsforlikes': 4184, 'trapmusic': 4185, 'hotmusicdelocos': 4186, 'kinickers': 4187, '01282': 4188, '452096': 4189, 'shady': 4190, 'management': 4191, 'reservation': 4192, 'tkts': 4193, 'likewise': 4194, 'overgeneralization': 4195, 'ikr': 4196, '😍': 4197, 'consumerism': 4198, 'rid': 4199, 'recently': 4200, 'fics': 4201, 'ouch': 4202, 'slip': 4203, 'disc': 4204, 'thw': 4205, 'swimming': 4206, 'chute': 4207, 'chalut': 4208, 'minute': 4209, 'replay': 4210, 'iplayer': 4211, '11am': 4212, 'unneeded': 4213, 'megamoh': 4214, '7/29': 4215, 'power': 4216, 'tool': 4217, 'zealand': 4218, 'pile': 4219, 'dump': 4220, 'couscous': 4221, \"women's\": 4222, 'fiction': 4223, 'wahahaah': 4224, 'x17': 4225, 'orhan': 4226, 'pamuk': 4227, 'hero': 4228, 'canopy': 4229, 'maple': 4230, 'leaf': 4231, 'syrup': 4232, 'farm': 4233, 'stephanie': 4234, '💖': 4235, 'congrtaualtions': 4236, 'phileas': 4237, 'club': 4238, 'inc': 4239, 'photograph': 4240, 'phonegraphs': 4241, 'srsly': 4242, '10:17': 4243, 'ripaaa': 4244, 'banate': 4245, 'ray': 4246, 'dept': 4247, 'hospital': 4248, 'grt': 4249, 'infographic': 4250, \"o'clock\": 4251, 'habit': 4252, '1dfor': 4253, 'roadtrip': 4254, '19:30': 4255, 'ifc': 4256, 'whip': 4257, 'lilsisbro': 4258, 'pre-ordered': 4259, \"pixar's\": 4260, 'steelbook': 4261, 'hmm': 4262, 'pegell': 4263, 'lemess': 4264, 'kyle': 4265, 'paypal': 4266, 'confirmation': 4267, 'oct': 4268, 'tud': 4269, 'jst': 4270, 'addictive': 4271, 'humphrey': 4272, 'yell': 4273, 'erm': 4274, 'breach': 4275, 'lemon': 4276, 'yogurt': 4277, 'pot': 4278, 'discover': 4279, 'liquorice': 4280, 'pud': 4281, 'cajun': 4282, 'spiced': 4283, 'yum': 4284, 'cajunchicken': 4285, 'infinite': 4286, 'gern': 4287, 'cikaaa': 4288, 'maaf': 4289, 'telat': 4290, 'ngucapinnya': 4291, 'maaay': 4292, 'x16': 4293, 'viparita': 4294, 'karani': 4295, 'legsupthewall': 4296, 'unwind': 4297, 'coco': 4298, 'comfy': 4299, 'jalulu': 4300, 'rosh': 4301, 'gla': 4302, 'avail': 4303, 'suit': 4304, 'pallavi': 4305, 'nairobi': 4306, 'hrdstellobama': 4307, 'regional': 4308, 'civil': 4309, 'society': 4310, 'region': 4311, 'globe': 4312, 'hajur': 4313, 'yayy': 4314, \"must've\": 4315, 'nerve': 4316, 'prelim': 4317, 'costacc': 4318, 'nwb': 4319, 'shud': 4320, 'begin': 4321, 'cold': 4322, 'hmu': 4323, 'cala': 4324, 'brush': 4325, 'ego': 4326, 'wherever': 4327, 'interaction': 4328, 'dongsaeng': 4329, 'chorong': 4330, 'friendship': 4331, 'ffs': 4332, 'impressive': 4333, 'dragon': 4334, 'duck': 4335, 'mix': 4336, 'cheetah': 4337, 'wagga': 4338, 'coursework': 4339, 'lorna': 4340, 'scan': 4341, 'x12': 4342, 'canvas': 4343, 'paint': 4344, 'iqbal': 4345, 'ima': 4346, 'knowing': 4347, 'hon': 4348, 'aja': 4349, 'besi': 4350, 'chati': 4351, 'phulani': 4352, 'swasa': 4353, 'bahari': 4354, 'jiba': 4355, 'mumbai': 4356, 'gujarat': 4357, 'distrubed': 4358, 'otherwise': 4359, '190cr': 4360, 'inspite': 4361, 'holder': 4362, 'threatens': 4363, 'daily': 4364, 'basis': 4365, 'vr': 4366, 'angelo': 4367, 'quezon': 4368, 'sweatpants': 4369, 'breath': 4370, 'tripping': 4371, 'farbridges': 4372, 'segalakatakata': 4373, 'nixus': 4374, 'flint': 4375, '🍰': 4376, 'separately': 4377, 'criticise': 4378, 'gesture': 4379, 'pedal': 4380, 'stroke': 4381, 'attentive': 4382, 'caro': 4383, 'deposit': 4384, 'secure': 4385, 'shock': 4386, 'coffe': 4387, 'tenerina': 4388, 'auguri': 4389, 'iso': 4390, 'certification': 4391, 'paralyze': 4392, 'anxiety': 4393, 'sadness': 4394, \"it'd\": 4395, 'development': 4396, 'spain': 4397, 'def': 4398, 'bantime': 4399, 'fail': 4400, '2ban': 4401, 'x15': 4402, 'awkward': 4403, 'abs': 4404, 'galing': 4405, 'founder': 4406, 'loveyaaah': 4407, '⅛': 4408, '⅞': 4409, '∞': 4410, 'specialist': 4411, 'aw': 4412, 'babyyy': 4413, 'djstruthmate': 4414, 're-cap': 4415, 'flickr': 4416, 'tack': 4417, 'zephbot': 4418, 'hhahahahaha': 4419, 'blew': 4420, 'upp': 4421, 'entire': 4422, 'vega': 4423, 'strip': 4424, 'hahahahahhaha': 4425, \"callie's\": 4426, 'puppy': 4427, 'owner': 4428, 'callinganimalabusehotlineasap': 4429, 'gorefiend': 4430, 'mythic': 4431, 'reminder': 4432, '9:00': 4433, '▪': 4434, '️bea': 4435, 'miller': 4436, 'lockscreen': 4437, 'mbf': 4438, 'keesh': 4439, \"yesterday's\": 4440, 'groupie': 4441, 'bebe': 4442, 'sizams': 4443, 'color': 4444, 'invoice': 4445, 'kanina': 4446, 'pong': 4447, 'umaga': 4448, 'browser': 4449, 'typically': 4450, 'pleasse': 4451, 'leeteuk': 4452, 'pearl': 4453, 'thusi': 4454, 'pour': 4455, 'milk': 4456, 'tgv': 4457, 'paris': 4458, 'austerlitz': 4459, 'blois': 4460, 'mile': 4461, 'chateau': 4462, 'de': 4463, 'marais': 4464, 'taxi': 4465, 'x14': 4466, 'noms': 4467, 'enji': 4468, 'hater': 4469, 'purchase': 4470, 'specially-marked': 4471, 'custard': 4472, 'sm': 4473, 'on-pack': 4474, 'instruction': 4475, 'tile': 4476, 'downstairs': 4477, 'kelly': 4478, 'greek': 4479, 'petra': 4480, 'shadowplaylouis': 4481, 'mutual': 4482, 'cuz': 4483, 'liveonstreamate': 4484, 'lani': 4485, 'graze': 4486, 'pride': 4487, 'bristolart': 4488, 'in-app': 4489, 'ensure': 4490, 'item': 4491, 'screw': 4492, 'amber': 4493, 'noticing': 4494, '43': 4495, 'hpc': 4496, 'wip': 4497, 'sws': 4498, 'newsround': 4499, 'hound': 4500, '7:40': 4501, 'ada': 4502, 'racist': 4503, 'hulk': 4504, 'tight': 4505, 'prayer': 4506, 'pardon': 4507, 'phl': 4508, 'abu': 4509, 'dhabi': 4510, 'blessing': 4511, 'hihihi': 4512, 'teamjanuaryclaims': 4513, 'godonna': 4514, 'msg': 4515, 'bowwowchicawowwow': 4516, 'settle': 4517, 'dkt': 4518, 'porch': 4519, 'uber': 4520, 'mobile': 4521, 'application': 4522, 'giggle': 4523, 'delight': 4524, 'bare': 4525, 'wind': 4526, 'kahlil': 4527, 'gibran': 4528, 'flash': 4529, 'stiff': 4530, 'upper': 4531, 'lip': 4532, 'britain': 4533, 'latmon': 4534, 'endeavour': 4535, 'anne': 4536, 'joy': 4537, 'exploit': 4538, 'ign': 4539, 'au': 4540, 'pubcast': 4541, 'tengaman': 4542, '21': 4543, 'celebratio': 4544, 'determine': 4545, 'install': 4546, 'glorify': 4547, 'infirmity': 4548, 'silly': 4549, 'suave': 4550, 'gentlemen': 4551, 'monthly': 4552, 'mileage': 4553, 'target': 4554, 'samsung': 4555, 'quality': 4556, 'ey': 4557, 'beth': 4558, 'watched': 4559, 'gangster': 4560, \"athena's\": 4561, 'fancy': 4562, 'wellington': 4563, 'rich': 4564, 'christina': 4565, 'newsletter': 4566, 'zy': 4567, 'olur': 4568, 'x13': 4569, 'flawless': 4570, 'remix': 4571, 'reaction': 4572, 'hayli': 4573, 'edwin': 4574, 'elvena': 4575, 'emc': 4576, 'rubber': 4577, 'swearword': 4578, 'infection': 4579, '10:16': 4580, 'christophe': 4581, 'gans': 4582, 'brotherhood': 4583, 'pill': 4584, 'nocturnal': 4585, 'rrp': 4586, '18.99': 4587, '13.99': 4588, 'jah': 4589, 'wobble': 4590, 'retard': 4591, '50notifications': 4592, 'check-up': 4593, 'pun': 4594, 'elite': 4595, 'camillus': 4596, 'pleaseee': 4597, 'spare': 4598, 'tyre': 4599, 'joke': 4600, 'ahahah': 4601, 'shame': 4602, 'abandon': 4603, 'disagree': 4604, 'nowhere': 4605, 'contradict': 4606, 'continuously': 4607, 'chaos': 4608, 'contain': 4609, 'cranium': 4610, 'sneaker': 4611, 'nike': 4612, 'nikeoriginal': 4613, 'nikeindonesia': 4614, 'pierojogger': 4615, 'skoy': 4616, 'winter': 4617, 'falklands': 4618, 'jamie-lee': 4619, 'congraaats': 4620, 'hooh': 4621, 'chrome': 4622, 'storm': 4623, 'thunderstorm': 4624, 'vegas': 4625, 'circuscircus': 4626, 'omgg': 4627, 'thankie': 4628, 'tdy': 4629, '(-:': 4630, 'peter': 4631, 'expel': 4632, 'boughy': 4633, 'kernel': 4634, 'paralysis': 4635, 'liza': 4636, 'lol.hook': 4637, 'vampire': 4638, 'diaries': 4639, 'twice': 4640, 'thanq': 4641, 'goodwill': 4642, 'vandr': 4643, 'ash': 4644, 'debatable': 4645, 'solar': 4646, '6-5': 4647, 'ek': 4648, 'taco': 4649, 'mexico': 4650, 'viva': 4651, 'méxico': 4652, 'burger': 4653, 'thebestangkapuso': 4654, 'tooth': 4655, 'korean': 4656, 'netizen': 4657, 'cruel': 4658, 'elephant': 4659, 'marula': 4660, 'tdif': 4661, 'shoutouts': 4662, 'shortly': 4663, 'itsamarvelthing': 4664, 'marvel': 4665, \"japan's\": 4666, 'artist': 4667, 'homework': 4668, 'marco': 4669, 'herb': 4670, 'pm': 4671, 'self': 4672, 'esteem': 4673, 'patience': 4674, 'sobtian': 4675, 'coworker': 4676, 'deathly': 4677, 'hallows': 4678, 'supernatural': 4679, 'consultant': 4680, 'himachal': 4681, '2.25': 4682, 'ashamed': 4683, 'inform': 4684, 'where.do.i.start': 4685, 'moviemarathon': 4686, 'conversational': 4687, 'skill': 4688, 'shadow': 4689, 'own': 4690, 'pair': 4691, 'typical': 4692, \"it'll\": 4693, 'cortez': 4694, 'superstar': 4695, 'tthanks': 4696, 'colin': 4697, 'luxuous': 4698, 'tarryn': 4699, 'practice': 4700, 'goodness': 4701, 'hbdme': 4702, 'yeeeyyy': 4703, 'barsostay': 4704, 'malese': 4705, 'independent': 4706, 'sum': 4707, 'debacle': 4708, 'perfectly': 4709, 'amyjackson': 4710, 'omegle': 4711, 'countrymusic': 4712, 'five': 4713, \"night's\": 4714, \"freddy's\": 4715, 'demo': 4716, 'pump': 4717, 'fanboy': 4718, 'thegrandad': 4719, 'impression': 4720, 'grand': 4721, 'sidni': 4722, 'remarriage': 4723, 'occasion': 4724, 'completion': 4725, 'language': 4726, 'java': 4727, \"php's\": 4728, 'notion': 4729, 'reference': 4730, 'equally': 4731, 'confuse': 4732, 'ohioan': 4733, 'doctor': 4734, 'offline': 4735, 'thesims': 4736, 'mb': 4737, 'meaningless': 4738, 'common': 4739, 'celebrate': 4740, 'muertosatfringe': 4741, 'emulation': 4742, 'enemy': 4743, 'relax': 4744, 'ou': 4745, 'pink': 4746, 'cc': 4747, 'meooowww': 4748, 'barkkkiiideee': 4749, 'bark': 4750, 'x11': 4751, 'routine': 4752, 'aleks': 4753, 'awh': 4754, 'kumpul': 4755, 'cantik': 4756, 'ganteng': 4757, 'kresna': 4758, 'jelly': 4759, 'simon': 4760, 'lesley': 4761, 'blood': 4762, 'panty': 4763, 'lion': 4764, 'artworkbylie': 4765, 'judo': 4766, 'presentation': 4767, 'daredevil': 4768, 'despondently': 4769, 're-watch': 4770, 'welcoma.have': 4771, 'favor': 4772, 'tridon': 4773, '21pics': 4774, 'master': 4775, 'nim': 4776, \"there're\": 4777, '22pics': 4778, 'kebun': 4779, 'adorable': 4780, 'ubud': 4781, 'ladyposse': 4782, 'xoxoxo': 4783, 'sneak': 4784, 'peek': 4785, 'tuned': 4786, 'inbox': 4787, 'happyweekend': 4788, 'therealgolden': 4789, '47': 4790, 'girlfriendsmya': 4791, 'ppl': 4792, 'njoy': 4793, 'followingg': 4794, 'private': 4795, 'pusher': 4796, 'pri': 4797, 'stun': 4798, 'wooohooo': 4799, 'cuss': 4800, 'teenage': 4801, 'ace': 4802, 'sauce': 4803, 'livi': 4804, 'fowles': 4805, 'oliviafowles': 4806, '891': 4807, 'burnout': 4808, 'johnforceo': 4809, 'matthew': 4810, 'provoke': 4811, 'indiankulture': 4812, 'oppose': 4813, 'biker': 4814, 'dis': 4815, 'lyk': 4816, 'gud': 4817, 'weight': 4818, 'bcus': 4819, 'rubbish': 4820, 'veggie': 4821, 'steph': 4822, 'nj': 4823, 'x10': 4824, 'explore': 4825, 'listenable': 4826, 'cohesive': 4827, 'gossip': 4828, 'alex': 4829, 'heswifi': 4830, '7am': 4831, 'wub': 4832, 'cerbchan': 4833, 'jarraaa': 4834, 'morrrning': 4835, 'snooze': 4836, 'clicksco': 4837, 'gay': 4838, 'lesbian': 4839, 'rigid': 4840, 'theocratic': 4841, 'wing': 4842, 'fundamentalist': 4843, 'islamist': 4844, 'brianaaa': 4845, 'brianazabrocki': 4846, 'sky': 4847, 'batb': 4848, 'clap': 4849, 'working': 4850, 'whilst': 4851, 'aki': 4852, 'thencerest': 4853, '547': 4854, 'indiemusic': 4855, 'sexyjudy': 4856, 'pussy': 4857, 'sexo': 4858, 'humidity': 4859, '87': 4860, 'promotional': 4861, 'sloppy': 4862, \"second's\": 4863, 'stock': 4864, 'marmite': 4865, 'x9': 4866, 'nic': 4867, 'taft': 4868, 'finalist': 4869, 'lottery': 4870, 'award': 4871, 'usagi': 4872, 'looove': 4873, 'wowww': 4874, '💙': 4875, '💚': 4876, '💕': 4877, 'lepas': 4878, 'sembuh': 4879, 'sibuk': 4880, 'balik': 4881, 'kin': 4882, 'gotham': 4883, 'sunnyday': 4884, 'texting': 4885, 'dudettes': 4886, 'cost': 4887, 'flippin': 4888, 'fortune': 4889, 'divinediscontent': 4890, ';}': 4891, 'amnotness': 4892, 'autofollow': 4893, 'teamfollowback': 4894, 'geer': 4895, 'bat': 4896, 'mz': 4897, 'yang': 4898, 'deennya': 4899, 'jehwan': 4900, '11:00': 4901, 'julie': 4902, 'ashton': 4903, '✧': 4904, '｡': 4905, 'chelny': 4906, 'datz': 4907, 'jeremy': 4908, 'fmt': 4909, 'dat': 4910, 'heartbeat': 4911, 'clutch': 4912, '🐢': 4913, 'watching': 4914, 'besteverdoctorwhoepisode': 4915, 'relevant': 4916, 'puke': 4917, 'proper': 4918, 'x8': 4919, 'subliminal': 4920, 'eatmeat': 4921, 'brewproject': 4922, 'lovenafianna': 4923, 'mr': 4924, 'lewis': 4925, 'clock': 4926, '3:02': 4927, 'happens': 4928, 'muslim': 4929, 'prophet': 4930, 'غردلي': 4931, 'is.he': 4932, 'mistake': 4933, 'politician': 4934, 'argue': 4935, 'intellect': 4936, 'recommendation': 4937, 'shiva': 4938, 'mp3': 4939, 'apps': 4940, 'standrews': 4941, 'sandcastle': 4942, 'ewok': 4943, 'nate': 4944, 'brawl': 4945, 'rear': 4946, 'naked': 4947, 'choke': 4948, 'heck': 4949, 'gun': 4950, 'associate': 4951, 'um': 4952, 'endowment': 4953, 'ai': 4954, 'sikandar': 4955, 'pti': 4956, 'standwdik': 4957, 'westandwithik': 4958, 'starbucks': 4959, 'logo': 4960, 'obsession': 4961, 'addiction': 4962, 'renew': 4963, 'charity': 4964, 'جمعة_مباركة': 4965, 'hokies': 4966, 'biz': 4967, 'non': 4968, 'america': 4969, 'california': 4970, '01:16': 4971, '45gameplay': 4972, 'iloveyou': 4973, 'vex': 4974, 'igers': 4975, 'leicaq': 4976, 'leica': 4977, 'dudeee': 4978, 'persona': 4979, 'yepp': 4980, '5878e503': 4981, 'x7': 4982, 'greg': 4983, 'useful': 4984, 'posey': 4985, 'miami': 4986, 'james_yammouni': 4987, 'breakdown': 4988, 'material': 4989, 'thorin': 4990, 'hunt': 4991, 'choroo': 4992, 'nahi': 4993, 'aztec': 4994, 'princess': 4995, 'rainy': 4996, 'kingfisher': 4997, 'relaxed': 4998, 'chinua': 4999, 'achebe': 5000, 'intellectual': 5001, 'liquid': 5002, 'melbournetrip': 5003, 'taxikitchen': 5004, 'nooow': 5005, 'mcdo': 5006, 'everywhere': 5007, 'dreamer': 5008, 'tanisha': 5009, '1nonly': 5010, 'attitude': 5011, 'kindle': 5012, 'flame': 5013, 'conviction': 5014, 'bar': 5015, 'repath': 5016, 'adis': 5017, 'stefanie': 5018, 'sg1': 5019, 'lightbox': 5020, 'incorrect': 5021, 'spelling': 5022, 'apologist': 5023, 'x6': 5024, 'vuly': 5025, '01:15': 5026, 'batman': 5027, 'pearson': 5028, 'reputation': 5029, 'nikkei': 5030, 'woodford': 5031, 'vscocam': 5032, 'vscoph': 5033, 'vscogood': 5034, 'vscophil': 5035, 'vscocousins': 5036, 'yaap': 5037, 'urwelc': 5038, 'neon': 5039, 'pant': 5040, 'haaa': 5041, 'willing': 5042, 'auspost': 5043, 'openfollow': 5044, 'rp': 5045, 'eng': 5046, 'yūjō-cosplay': 5047, 'cosplayers': 5048, 'luxembourg': 5049, 'bunnys': 5050, 'broadcast': 5051, 'needa': 5052, 'gal': 5053, 'bend': 5054, 'heaven': 5055, 'proposal': 5056, 'score': 5057, 'january': 5058, 'hanabutle': 5059, 'kikhorny': 5060, 'interracial': 5061, 'makeup': 5062, 'chu': 5063, \"weekend's\": 5064, 'punt': 5065, 'horseracing': 5066, 'horse': 5067, 'horseracingtips': 5068, 'soulful': 5069, 'guitar': 5070, 'cocoared': 5071, 'salut': 5072, 'brief': 5073, 'introduction': 5074, 'indian': 5075, 'subcontinent': 5076, 'bfr': 5077, 'mauryas': 5078, 'jordanian': 5079, '00962778381': 5080, '838': 5081, 'tenyai': 5082, 'hee': 5083, 'ss': 5084, 'semi': 5085, 'atp': 5086, 'wimbledon': 5087, 'federer': 5088, 'nadal': 5089, 'monfils': 5090, 'handsome': 5091, 'cilic': 5092, 'firm': 5093, 'diary': 5094, 'potentially': 5095, 'nyc': 5096, 'chillin': 5097, 'lils': 5098, 'tail': 5099, 'kitten': 5100, 'garret': 5101, 'baz': 5102, 'leo': 5103, 'xst': 5104, 'centrifugal': 5105, 'haired': 5106, 'eternity': 5107, 'forgive': 5108, 'kangin': 5109, 'بندر': 5110, 'العنزي': 5111, 'kristin': 5112, 'ca': 5113, 'surajettan': 5114, 'kashi': 5115, 'ashwathy': 5116, 'mommy': 5117, 'tirth': 5118, 'brambhatt': 5119, 'playing': 5120, 'snooker': 5121, 'compensation': 5122, 'theoper': 5123, '479': 5124, 'premiostumundo': 5125, 'differ': 5126, 'philosophical': 5127, 'x5': 5128, 'graphic': 5129, 'skills': 5130, 'level': 5131, 'thurs': 5132, 'aug': 5133, 'excl': 5134, 'raw': 5135, 'weenie': 5136, 'annoyingbaby': 5137, 'lazy': 5138, 'cosy': 5139, 'client_amends_edit': 5140, '_5_final_final_final': 5141, 'pdf': 5142, 'mauliate': 5143, 'ito': 5144, 'fridays': 5145, 'okkay': 5146, 'knock': 5147, \"soloist's\": 5148, 'ryu': 5149, 'saera': 5150, 'pinkeu': 5151, 'angry': 5152, 'animation': 5153, 'screencaps': 5154, 'jonghyun': 5155, 'seungyeon': 5156, 'cnblue': 5157, 'mbc': 5158, 'wgm': 5159, 'masa': 5160, 'entrepreneurship': 5161, 'empower': 5162, 'limpopo': 5163, 'picts': 5164, 'norapowel': 5165, 'hornykik': 5166, 'livesex': 5167, 'emirates': 5168, 'pumpkin': 5169, 'reserve': 5170, 'thrice': 5171, 'patron': 5172, 'venture': 5173, 'deathcure': 5174, 'boob': 5175, 'blame': 5176, 'dine': 5177, 'modern': 5178, 'grill': 5179, 'disk': 5180, 'nt4': 5181, 'iirc': 5182, 'ux': 5183, 'refinement': 5184, 'zdps': 5185, 'didnt': 5186, 'justice': 5187, 'daw': 5188, 'tine': 5189, 'gensan': 5190, 'frightlings': 5191, 'undead': 5192, 'plush': 5193, 'cushion': 5194, 'nba': 5195, '2k15': 5196, 'mypark': 5197, 'chronicle': 5198, 'gryph': 5199, 'volume': 5200, 'favorable': 5201, 'ellen': 5202, 'degeneres': 5203, 'shirt': 5204, 'mint': 5205, 'superdry': 5206, 'berangkaat': 5207, 'lagiii': 5208, 'siguro': 5209, 'un': 5210, 'kesa': 5211, 'lotsa': 5212, 'organisation': 5213, '4am': 5214, 'fingers-crossed': 5215, 'deep': 5216, 'htaccess': 5217, 'file': 5218, 'adf': 5219, 'womad': 5220, 'gran': 5221, 'canaria': 5222, 'gig': 5223, 'twist': 5224, 'define': 5225, 'youve': 5226, 'teamnatural': 5227, 'huni': 5228, 'yayayayay': 5229, 'yt': 5230, 'convention': 5231, 'barely': 5232, 'brighton': 5233, 'slay': 5234, 'nickname': 5235, 'babygirl': 5236, 'regard': 5237, 'himmat': 5238, 'karain': 5239, 'baat': 5240, 'meri': 5241, 'debate': 5242, 'hotee-my': 5243, 'uncle': 5244, 'tongue': 5245, 'pronounce': 5246, 'native': 5247, 'american': 5248, 'proverb': 5249, 'lovable': 5250, 'yesha': 5251, 'montoya': 5252, 'eagerly': 5253, 'waiting': 5254, 'payment': 5255, 'supreme': 5256, 'leon': 5257, 'randy': 5258, '9bis': 5259, 'physique': 5260, 'shave': 5261, 'uncut': 5262, 'boi': 5263, 'printing': 5264, 'regular': 5265, 'printer': 5266, 'nz': 5267, 'large': 5268, 'format': 5269, '10/10': 5270, 'senior': 5271, 'raid': 5272, 'conserve': 5273, 'battery': 5274, 'comfortable': 5275, 'swt': 5276, 'reservations@sandsbeach.eu': 5277, 'localgaragederby': 5278, 'campus': 5279, 'subgames': 5280, 'faceit': 5281, 'snpcaht': 5282, 'hakhakhak': 5283, 't___t': 5284, \"kyungsoo's\": 5285, 'animated': 5286, '3d': 5287, 'property': 5288, 'agent': 5289, 'accurate': 5290, 'description': 5291, 'theory': 5292, 'x4': 5293, '15.90': 5294, 'yvette': 5295, 'author': 5296, 'mwf': 5297, 'programme': 5298, 'taal': 5299, 'lake': 5300, '2emt': 5301, '«': 5302, 'scurri': 5303, 'agile': 5304, 'shipping': 5305, 'solution': 5306, 'sme': 5307, 'omar': 5308, 'kamaal': 5309, 'amm': 5310, '3am': 5311, 'hopehousekids': 5312, 'pitmantraining': 5313, 'walkersmithway': 5314, 'keepitlocal': 5315, 'sehun': 5316, 'se100leaders': 5317, 'uneventful': 5318, 'sofa': 5319, 'surf': 5320, 'cunt': 5321, 'unfollow': 5322, 'convos': 5323, 'rescoops': 5324, 'multiracial': 5325, 'fk': 5326, 'narrow': 5327, 'warlock': 5328, 'faithful': 5329, 'balloon': 5330, 'pas': 5331, 'mj': 5332, 'madison': 5333, 'beonknockknock': 5334, 'con-graduation': 5335, 'gent': 5336, 'bitchface': 5337, '😒': 5338, 'organic': 5339, '12pm': 5340, 'york': 5341, 'lendal': 5342, 'pikami': 5343, 'capture': 5344, 'fulton': 5345, 'sheen': 5346, 'baloney': 5347, 'unvarnished': 5348, 'thick': 5349, 'blarney': 5350, 'flattery': 5351, 'laid': 5352, 'thin': 5353, 'sachin': 5354, 'unimportant': 5355, 'context': 5356, 'dampen': 5357, 'excitement': 5358, 'yu': 5359, 'compare': 5360, 'rocket': 5361, 'narendra': 5362, 'modi': 5363, 'aaaand': 5364, \"team's\": 5365, 'macauley': 5366, 'however': 5367, 'x3': 5368, 'wheeen': 5369, 'heechul': 5370, 'toast': 5371, 'coffee-weekdays': 5372, '9-11': 5373, 'sail': 5374, \"friday's\": 5375, 'commercial': 5376, 'insurance': 5377, 'requirement': 5378, 'lookfortheo': 5379, 'updated': 5380, 'cl': 5381, 'thou': 5382, 'april': 5383, 'airforce': 5384, 'clark': 5385, 'field': 5386, 'pampanga': 5387, 'troll': 5388, '⚡': 5389, 'brow': 5390, 'oily': 5391, 'maricarljanah': 5392, '6:15': 5393, 'degree': 5394, 'fahrenheit': 5395, '🍸': 5396, '╲': 5397, '─': 5398, '╱': 5399, '🍤': 5400, '╭': 5401, '╮': 5402, '┓': 5403, '┳': 5404, '┣': 5405, '╰': 5406, '╯': 5407, '┗': 5408, '┻': 5409, 'stool': 5410, 'topple': 5411, 'findyourfit': 5412, 'preferred': 5413, 'whomosexual': 5414, 'stack': 5415, 'pandora': 5416, 'attend': 5417, 'digitalexeter': 5418, 'digitalmarketing': 5419, 'sociamedia': 5420, 'nb': 5421, 'bom': 5422, 'dia': 5423, 'todos': 5424, 'forklift': 5425, 'worker': 5426, 'lsceens': 5427, 'immature': 5428, 'gandhi': 5429, 'grassy': 5430, 'feetblog': 5431, 'daughter': 5432, '4yrs': 5433, 'old-porridge': 5434, 'fiend': 5435, '2nite': 5436, 'comp': 5437, 'viking': 5438, 't20blast': 5439, 'np': 5440, 'tax': 5441, 'feets': 5442, 'ooohh': 5443, 'petjam': 5444, 'virtual': 5445, 'pounce': 5446, 'benteke': 5447, 'agnes': 5448, 'socialmedia@dpdgroup.co.uk': 5449, 'sam': 5450, 'fruity': 5451, 'vodka': 5452, 'sellyourcarin': 5453, '5words': 5454, 'chaloniklo': 5455, 'pic.twitter.com/jxz2lbv6o': 5456, \"paperwhite's\": 5457, 'laser-like': 5458, 'focus': 5459, 'ghost': 5460, 'tagsforlikesapp': 5461, 'instagood': 5462, 'tbt': 5463, 'socket': 5464, 'spanner': 5465, 'patiently': 5466, '😴': 5467, 'pglcsgo': 5468, 'x2': 5469, 'tend': 5470, 'crave': 5471, 'sjw': 5472, 'cakehamper': 5473, 'glow': 5474, 'yayyy': 5475, 'mercedes': 5476, 'hood': 5477, 'badge': 5478, 'hosted': 5479, 'drone': 5480, 'blow': 5481, 'ignore': 5482, 'retaliate': 5483, 'bollinger': 5484, \"where's\": 5485, 'planning': 5486, 'denmark': 5487, 'whitey': 5488, 'culture': 5489, 'coursee': 5490, 'intro': 5491, 'graphicdesign': 5492, 'videographer': 5493, 'youtuber': 5494, 'space': 5495, \"ted's\": 5496, 'bogus': 5497, '1000': 5498, 'hahahaaah': 5499, 'owly': 5500, 'afternon': 5501, 'whangarei': 5502, 'katie': 5503, 'nicholas': 5504, 'pauline': 5505, 'trafficker': 5506, 'daring': 5507, 'hence': 5508, 'expression': 5509, 'wot': 5510, 'hand-lettering': 5511, 'roof': 5512, 'ease': 5513, '2/2': 5514, 'sour': 5515, 'dough': 5516, 'egypt': 5517, 'hubby': 5518, 'mixed': 5519, 'sakin': 5520, 'six': 5521, 'christmas': 5522, 'avril': 5523, 'n04js': 5524, '25': 5525, 'prosecco': 5526, 'pech': 5527, 'micro': 5528, 'catspjs': 5529, '4:15': 5530, 'lazyweekend': 5531, 'overdue': 5532, '💃': 5533, 'jurassic': 5534, 'ding': 5535, 'nila': 5536, 'pairing': 5537, '8)': 5538, 'unfortunately': 5539, 'cookie': 5540, 'shir': 5541, '0': 5542, 'hale': 5543, 'cheshire': 5544, 'decorate': 5545, 'lemme': 5546, 'recs': 5547, 'ingat': 5548, 'din': 5549, 'mono': 5550, 'kathryn': 5551, 'jr': 5552, 'develop': 5553, 'hsr': 5554, 'base': 5555, 'major': 5556, 'sugarrush': 5557, 'knitting': 5558, 'partly': 5559, 'binge': 5560, 'homegirl': 5561, 'nancy': 5562, 'fenja': 5563, 'aapke': 5564, 'benchmark': 5565, 'ke': 5566, 'hisaab': 5567, 'ho': 5568, 'gaya': 5569, 'ofc': 5570, 'influence': 5571, 'rtss': 5572, 'hwaiting': 5573, 'titanfall': 5574, 'xbox': 5575, 'ultimate': 5576, 'experiment': 5577, 'gastronomy': 5578, 'newblogpost': 5579, 'foodiefridays': 5580, 'foodie': 5581, 'yoghurt': 5582, 'pancake': 5583, 'sabah': 5584, 'kapima': 5585, 'gelen': 5586, 'guzel': 5587, 'bir': 5588, 'hediye': 5589, 'thanx': 5590, '💞': 5591, 'visa': 5592, 'parisa': 5593, 'epiphany': 5594, 'lit': 5595, 'em-con': 5596, 'defender': 5597, '0330 333 7234': 5598, 'retailer': 5599, 'kianweareproud': 5600, 'distract': 5601, 'dayofarch': 5602, '10-20': 5603, 'bapu': 5604, 'ivypowel': 5605, 'newmusic': 5606, 'sexchat': 5607, '🍅': 5608, 'pathway': 5609, 'balkan': 5610, 'gypsy': 5611, 'mayhem': 5612, 'burek': 5613, 'meat': 5614, 'gibanica': 5615, 'pie': 5616, 'likely': 5617, 'surrey': 5618, 'afterwards': 5619, '10.30': 5620, 'heard': 5621, 'temporal': 5622, 'void': 5623, 'stem': 5624, 'sf': 5625, 'ykr': 5626, 'sparky': 5627, '40mm': 5628, '3.5': 5629, 'grs': 5630, 'rockfishing': 5631, 'topwater': 5632, 'twitlonger': 5633, 'me.so': 5634, 'jummah': 5635, 'durood': 5636, 'pak': 5637, 'cjradacomateada': 5638, 'suprised': 5639, 'debut': 5640, 'shipper': 5641, 'aside': 5642, 'housemate': 5643, '737bigatingconcert': 5644, 'jedzjabłka': 5645, 'pijjabłka': 5646, 'polish': 5647, 'cider': 5648, 'mustread': 5649, 'cricket': 5650, 'origin': 5651, '5pm': 5652, 'query': 5653, 'abby': 5654, 'sumedh': 5655, 'sunnah': 5656, 'عن': 5657, 'quad': 5658, 'bike': 5659, 'carrie': 5660, 'propriety': 5661, 'chronic': 5662, 'illness': 5663, 'superday': 5664, 'chocolatey': 5665, 'yasu': 5666, 'ooooh': 5667, 'fucked': 5668, 'hallo': 5669, 'improvement': 5670, 'dylan': 5671, 'laura': 5672, 'patrice': 5673, 'keepin': 5674, 'mohr': 5675, 'guest': 5676, \"o'neal\": 5677, 'tks': 5678, 'luas': 5679, 'stone': 5680, 'quicker': 5681, 'diet': 5682, 'sosweet': 5683, 'nominiere': 5684, 'und': 5685, 'hardcore': 5686, '😌': 5687, 'ff__special': 5688, 'acha': 5689, 'banda': 5690, '✌': 5691, 'bhi': 5692, 'krta': 5693, 'beautifully-crafted': 5694, 'mockingbird': 5695, 'diploma': 5696, 'blend': 5697, 'numbero': 5698, 'lolz': 5699, 'ambrose': 5700, 'gwinett': 5701, 'bierce': 5702, 'suffer': 5703, 'ravage': 5704, 'illadvised': 5705, 'marriage': 5706, 'virginity': 5707, 'cynical': 5708, 'yahuda': 5709, 'nosmet': 5710, 'pony': 5711, 'cuuute': 5712, \"f'ing\": 5713, 'vacant': 5714, 'hauc': 5715, 'hiss': 5716, 'overnight': 5717, 'cornish': 5718, 'all-clear': 5719, 'complains': 5720, 'raincoat': 5721, 'measure': 5722, 'wealth': 5723, 'invest': 5724, 'garbi': 5725, 'wash': 5726, 'refuel': 5727, 'dunedin': 5728, 'kalle': 5729, 'rakhi': 5730, 'photographer': 5731, '12th': 5732, 'carry': 5733, 'represent': 5734, 'slovenia': 5735, 'fridge': 5736, 'fighting': 5737, 'ludlow': 5738, '28th': 5739, 'selway': 5740, 'submit': 5741, 'spanish': 5742, 'greet': 5743, '90210': 5744, 'oitnb': 5745, 'prepare': 5746, 'condition': 5747, 'msged': 5748, 'chiquitos': 5749, 'ohaha': 5750, 'delhi': 5751, '95': 5752, 'webtogsawards': 5753, 'grace': 5754, 'sheffield': 5755, 'tramline': 5756, 'tl': 5757, 'hack': 5758, 'lad': 5759, 'beeepin': 5760, 'duper': 5761, 'handle': 5762, 'critique': 5763, 'contectually': 5764, 'beliebers': 5765, 'ultor': 5766, 'mamaya': 5767, 'loiyals': 5768, 'para': 5769, 'truthfulwordsof': 5770, 'beanatividad': 5771, 'nknkkpagpapakumbaba': 5772, 'birthdaypresent': 5773, 'compliment': 5774, 'swerve': 5775, 'goodtime': 5776, 'sinister': 5777, 'tryna': 5778, 'anonymous': 5779, 'dipsatch': 5780, 'aunt': 5781, 'dagga': 5782, 'burketeer': 5783, '2am': 5784, 'promo': 5785, 'required': 5786, 'twine': 5787, \"diane's\": 5788, 'happybirthday': 5789, 'thanksss': 5790, 'randomly': 5791, 'buckinghampalace': 5792, 'personality': 5793, 'chibi': 5794, 'maker': 5795, 'timog': 5796, '18th': 5797, 'otw': 5798, 'kami': 5799, 'feelinggood': 5800, 'demand': 5801, 'naman': 5802, 'barkin': 5803, 'yeap': 5804, 'onkey': 5805, 'umma': 5806, 'pervert': 5807, 'onyu': 5808, 'appa': 5809, 'lucy': 5810, 'horrible': 5811, 'quantum': 5812, 'blockchain': 5813, 'nowplaying': 5814, 'loftey': 5815, 'routte': 5816, 'assia': 5817, '.\\n.\\n.': 5818, 'joint': 5819, 'futurereleases': 5820, \"look's\": 5821, 'scary': 5822, 'murder': 5823, 'mystery': 5824, 'comma': 5825, \"j's\": 5826, 'hunny': 5827, 'diva': 5828, 'emily': 5829, 'nathan': 5830, 'meditation': 5831, 'alumnus': 5832, 'mba': 5833, 'representative': 5834, 'foto': 5835, 'what-is-your-fashionality': 5836, 'lorenangel': 5837, 'kw': 5838, 'tellanoldjokeday': 5839, 'reqd': 5840, 'speculation': 5841, 'consistency': 5842, 'tropic': 5843, 'startupph': 5844, 'zodiac': 5845, 'rapunzel': 5846, 'imaginative': 5847, 'therver': 5848, '85552': 5849, 'bestoftheday': 5850, 'oralsex': 5851, 'carly': 5852, 'happily': 5853, 'contract': 5854, 'matsu_bouzu': 5855, 'sonic': 5856, 'videogames': 5857, 'harana': 5858, 'belfast': 5859, 'danny': 5860, 'rare': 5861, 'sponsorship': 5862, 'aswell': 5863, 'gigi': 5864, 'nick': 5865, 'austin': 5866, 'youll': 5867, 'weak': 5868, '10,000': 5869, 'bravo': 5870, 'iamamonster': 5871, 'rxthedailysurveyvotes': 5872, 'as': 5873, 'roux': 5874, 'walkin': 5875, 'audience': 5876, 'pfb': 5877, 'jute': 5878, 'walangmakakapigilsakin': 5879, 'lori': 5880, 'ehm': 5881, 'trick': 5882, 'baekhyun': 5883, 'eyesmiles': 5884, 'borrow': 5885, 'knife': 5886, 'thek': 5887, 'widely': 5888, 'eventually': 5889, 'reaapearing': 5890, 'kno': 5891, 'whet': 5892, 'grattis': 5893, 'tweetin': 5894, 'inshallah': 5895, 'banana': 5896, 'raspberry': 5897, 'healthylifestyle': 5898, 'aint': 5899, 'skate': 5900, 'analyze': 5901, 'variety': 5902, 'twitching': 5903, '4:13': 5904, 'insomnia': 5905, 'medication': 5906, 'opposite': 5907, 'everlasting': 5908, 'yoga': 5909, 'massage': 5910, 'osteopath': 5911, 'trainer': 5912, 'sharm': 5913, 'al_master_band': 5914, 'tbc': 5915, 'univesity': 5916, 'architecture': 5917, 'random': 5918, 'isnt': 5919, 'typo': 5920, 'snark': 5921, 'lessions': 5922, 'drunk': 5923, 'bruuh': 5924, '2weeks': 5925, '50europe': 5926, '🇫🇷': 5927, 'iove': 5928, 'accord': 5929, 'mne': 5930, 'pchelok': 5931, 'ja': 5932, 'carefully': 5933, '=:': 5934, 'comet': 5935, 'ahah': 5936, 'candy': 5937, 'axio': 5938, 'remind': 5939, 'rabbit': 5940, 'nutshell': 5941, 'letshavecocktailsafternuclai': 5942, 'malik': 5943, 'umair': 5944, 'celebrity': 5945, 'canon': 5946, 'gang': 5947, 'grind': 5948, 'thoracicbridge': 5949, '5minute': 5950, 'nonscripted': 5951, 'password': 5952, 'shoshannavassil': 5953, 'addmeonsnapchat': 5954, 'dmme': 5955, 'mpoints': 5956, 'soph': 5957, 'subjective': 5958, 'painful': 5959, 'hopeless': 5960, 'ikea': 5961, 'slide': 5962, 'ban': 5963, 'neighbour': 5964, 'motor': 5965, 'search': 5966, 'sialan': 5967, 'athabasca': 5968, 'glacier': 5969, '1948': 5970, ':-(': 5971, 'jasper': 5972, 'jaspernationalpark': 5973, 'alberta': 5974, 'explorealberta': 5975, 'mare': 5976, 'ivan': 5977, 'hahahah': 5978, 'replacement': 5979, 'bi-polar': 5980, 'grasp': 5981, 'basic': 5982, 'digital': 5983, 'research': 5984, '😩': 5985, 'choreographing': 5986, 'longer': 5987, 'mingming': 5988, 'truly': 5989, 'pee': 5990, 'newwine': 5991, 'doushite': 5992, 'gundam': 5993, 'dollar': 5994, 'pale': 5995, 'imitation': 5996, 'rash': 5997, 'absent': 5998, 'sequel': 5999, 'ouucchhh': 6000, 'wisdom': 6001, 'teeth': 6002, 'frighten': 6003, 'pret': 6004, 'wkwkw': 6005, 'verfied': 6006, 'sentir-se': 6007, 'incompleta': 6008, 'thwarting': 6009, 'zante': 6010, \"when's\": 6011, 'audraesar': 6012, 'craaazzyy': 6013, 'helium': 6014, 'climatechange': 6015, \"california's\": 6016, 'influential': 6017, 'pollution': 6018, 'watchdog': 6019, 'califor': 6020, 'elhaida': 6021, 'jury': 6022, '10th': 6023, 'televoting': 6024, 'idaho': 6025, 'drought-linked': 6026, 'die-of': 6027, 'abrupt': 6028, 'climate': 6029, 'mammoth': 6030, 'megafauna': 6031, \"australia's\": 6032, 'considers': 6033, 'energy': 6034, 'biomass': 6035, 'golf': 6036, 'ulti': 6037, 'prexy': 6038, 'kindergarten': 6039, 'sane': 6040, 'boss': 6041, 'hozier': 6042, 'soooner': 6043, 'respectlost': 6044, 'hypercholesteloremia': 6045, 'calibraska': 6046, 'genuine': 6047, 'contender': 6048, 'aos': 6049, 'yun': 6050, 'encore': 6051, '4thwin': 6052, 'baymax': 6053, 'mixer': 6054, 'wft': 6055, 'promotion': 6056, 'hub': 6057, 'finale': 6058, 'parasyte': 6059, 'alll': 6060, 'zayniscomingbackonjuly': 6061, '26': 6062, 'era': 6063, '。': 6064, 'ω': 6065, '」': 6066, '∠': 6067, 'nathann': 6068, 'dieididieieiei': 6069, 'netflix': 6070, 'ervin': 6071, 'accept': 6072, 'desperate': 6073, 'amargolonnard': 6074, 'batalladelosgallos': 6075, 'webcamsex': 6076, 'duty': 6077, \"u've\": 6078, 'alien': 6079, 'sorka': 6080, 'funeral': 6081, 'nonexistent': 6082, 'wowza': 6083, 'fah': 6084, 'boo': 6085, 'hyung': 6086, 'predict': 6087, 'sj': 6088, 'nominate': 6089, 'brace': 6090, 'omggg': 6091, 'whyyy': 6092, 'annoying': 6093, 'evan': 6094, 'opt': 6095, 'muster': 6096, 'merchs': 6097, 'drinking': 6098, 'savanna': 6099, 'straw': 6100, 'yester': 6101, 'whens': 6102, 'consume': 6103, 'werk': 6104, 'foreals': 6105, 'wesen': 6106, 'uwesiti': 6107, 'bosen': 6108, 'egg': 6109, 'benny': 6110, 'badly': 6111, '>:(': 6112, 'zaz': 6113, 'rehash': 6114, 'mushroom': 6115, 'piece': 6116, 'exception': 6117, 'vicky': 6118, '45': 6119, 'hahah': 6120, 'ninasty': 6121, 'tsktsk': 6122, 'dick': 6123, 'kawaii': 6124, 'manly': 6125, 'youu': 6126, 'potato': 6127, 'fry': 6128, 'asf': 6129, 'eish': 6130, 'ive': 6131, 'mojo': 6132, 'mara': 6133, 'neh': 6134, 'association': 6135, 'councillor': 6136, 'sholong': 6137, 'reject': 6138, 'gee': 6139, 'gidi': 6140, 'lagos': 6141, 'ehn': 6142, 'arrest': 6143, 'idk': 6144, 'anybody': 6145, 'disappear': 6146, 'daze': 6147, 'fragment': 6148, \"would've\": 6149, 'walao': 6150, 'kbs': 6151, 'djderek': 6152, 'legend': 6153, 'tragedy': 6154, '💪🏻': 6155, '🐒': 6156, 'strucked': 6157, 'belgium': 6158, 'fabian': 6159, 'delph': 6160, 'draking': 6161, 'silently': 6162, 'mumma': 6163, 'pray': 6164, 'appropriate': 6165, 'woke': 6166, 'across': 6167, 'hindi': 6168, 'disability': 6169, 'pension': 6170, 'ptsd': 6171, 'impossible': 6172, 'physically': 6173, 'financially': 6174, 'nooo': 6175, 'togheter': 6176, '21st': 6177, 'snsd': 6178, 'anna': 6179, 'akana': 6180, 'askip': 6181, \"t'existes\": 6182, 'decides': 6183, 'kei': 6184, 'kate': 6185, 'spade': 6186, 'pero': 6187, 'walang': 6188, 'agesss': 6189, 'corinehurleigh': 6190, 'snapchatme': 6191, 'sfs': 6192, 'zara': 6193, 'trouser': 6194, \"it'okay\": 6195, 'luckely': 6196, 'orcalove': 6197, 'lew': 6198, 'crumble': 6199, 'mcflurry': 6200, 'cable': 6201, 'scared': 6202, \"venus's\": 6203, 'concept': 6204, 'rly': 6205, 'tagal': 6206, 'awful': 6207, 'appointment': 6208, 'cereal': 6209, 'previous': 6210, '73': 6211, 'fansign': 6212, 'expensive': 6213, 'zzzz': 6214, \"bff's\": 6215, 'extremely': 6216, \"deosn't\": 6217, 'liverpool': 6218, 'tacky': 6219, 'haaretz': 6220, 'israel': 6221, 'syria': 6222, 'chemical': 6223, 'wsj': 6224, 'rep': 6225, 'bts': 6226, 'wong': 6227, 'confiscate': 6228, 'icepack': 6229, 'dos': 6230, 'whimper': 6231, 'senpai': 6232, 'buttsex': 6233, \"dn't\": 6234, 'brk': 6235, \":'(\": 6236, 'falsetto': 6237, 'zone': 6238, 'loveofmylife': 6239, 's2': 6240, 'untruth': 6241, 'ij': 6242, '💥': 6243, '💫': 6244, 'soshi': 6245, 'buttt': 6246, 'heyyy': 6247, 'yeols': 6248, 'danceee': 6249, 'nemanja': 6250, 'vidic': 6251, 'roma': 6252, \"mom's\": 6253, 'linguist': 6254, \"dad's\": 6255, 'dumb': 6256, 'wanted': 6257, 'pouring': 6258, 'crash': 6259, 'resource': 6260, 'vehicle': 6261, 'ayex': 6262, 'lamon': 6263, 'scroll': 6264, 'curve': 6265, 'cement': 6266, '10.3': 6267, 'jmu': 6268, 'camp': 6269, 'tease': 6270, 'awuna': 6271, 'mbulelo': 6272, 'although': 6273, 'crackling': 6274, 'plug': 6275, 'fuse': 6276, 'dammit': 6277, 'carlton': 6278, 'aflblueshawks': 6279, \"alex's\": 6280, 'motorsport': 6281, 'cheeky': 6282, 'seo': 6283, 'nls': 6284, 'christy': 6285, 'niece': 6286, 'bloody': 6287, 'sandwhich': 6288, 'buset': 6289, 'discrimination': 6290, 'pregnancy': 6291, 'maternity': 6292, 'kick': 6293, 'domesticviolence': 6294, 'domestic': 6295, 'violence': 6296, 'victim': 6297, '98fm': 6298, 'stressful': 6299, 'upsetting': 6300, 'government': 6301, 'sapiosexual': 6302, 'beta': 6303, 'hogan': 6304, 'scrub': 6305, 'wwe': 6306, 'irene': 6307, 'naa': 6308, 'h_my_king': 6309, 'valentine': 6310, 'et': 6311, \"r'ships\": 6312, 'btwn': 6313, 'homo': 6314, 'biphobic': 6315, 'discipline': 6316, 'incl': 6317, 'european': 6318, 'langs': 6319, 'fresherstofinals': 6320, '💔': 6321, 'realistic': 6322, 'liking': 6323, 'prefer': 6324, 'benzema': 6325, 'hahahahahaah': 6326, 'donno': 6327, 'russian': 6328, 'waaa': 6329, 'eidwithgrofers': 6330, 'boreddd': 6331, 'mug': 6332, 'tiddler': 6333, '에이핑크': 6334, '더쇼': 6335, 'clan': 6336, 'slot': 6337, 'pfff': 6338, 'bugbounty': 6339, 'self-xss': 6340, 'host': 6341, 'header': 6342, 'poison': 6343, 'execution': 6344, 'ktksbye': 6345, 'cancel': 6346, 'petite': 6347, '600': 6348, 'security': 6349, 'odoo': 6350, 'partner': 6351, 'effin': 6352, 'ap': 6353, 'living': 6354, 'elsewhere': 6355, 'branch': 6356, 'concern': 6357, 'pod': 6358, 'D;': 6359, 'talk-kama': 6360, 'hawako': 6361, 'waa': 6362, 'kimaaani': 6363, 'prisss': 6364, 'baggage': 6365, 'sugar': 6366, 'rarely': 6367, 'agh': 6368, 'undercoverboss': 6369, 'تكفى': 6370, 'ache': 6371, 'wrist': 6372, 'ligo': 6373, 'dozen': 6374, 'shark': 6375, 'heartache': 6376, 'zayniscomingback': 6377, 'sweden': 6378, 'elmhurst': 6379, 'etid': 6380, \"chillin'with\": 6381, 'father': 6382, 'istanya': 6383, '2supply': 6384, 'infrastructure': 6385, 'nurse': 6386, 'paramedic': 6387, 'countless': 6388, '2cope': 6389, 'bored': 6390, 'plea': 6391, 'arianator': 6392, 'streaming': 6393, 'kendall': 6394, 'kylie': 6395, \"kylie's\": 6396, 'manila': 6397, 'jeebus': 6398, 'reabsorbtion': 6399, 'abscess': 6400, 'threaten': 6401, 'crown': 6402, 'ooouch': 6403, 'barney': 6404, \"be's\": 6405, 'problematic': 6406, 'mess': 6407, 'maa': 6408, 'bangalore': 6409, 'luis': 6410, 'manzano': 6411, 'shaaa': 6412, 'convener': 6413, '2:30': 6414, 'delete': 6415, 'airfields': 6416, 'jet': 6417, \"jack's\": 6418, 'spamming': 6419, \"mommy's\": 6420, 'overweight': 6421, 'sigeg': 6422, 'habhab': 6423, 'masud': 6424, 'kaha': 6425, 'akong': 6426, 'pala': 6427, 'dolphin': 6428, 'holy': 6429, 'anythin': 6430, 'beware': 6431, 'agonising': 6432, 'modimo': 6433, 'tseba': 6434, 'wena': 6435, 'fela': 6436, 'nowt': 6437, 'willy': 6438, 'gon': 6439, 'vomit': 6440, 'bowl': 6441, 'devastate': 6442, 'titan': 6443, 'ae': 6444, 'shiny': 6445, 'wavy': 6446, 'emo': 6447, 'germany': 6448, 'shedding': 6449, 'bheyps': 6450, 'ayemso': 6451, 'left': 6452, 'swell': 6453, 'wut': 6454, 'vicious': 6455, 'simpson': 6456, 'singapore': 6457, 'pooo': 6458, 'bh3s': 6459, 'pitchwars': 6460, 'chap': 6461, \"mine's\": 6462, 'transcript': 6463, \"apma's\": 6464, 'timw': 6465, 'accs': 6466, 'vitamin': 6467, 'stretch': 6468, 'blockjam': 6469, \"schedule's\": 6470, 'whack': 6471, 'thelock': 6472, '76': 6473, 'hotgirls': 6474, 'ghantay': 6475, 'nai': 6476, 'hay': 6477, 'rejection': 6478, 'deny': 6479, 'laguna': 6480, 'exit': 6481, 'gomen': 6482, 'inch': 6483, 'suuuper': 6484, '65': 6485, 'anyones': 6486, 'inactive': 6487, 'orphan': 6488, 'whaaat': 6489, 'kaya': 6490, 'naaan': 6491, 'pause': 6492, '3:30': 6493, 'inglewood': 6494, 'ummm': 6495, 'charcoal': 6496, 'mid-end': 6497, 'noooo': 6498, 'rodfanta': 6499, 'wasp': 6500, 'sting': 6501, 'avert': 6502, 'exo': 6503, 'seekly': 6504, 'riptito': 6505, 'manbearpig': 6506, 'shorter': 6507, 'academic': 6508, 'exclusive': 6509, 'unfair': 6510, 'esp': 6511, 'bleak': 6512, 'german': 6513, 'chart': 6514, 'pfft': 6515, 'washed': 6516, 'polaroid': 6517, 'newbethvideo': 6518, 'greece': 6519, 'xur': 6520, 'imy': 6521, 'stud': 6522, 'earring': 6523, 'hunde': 6524, '3.4': 6525, 'yach': 6526, 'huvvft': 6527, 'zoo': 6528, 'fieldtrips': 6529, 'sizwe': 6530, 'boredom': 6531, 'who': 6532, 'sinse': 6533, 'somehow': 6534, 'tiny': 6535, 'barbell': 6536, 'looong': 6537, 'apartement': 6538, 'longe': 6539, 'litro': 6540, 'shepherd': 6541, 'lami': 6542, 'lungomare': 6543, 'pesaro': 6544, 'giachietittiwedding': 6545, 'igersoftheday': 6546, 'summertime': 6547, 'nose': 6548, 'scarf': 6549, 'aus': 6550, 'afford': 6551, 'woe': 6552, 'nigga': 6553, 'motn': 6554, 'lighting': 6555, 'make-up': 6556, 'limited': 6557, 'ver': 6558, 'huhuhu': 6559, \"m'lady\": 6560, 'j8': 6561, 'j11': 6562, 'm20': 6563, 'acads': 6564, 'schedule': 6565, 'nowww': 6566, 'hugh': 6567, 'paw': 6568, 'muddy': 6569, 'distracted': 6570, 'heyy': 6571, 'cupcake': 6572, 'talaga': 6573, 'poppin': 6574, 'joc': 6575, 'playin': 6576, 'subj': 6577, 'sobrang': 6578, 'bv': 6579, 'zamn': 6580, 'afropunk': 6581, 'fest': 6582, 'shithouse': 6583, 'ladder': 6584, 'wrack': 6585, 'booset': 6586, 'restart': 6587, 'assassin': 6588, 'creed': 6589, 'ii': 6590, 'heap': 6591, 'ankle': 6592, 'puddle': 6593, 'wearing': 6594, 'slipper': 6595, 'eve': 6596, 'sararocs': 6597, 'anywayhedidanicejob': 6598, '😞': 6599, 'local': 6600, 'cruise': 6601, 'wail': 6602, 'wheelchair': 6603, '26weeks': 6604, 'sbenu': 6605, 'sasin': 6606, 'anarchy': 6607, 'candle': 6608, 'forehead': 6609, 'medicine': 6610, 'hoya': 6611, 'mah': 6612, 'aing': 6613, 'hush': 6614, 'gurly': 6615, 'purty': 6616, 'closer': 6617, 'shiver': 6618, 'properly': 6619, 'gol': 6620, 'pea': 6621, 'emotionally': 6622, 'mentally': 6623, 'tierd': 6624, \"eye's\": 6625, 'thnkyouuu': 6626, 'highlight': 6627, 'courage': 6628, 'fishy': 6629, 'idek': 6630, 'apink': 6631, 'performance': 6632, 'bulet': 6633, 'gendut': 6634, 'noo': 6635, 'hotwheels': 6636, 'm': 6637, 'patch': 6638, 'ahaha': 6639, 'akon': 6640, 'nightmare': 6641, 'mino': 6642, 'crazyyy': 6643, 'thooo': 6644, 'zz': 6645, 'straight': 6646, 'soundcheck': 6647, 'antagonistic': 6648, 'ob': 6649, 'phantasy': 6650, 'sleepdeprived': 6651, 'tiredashell': 6652, '4aspot': 6653, \"kinara's\": 6654, 'awami': 6655, 'niqqa': 6656, 'pb.contestants': 6657, 'aarww': 6658, 'lmbo': 6659, 'dangit': 6660, 'ohmygod': 6661, 'scenario': 6662, 'tooo': 6663, 'baechyyy': 6664, 'okayyy': 6665, 'noone': 6666, 'drag': 6667, 'seriously': 6668, 'misundersranding': 6669, 'chal': 6670, 'raha': 6671, 'yhm': 6672, 'edsa': 6673, 'jasmingarrick': 6674, 'milf': 6675, 'nakamaforever': 6676, 'kiksex': 6677, 'tried': 6678, \"unicef's\": 6679, 'fu': 6680, 'alone': 6681, 'manage': 6682, 'stephen': 6683, 'frustration': 6684, 'sent': 6685, 'woza': 6686, 'senight': 6687, '468': 6688, 'amateur': 6689, 'hotscratch': 6690, 'sock': 6691, '150-160': 6692, 'peso': 6693, 'degrassi': 6694, 'bcz': 6695, 'kat': 6696, 'chem': 6697, 'onscreen': 6698, 'ofscreen': 6699, 'irony': 6700, 'rhisfor': 6701, 'camsex': 6702, 'poopie': 6703, 'pip': 6704, 'uff': 6705, '1.300': 6706, 'glue': 6707, 'factory': 6708, 'kuchar': 6709, 'ups': 6710, 'definite': 6711, 'uni': 6712, 'anyways': 6713, 'ee': 6714, 'tommy': 6715, 'georgia': 6716, 'transmission': 6717, 'orang': 6718, 'suma': 6719, 'shouldeeerr': 6720, 'repack': 6721, 'repacks': 6722, 'dye': 6723, 'rihanna': 6724, 'ginge': 6725, 'adidas': 6726, 'pro@illamasqua.com': 6727, 'ifeelyou': 6728, 'ratbaglater': 6729, 'semester': 6730, 'gin': 6731, 'gutted': 6732, 'reynold': 6733, 'dessert': 6734, 'village': 6735, 'unite': 6736, 'oppressed': 6737, 'mass': 6738, 'afghanistn': 6739, 'war': 6740, 'sunggyu': 6741, 'injure': 6742, 'plaster': 6743, 'rtd': 6744, 'stadium': 6745, 'welder': 6746, 'hogo': 6747, 'vishaya': 6748, 'adu': 6749, 'bjp': 6750, 'madatte': 6751, 'anta': 6752, 'vishwas': 6753, 'ne': 6754, 'illa': 6755, 'wua': 6756, 'noticed': 6757, 'picky': 6758, 'mutuals': 6759, 'unfollowed': 6760, 'thenting': 6761, '423': 6762, 'sexual': 6763, 'sync': 6764, 'plug.dj': 6765, 'suspemsion': 6766, 'cope': 6767, 'offroading': 6768, 'theres': 6769, 'harvest': 6770, 'machinery': 6771, 'inapropriate': 6772, 'weave': 6773, 'investment': 6774, 'scottish': 6775, 'football': 6776, 'dire': 6777, 'nomoney': 6778, 'nawf': 6779, 'bechos': 6780, 'overly': 6781, 'lab': 6782, 'zap': 6783, 'distressing': 6784, 'cinema': 6785, 'louisianashooting': 6786, 'laughing': 6787, 'har': 6788, 'chum': 6789, 'ncc': 6790, 'ph': 6791, 'kayo': 6792, 'itong': 6793, 'thaaat': 6794, 'ctto': 6795, 'expire': 6796, 'bis': 6797, 'broke': 6798, 'bi': 6799, '3:33': 6800, 'jfc': 6801, 'bodo': 6802, 'amat': 6803, 'yelaaa': 6804, 'dublin': 6805, 'potter': 6806, 'pining': 6807, 'keybind': 6808, 'warfare': 6809, 'controlls': 6810, 'diagnose': 6811, 'wiv': 6812, \"scheuermann's\": 6813, 'disease': 6814, 'rlyhurts': 6815, 'howdo': 6816, 'georgesampson': 6817, 'signal': 6818, 'reckon': 6819, 't20': 6820, 'taunton': 6821, 'hopeful': 6822, 'justiceforsandrabland': 6823, 'sandrabland': 6824, 'disturb': 6825, 'happpy': 6826, 'justinbieber': 6827, 'daianerufato': 6828, 'ilysm': 6829, '07:34': 6830, 'delphy': 6831, 'dom': 6832, 'technique': 6833, 'mince': 6834, 'symphony': 6835, 'joe': 6836, 'wth': 6837, 'aisyhhh': 6838, 'bald': 6839, 'seungchan': 6840, 'aigooo': 6841, 'riri': 6842, 'vet': 6843, 'va': 6844, 'luminous': 6845, 'km': 6846, 'horribly': 6847, 'z': 6848, 'popularity': 6849, 'bebeee': 6850, 'lt': 6851, 'inaccuracy': 6852, 'inaccurate': 6853, 'worried': 6854, 'tragic': 6855, 'toronto': 6856, 'stuart': 6857, \"party's\": 6858, 'iyalaya': 6859, ';(': 6860, 'ubusy': 6861, 'gymnastics': 6862, 'aahhh': 6863, 'noggin': 6864, 'bump': 6865, 'feelslikeanidiot': 6866, 'pregnant': 6867, 'dearly': 6868, 'suk': 6869, 'scone': 6870, 'outnumber': 6871, 'eris': 6872, 'geez': 6873, 'precious': 6874, 'hive': 6875, 'voting': 6876, 'vietnam': 6877, 'dunt': 6878, 'sob': 6879, 'buff': 6880, 'toni': 6881, 'deactivate': 6882, \"shady's\": 6883, 'isibaya': 6884, '😓': 6885, 'colder': 6886, 'med': 6887, 'sausage': 6888, 'adios': 6889, 'h8': 6890, 'messenger': 6891, 'shittier': 6892, 'leno': 6893, 'identity': 6894, 'crisis': 6895, 'roommate': 6896, 'nighter': 6897, 'wetherspoons': 6898, 'pubs': 6899, 'police': 6900, 'ocean': 6901, 'lisaherring': 6902, 'ebony': 6903, 'polka': 6904, 'ndi': 6905, 'leftover': 6906, 'walnut': 6907, 'boah': 6908, 'mady': 6909, 'manga': 6910, 'giant': 6911, 'aminormalyet': 6912, 'poorly': 6913, 'tummy': 6914, 'pjs': 6915, 'groaning': 6916, 'nou': 6917, 'ken': 6918, 'saras': 6919, 'accident': 6920, 'freaking': 6921, 'describe': 6922, 'prydz': 6923, 'sister-in-law': 6924, 'instal': 6925, 'rear-ended': 6926, \"everyone's\": 6927, 'trash': 6928, 'boobs': 6929, 'stair': 6930, 'childhood': 6931, 'toothsensitivity': 6932, 'shem': 6933, 'awell': 6934, 'weekendofmadness': 6935, '🍹': 6936, 'cb': 6937, 'dancer': 6938, 'choregrapher': 6939, '626-430-8715': 6940, 'replied': 6941, 'hoe': 6942, 'xiu': 6943, 'nk': 6944, 'gi': 6945, 'us': 6946, 'elisse': 6947, 'ksoo': 6948, 'tat': 6949, 'bcoz': 6950, 'rancho': 6951, 'imperial': 6952, 'silang': 6953, 'subdivision': 6954, 'center': 6955, '39': 6956, 'cornwall': 6957, 'veritably': 6958, 'penny': 6959, 'ebook': 6960, 'фотосет': 6961, 'addicted-to-analsex': 6962, 'sweetbj': 6963, 'blowjob': 6964, 'mhhh': 6965, 'sed': 6966, 'mee': 6967, 'envious': 6968, 'eonni': 6969, 'lovey': 6970, 'dovey': 6971, 'workin': 6972, 'schade': 6973, 'isco': 6974, 'penis': 6975, 'nation': 6976, 'louisiana': 6977, 'lafayette': 6978, 'matteroftheheart': 6979, 'waduh': 6980, 'suspend': 6981, 'smoking': 6982, 'cliche': 6983, 'rma': 6984, 'jersey': 6985, 'texted': 6986, 'jaclintiler': 6987, 'teens': 6988, 'likeforlike': 6989, 'hotfmnoaidilforariana': 6990, 'fuckkk': 6991, 'sanum': 6992, 'llaollao': 6993, 'foood': 6994, 'ubericecream': 6995, 'glare': 6996, 'choreo': 6997, 'offensive': 6998, 'yeyy': 6999, 'hd': 7000, 'sux': 7001, 'nothaveld': 7002, '765': 7003, 'likeforfollow': 7004, 'mosquitoe': 7005, 'kinky': 7006, 'hsould': 7007, 'justget': 7008, 'married': 7009, 'shuffle': 7010, 'inte': 7011, 'buckling': 7012, 'millz': 7013, 'askies': 7014, 'awusasho': 7015, 'unlucky': 7016, 'briefly': 7017, 'greatful': 7018, '144p': 7019, 'brooke': 7020, 'cracked': 7021, '＠': 7022, 'maverickgamer': 7023, '07:32': 7024, '07:25': 7025, 'external': 7026, 'sd': 7027, 'airdroid': 7028, '4.4+': 7029, 'cramp': 7030, 'unstan': 7031, 'tay': 7032, 'ngeze': 7033, 'cocktaily': 7034, 'classy': 7035, '07:24': 7036, '✈': 7037, '️2': 7038, '☔': 7039, 'pen': 7040, 'spar': 7041, 'barcelona': 7042, 'bilbao': 7043, 'sharyl': 7044, 'shane': 7045, 'giddy': 7046, 'd1': 7047, 'zipper': 7048, 'repair': 7049, '2016': 7050, 'cont': 7051, 'wore': 7052, 'tempt': 7053, 'oreo': 7054, 'network': 7055, 'lolipop': 7056, 'kebab': 7057, 'klappertart': 7058, 'moodboster': 7059, 'unprepared': 7060, 'sry': 7061, 'dresscode': 7062, 'closed': 7063, 'iam': 7064, 'stab': 7065, 'meh': 7066, 'wrocilam': 7067, 'looww': 7068, 'recover': 7069, 'wayne': 7070, 'loss': 7071, 'steal': 7072, 'accidentally': 7073, 'damage': 7074, 'device': 7075, 'warranty': 7076, 'lmfaoo': 7077, 'fra': 7078, 'otamendi': 7079, 'ny': 7080, '🚖': 7081, '🗽': 7082, '🌃': 7083, 'stealth': 7084, 'bastard': 7085, 'therapy': 7086, 'exhausting': 7087, 'understandable': 7088, 'switzerland': 7089, 'th': 7090, 'wolrd': 7091, 'fyn': 7092, 'genuinely': 7093, '3g': 7094, 'christ': 7095, 'scale': 7096, 'deck': 7097, 'chair': 7098, 'yk': 7099, 'resy': 7100, 'bruh': 7101, 'lock': 7102, 'fbc': 7103, 'mork': 7104, '873': 7105, 'hotspotwithdanris': 7106, 'sone': 7107, 'produce': 7108, 'potager': 7109, 'blight': 7110, 'mych': 7111, 'shiiit': 7112, 'prompt': 7113, 'aready': 7114, 'similar': 7115, 'soulmate': 7116, 'careful': 7117, 'hws': 7118, 'jouch': 7119, 'por': 7120, 'que': 7121, 'liceooo': 7122, 'ayala': 7123, 'tunnel': 7124, 'thatscold': 7125, 'lourdes': 7126, 'bang': 7127, 'anywhere': 7128, 'showbox': 7129, 'naruto': 7130, 'companion': 7131, 'skinny': 7132, 'dubai': 7133, 'laribuggy': 7134, 'nutella': 7135, \"could've\": 7136, 'sirius': 7137, 'frudging': 7138, 'bbz': 7139, 'angeke': 7140, 'sbali': 7141, 'euuuwww': 7142, 'construction': 7143, '1k': 7144, 'nelle': 7145, 'ik': 7146, 'jaysus': 7147, 'buti': 7148, 'poop': 7149, 'rome': 7150, 'throat': 7151, 'llama': 7152, 'getwellsoonamber': 7153, 'heath': 7154, 'ledger': 7155, 'permission': 7156, '2-0': 7157, 'supersport': 7158, 'milkshake': 7159, 'witcher': 7160, 'papertown': 7161, 'bale': 7162, 'bahay': 7163, 'bahayan': 7164, 'magisa': 7165, 'lang': 7166, 'sadlyf': 7167, 'bunso': 7168, 'sleeep': 7169, 'astonvilla': 7170, 'berigaud': 7171, 'bakar': 7172, 'allergic': 7173, 'depress': 7174, \"blaine's\": 7175, 'acoustic': 7176, 'hernia': 7177, 'toxin': 7178, 'ariel': 7179, 'slam': 7180, 'bee': 7181, 'finddjderek': 7182, 'uuughhh': 7183, 'grabe': 7184, 'wheres': 7185, 'smi': 7186, 'nemesis': 7187, 'neeein': 7188, 'saaad': 7189, 'crease': 7190, 'tanned': 7191, 'dallas': 7192, 'infront': 7193, 'beato': 7194, 'tim': 7195, 'minha': 7196, 'deleicious': 7197, 'pcb': 7198, 'got': 7199, 'peregrine': 7200, '8.40': 7201, 'pigeon': 7202, 'flew': 7203, 'tram': 7204, 'hav': 7205, 'spent': 7206, 'apt': 7207, 'bldg': 7208, 'mmmm': 7209, 'nicki': 7210, 'fucjikg': 7211, 'disgust': 7212, 'buynotanapologyonitunes': 7213, 'avalible': 7214, 'frustrate': 7215, 'nw': 7216, 'sch': 7217, 'jeslyn': 7218, '72': 7219, 'root': 7220, 'kuch': 7221, 'hua': 7222, 'newbie': 7223, 'miracle': 7224, 'linda': 7225, 'nux': 7226, 'hinanap': 7227, 'uy': 7228, 'sched': 7229, 'anyare': 7230, 'entertain': 7231, 'typa': 7232, 'transparency': 7233, 'photoshop': 7234, 'planner': 7235, 'helppp': 7236, 'wearig': 7237, 'dri': 7238, 'prey': 7239, 'ausfailia': 7240, 'snow': 7241, 'footy': 7242, 'row': 7243, \"m's\": 7244, 'kitkat': 7245, '😢': 7246, 'suger': 7247, 'olivia': 7248, 'audition': 7249, 'injury': 7250, 'appendix': 7251, 'appendicitis': 7252, 'fack': 7253, 'nhl': 7254, 'khamis': 7255, 'reaaly': 7256, 'naomi': 7257, 'contemporary': 7258, 'slacke': 7259, '565': 7260, 'jahat': 7261, 'discount': 7262, 'thorpe': 7263, 'nicely': 7264, 'esnho': 7265, 'node': 7266, 'directx': 7267, 'p2': 7268, 'uploaded': 7269, 'blackberry': 7270, 'shitty': 7271, 'povertyyouareevil': 7272, 'struggle': 7273, 'emm': 7274, 'elgin': 7275, 'vava': 7276, 'makati': 7277, '💛': 7278, 'baon': 7279, 'soak': 7280, 'mush': 7281, \"they'd\": 7282, 'ouat': 7283, 'blinkin': 7284, 'headack': 7285, 'tension': 7286, 'eritation': 7287, 'frustrated': 7288, 'perspective': 7289, 'endlessly': 7290, 'blush': 7291, 'kiddo': 7292, 'rumbelle': 7293, 'overwhelming': 7294, 'irresponsibly': 7295, 'pakighinabi': 7296, 'pinkfinite': 7297, 'beb': 7298, 'migraine': 7299, 'coyote': 7300, 'headache': 7301, '인피니트': 7302, 'baechu': 7303, 'calibraskaep': 7304, 'elgato': 7305, 'ant': 7306, 'unexpect': 7307, 'faint': 7308, 'bp': 7309, 'subway': 7310, 'fragile': 7311, 'gap': 7312, 'plot': 7313, 'bungie': 7314, 'woohyun': 7315, 'guilty': 7316, 'davao': 7317, 'luckyyy': 7318, 'confidence': 7319, 'eunhae': 7320, 'misplace': 7321, 'den': 7322, 'dae': 7323, 'bap': 7324, 'huehue': 7325, 'rice': 7326, 'krispy': 7327, 'marshmallow': 7328, 'm5m6junction': 7329, 'soulsurvivor': 7330, 'stafford': 7331, 'progress': 7332, 'mixture': 7333, \"they've\": 7334, 'shems': 7335, 'lage': 7336, 'ramd': 7337, 'opening': 7338, 'munchkin': 7339, 'parting': 7340, 'juja': 7341, 'murugan': 7342, 'bgtau': 7343, 'harap': 7344, 'bagi': 7345, 'aminn': 7346, 'fraand': 7347, '😬': 7348, 'bigbang': 7349, 'sian': 7350, 'nicoleapage': 7351, 'surprised': 7352, 'hellish': 7353, 'thirstyyy': 7354, 'chesties': 7355, \"nando's\": 7356, 'bow': 7357, 'hen': 7358, 'rdd': 7359, 'dissipate': 7360, 'capeee': 7361, 'japan': 7362, 'outlive': 7363, 'x-ray': 7364, 'dental': 7365, 'spine': 7366, 'relief': 7367, 'popol': 7368, 'stomach': 7369, 'frog': 7370, 'brad': 7371, 'gen.ad': 7372, 'negotiable': 7373, 'huhuhuhuhu': 7374, 'bbmadeinmanila': 7375, 'findavip': 7376, 'boyirl': 7377, 'yasss': 7378, '6th': 7379, 'june': 7380, 'laine': 7381, 'difficiency': 7382, 'speed': 7383, 'rapist': 7384, 'commit': 7385, 'crime': 7386, 'bachpan': 7387, 'yaadein': 7388, 'finnair': 7389, 'heathrow': 7390, 'norwegian': 7391, ':\\\\': 7392, 'upvotes': 7393, 'keeno': 7394, 'whatthefuck': 7395, 'grotty': 7396, 'seeker': 7397, 'morality': 7398, 'fern': 7399, 'mimi': 7400, 'bali': 7401, 'editing': 7402, 'lowbat': 7403, 'funk': 7404, 'wewanticecream': 7405, 'sweat': 7406, 'eugh': 7407, 'sara': 7408, 'occasionally': 7409, \"izzy's\": 7410, 'dorm': 7411, 'choppy': 7412, \"infinite's\": 7413, '5:30': 7414, 'cayton': 7415, 'emma': 7416, 'darcey': 7417, 'connor': 7418, 'roommateexperience': 7419, 'avoid': 7420, 'ic': 7421, 'te': 7422, 'auto-followback': 7423, 'ljp': 7424, 'nowdays': 7425, 'attach': 7426, 'numb': 7427, 'dentist': 7428, 'tab': 7429, 'ucas': 7430, 'bigtime': 7431, 'rumor': 7432, 'chin': 7433, 'tickle': 7434, '♫': 7435, 'zikra': 7436, 'lusi': 7437, 'hasya': 7438, 'nugget': 7439, 'olympic': 7440, \"millie's\": 7441, '748292': 7442, 'ano': 7443, '22stans': 7444, 'mag': 7445, 'hateee': 7446, 'lease': 7447, '6g': 7448, 'unsuccessful': 7449, 'earlobes': 7450, 'girls': 7451, 'sue': 7452, 'dreary': 7453, 'denise': 7454, 'murielle': 7455, 'ahouré': 7456, 'pr': 7457, \"kath'd\": 7458, 'respond': 7459, 'chopped': 7460, 'wbu': 7461, 'activity': 7462, 'kme': 7463, 'cram': 7464, 'curious': 7465, 'announcement': 7466, 'trespasser': 7467, 'clandestins': 7468, 'muller': 7469, 'obvious': 7470, 'mufc': 7471, 'stu': 7472, 'buddyyy': 7473, 'feelgoodfriday': 7474, '6:30': 7475, 'babysit': 7476, 'opixer': 7477, '805': 7478, 'pilllow': 7479, 'fool': 7480, 'brag': 7481, 'skrillah': 7482, 'drown': 7483, 'gue': 7484, 'north': 7485, 'sjkaos': 7486, 'disappointed': 7487, 'srry': 7488, 'honma': 7489, 'yeh': 7490, 'walay': 7491, 'bohat': 7492, 'wailay': 7493, 'pre-season': 7494, 'pe': 7495, 'itna': 7496, 'shor': 7497, 'machaya': 7498, 'samjha': 7499, '👍': 7500, '😔': 7501, 'sirkay': 7502, 'wali': 7503, 'pyaaz': 7504, 'daal': 7505, 'onion': 7506, 'vinegar': 7507, 'cooking': 7508, 'tutorial': 7509, 'soho': 7510, 'wobbly': 7511, 'ciao': 7512, 'masaan': 7513, 'muv': 7514, 'beast': 7515, 'hayst': 7516, 'cr': 7517, 'hnnn': 7518, 'optimisation': 7519, 'soniii': 7520, 'kahaaa': 7521, 'freeze': 7522, 'fml': 7523, 'jacket': 7524, 'sleepy': 7525, 'bully': 7526, 'racial': 7527, 'loool': 7528, 'onwards': 7529, 'coincidence': 7530, 'imac': 7531, 'gram': 7532, 'nearer': 7533, 'blaine': 7534, 'darren': 7535, 'fuuuck': 7536, 'gishwhes': 7537, 'exclude': 7538, 'movement': 7539, 'frou': 7540, 'vaccine': 7541, 'armor': 7542, 'legendary': 7543, 'cash': 7544, 'effort': 7545, 'nat': 7546, 'brake': 7547, 'grumpy': 7548, 'wreck': 7549, 'gahhh': 7550, 'terible': 7551, 'kiligs': 7552, 'shravan': 7553, 'maas': 7554, 'stooop': 7555, 'gi-guilty': 7556, 'akooo': 7557, 'imveryverysorry': 7558, 'cd': 7559, 'basename': 7560, 'theme': 7561, 'cigar': 7562, 'speaker': 7563, 'promethazine': 7564, 'zopiclone': 7565, 'addition': 7566, 'quetiapine': 7567, 'modified': 7568, 'prescription': 7569, 'greska': 7570, 'macedonian': 7571, 'slovak': 7572, 'hike': 7573, 'zokay': 7574, 'accent': 7575, 'b-but': 7576, 'gintama': 7577, 'shinsengumi': 7578, 'chapter': 7579, 'crapple': 7580, 'agrees': 7581, 'ftw': 7582, 'phandroid': 7583, 'tline': 7584, 'orchestra': 7585, 'rehearsal': 7586, 'bittersweetness': 7587, 'eunji': 7588, 'bakit': 7589, '121st': 7590, 'ehdar': 7591, 'pegea': 7592, 'panga': 7593, 'dosto': 7594, 'nd': 7595, 'real_liam_payne': 7596, '3/10': 7597, 'dmed': 7598, '23': 7599, 'alreaddyyy': 7600, 'luceleva': 7601, 'naeun': 7602, \"son's\": 7603, 'kidney': 7604, 'ink': 7605, 'bullying': 7606, 'ihatesomepeople': 7607, 'table': 7608, '0-2': 7609, 'hard-wired': 7610, 'canadian': 7611, 'enjoyed': 7612, 'acne': 7613, 'gulo': 7614, 'kandekjs': 7615, 'rize': 7616, 'meydan': 7617, 'fcking': 7618, 'crei': 7619, 'connection': 7620, 'dormmates': 7621, 'bo3': 7622, 'activation': 7623, 'cod': 7624, 'redeem': 7625, 'invalid': 7626, 'hopia': 7627, 'editor': 7628, 'reveal': 7629, 'booo': 7630, 'extension': 7631, 'rightnow': 7632, 'btu': 7633, 'karaoke': 7634, 'licence': 7635, 'apb': 7636, 'hahahaokay': 7637, 'basara': 7638, 'capcom': 7639, 'url': 7640, 'grumble': 7641, 'migrant': 7642, 'awsme': 7643, 'picking': 7644, 'tmw': 7645, 'uwu': 7646, 'jinki': 7647, 'taems': 7648, 'gifs': 7649, 'cambridge': 7650, 'viathe': 7651, 'cyprus': 7652, 'zayncomebackto': 7653, 'spazzing': 7654, 'soobin': 7655, '27': 7656, 'float': 7657, 'pressure': 7658, 'lifetime': 7659, 'hiondsheings': 7660, '58543': 7661, 'sexdate': 7662, \"demi's\": 7663, 'junjou': 7664, 'romantica': 7665, 'privilege': 7666, 'mixtape': 7667, 'convince': 7668, 'friex': 7669, 'shaylan': 7670, '4:20': 7671, 'ylona': 7672, 'nah': 7673, 'waitting': 7674, 'noon': 7675, 'ouh': 7676, 'nm': 7677, 'encanta': 7678, 'vale': 7679, 'osea': 7680, 'bea': 7681, '♛': 7682, '》': 7683, 'beli̇eve': 7684, 'wi̇ll': 7685, 'justi̇n': 7686, '350': 7687, 'ｓｅｅ': 7688, 'ｍｅ': 7689, '349': 7690, 'baek': 7691, 'dunwan': 7692, 'suan': 7693, 'haiz': 7694, '348': 7695, 'adult': 7696, '347': 7697, '😕': 7698, 'insonia': 7699, '346': 7700, 'rick': 7701, 'ross': 7702, 'heartbreaking': 7703, '345': 7704, 'millie': 7705, 'diff': 7706, 'golden': 7707, 'rosebury': 7708, 'familyhome': 7709, '344': 7710, 'monkey': 7711, '343': 7712, 'erica': 7713, 'istg': 7714, 'jackson': 7715, 'nsbzhdnxndamal': 7716, '342': 7717, '11:15': 7718, '2hours': 7719, '11:25': 7720, '341': 7721, 'mahilig': 7722, 'mam-bully': 7723, 'mtaani': 7724, 'tunaita': 7725, 'viazi': 7726, 'choma': 7727, 'jerk': 7728, 'menille': 7729, '340': 7730, \"kam's\": 7731, 'meee': 7732, 'diz': 7733, 'biooo': 7734, 'ay': 7735, 'taray': 7736, 'yumu-youtuber': 7737, '339': 7738, 'parijat': 7739, 'willmissyouparijat': 7740, 'jolly': 7741, '338': 7742, 'mcnuggets': 7743, 'sophie': 7744, 'caramello': 7745, 'koala': 7746, 'suckmejimin': 7747, '337': 7748, 'sucky': 7749, 'dying': 7750, 'pou': 7751, 'goddamn': 7752, 'nje': 7753, 'dbn': 7754, '🎀': 7755, '336': 7756, '335': 7757, 'supporting': 7758, 'pledge': 7759, 'viber': 7760, 'mwah': 7761, 'estate': 7762, 'lansi': 7763, '334': 7764, 'hp': 7765, 'waah': 7766, 'vandag': 7767, 'kgola': 7768, 'neng': 7769, 'eintlik': 7770, 'porn': 7771, '4like': 7772, 'repost': 7773, '333': 7774, 'magpie': 7775, '22.05': 7776, '15-24': 7777, '05.15': 7778, 'chswiyfxcskcalum': 7779, 'nvm': 7780, 'foof': 7781, '332': 7782, 'casillas': 7783, 'manchester': 7784, 'xi': 7785, 'rmtour': 7786, 'eating': 7787, 'irl': 7788, 'blooper': 7789, 'huhuhuhu': 7790, 'na-take': 7791, 'sorta': 7792, 'unfriend': 7793, 'greysonchance': 7794, 'sandwich': 7795, 'belle': 7796, 'sebastian': 7797, 'rewatched': 7798, 's4': 7799, 'sers': 7800, 'heart-breaking': 7801, 'outdated': 7802, 'm4': 7803, 'theater': 7804, '7-3': 7805, '7.30-': 7806, 'ekk': 7807, 'giriboy': 7808, 'danni': 7809, 'harriet': 7810, 'gegu': 7811, 'gray': 7812, '331': 7813, 'politics': 7814, 'blaming': 7815, '68': 7816, 'corbyn': 7817, \"labour's\": 7818, 'ry': 7819, 'lfccw': 7820, '5ever': 7821, 'ontheroadagain': 7822, 'halaaang': 7823, 'recieved': 7824, 'flop': 7825, 'caesarspalace': 7826, 'socialrewards': 7827, 'cali': 7828, 'fuckboys': 7829, '330': 7830, 'chrompet': 7831, 'immune': 7832, 'lush': 7833, 'bathtub': 7834, 'mysql': 7835, 'libmysqlclient-dev': 7836, 'dev': 7837, 'pleasanton': 7838, 'wala': 7839, '329': 7840, 'heed': 7841, '328': 7842, 'gwss': 7843, 'thankyouu': 7844, 'charade': 7845, 'piano': 7846, '327': 7847, 'complaint': 7848, 'yelling': 7849, 'whatsoever': 7850, 'pete': 7851, 'wentz': 7852, 'shogi': 7853, 'blameshoghicp': 7854, 'classmate': 7855, 'fixedgearfrenzy': 7856, 'dispatch': 7857, 'theyre': 7858, \"shamuon's\": 7859, 'toe': 7860, 'horrendously': 7861, \"someone's\": 7862, '326': 7863, 'hasb': 7864, 'atty': 7865, 'mujy': 7866, 'sirf': 7867, 'sensible': 7868, 'brum': 7869, 'cyclerevolution': 7870, 'caaannnttt': 7871, 'overdraw': 7872, 'tbf': 7873, 'perfume': 7874, 'sample': 7875, 'chanel': 7876, 'burberry': 7877, 'prada': 7878, '325': 7879, 'noesss': 7880, 'topgear': 7881, 'bridesmaid': 7882, 'nhs': 7883, \"tomorrow's\": 7884, 'gathering': 7885, 'sudden': 7886, '324': 7887, 'randomrestart': 7888, 'randomreboot': 7889, 'lumia': 7890, 'windowsphone': 7891, \"microsoft's\": 7892, 'addressing': 7893, 'mañana': 7894, 'rappings': 7895, 'striker': 7896, 'lvg': 7897, 'refurbish': 7898, 'cintiq': 7899, 'originate': 7900, \"finnick's\": 7901, 'askfinnick': 7902, 'container': 7903, 'hairy': 7904, '323': 7905, 'bury': 7906, 'omaygad': 7907, 'vic': 7908, 'surgery': 7909, 'tt.tt': 7910, 'hyper': 7911, '322': 7912, 'imiss': 7913, '321': 7914, '320': 7915, 'know.for': 7916, 'prepay': 7917, '319': 7918, 'grandma': 7919, \"grandpa's\": 7920, 'cow': 7921, 'sheeps': 7922, 'vegetable': 7923, 'delirious': 7924, 'motilium': 7925, 'shite': 7926, '318': 7927, 'schoolwork': 7928, \"phoebe's\": 7929, '317': 7930, 'pothole': 7931, '316': 7932, '1,300': 7933, 'robyn': 7934, 'necklace': 7935, 'rachel': 7936, 'ramzan': 7937, 'clapham': 7938, 'investigate': 7939, 'sth': 7940, 'essentially': 7941, 'photoshooot': 7942, 'mahone': 7943, 'shut': 7944, 'andaming': 7945, 'memorization': 7946, 'cotton': 7947, 'swallow': 7948, 'snot': 7949, 'taknottem': 7950, '477': 7951, 'btob': 7952, 'percentage': 7953, 'swift': 7954, 'a9': 7955, 'sexyjane': 7956, 'horny': 7957, 'goodmusic': 7958, 'lart': 7959, 'sew': 7960, 'orange': 7961, 'skyfall': 7962, 'premiere': 7963, 'manteca': 7964, \"she'd\": 7965, 'shiatsu': 7966, 'setting': 7967, 'risk': 7968, 'hopper': 7969, 'eyyah': 7970, 'utd': 7971, 'born': 7972, '1-0': 7973, 'cart': 7974, 'aaa': 7975, 'waifu': 7976, 'breakup': 7977, 'bias': 7978, 'syndrome': 7979, 'shy': 7980, 'pixelated': 7981, 'weh': 7982, 'maymay': 7983, 'advance': 7984, 'allowance': 7985, 'magpaalam': 7986, 'tf': 7987, 'subtitle': 7988, 'chang': 7989, 'backstory': 7990, 'gimme': 7991, 'meal': 7992, 'neat-o': 7993, 'wru': 7994, 'scissors': 7995, 'creation': 7996, 'amtired': 7997, 'imysm': 7998, 'tut': 7999, 'trop': 8000, 'tard': 8001, 'deadline': 8002, 'st': 8003, 'premiun': 8004, 'making': 8005, 'notcool': 8006, '2/3': 8007, 'lahat': 8008, 'araw': 8009, 'nag': 8010, 'gyu': 8011, 'lmfaooo': 8012, 'mashup': 8013, 'eu': 8014, 'lcs': 8015, 'yass': 8016, 'relative': 8017, 'yr': 8018, 'sydney': 8019, 'perf': 8020, 'hashtags': 8021, 'omfg': 8022, 'combat': 8023, 'dosent': 8024, \"sod's\": 8025, '20mins': 8026, 'yahoo': 8027, 'yodel': 8028, 'jokingly': 8029, 'seriousness': 8030, 'gahd': 8031, 'zayns': 8032, '26th': 8033, '12.00': 8034, 'obyun': 8035, 'wayhh': 8036, 'poisoning': 8037, 'prevalent': 8038, 'controversy': 8039, '🍵': 8040, 'tube': 8041, 'strike': 8042, 'meck': 8043, 'mcfc': 8044, 'ucan': 8045, 'depressing': 8046, 'poc': 8047, 'sms': 8048, 'specific': 8049, 'sinhala': 8050, 'billionaire': 8051, '1645': 8052, '1190': 8053, 'maldives': 8054, 'dheena': 8055, 'fasgadah': 8056, 'alvadhaau': 8057, 'countdown': 8058, 'function': 8059, 'desktop': 8060, 'evelineconrade': 8061, 'kikmsn': 8062, 'selfshot': 8063, 'backkk': 8064, 'transfer': 8065, 'relaxing': 8066, 'dull': 8067, 'overcast': 8068, 'missin': 8069, 'hangin': 8070, 'wiff': 8071, 'interactive': 8072, 'cherry': 8073, 'bakewell': 8074, 'collect': 8075, 'teal': 8076, 'sect': 8077, 'tennunb': 8078, 'skip': 8079, 'doomsday': 8080, 'neglected': 8081, 'postie': 8082, 'bellamy': 8083, 'raven': 8084, 'clarke': 8085, 'helmy': 8086, 'uh': 8087, 'cnt': 8088, 'whereisthesun': 8089, 'summerismissing': 8090, 'longgg': 8091, 'ridiculous': 8092, 'stocko': 8093, 'lucozade': 8094, 'shooting': 8095, 'explosion': 8096, 'beh': 8097, 'half-remembered': 8098, \"melody's\": 8099, 'recall': 8100, 'difficult': 8101, 'expo': 8102, 'jisoo': 8103, 'anon': 8104, 'mager': 8105, 'wi': 8106, 'wht': 8107, 'distant': 8108, 'buffering': 8109, 'insane': 8110, 'charli': 8111, 'ganas': 8112, 'studio': 8113, 'arch': 8114, 'lyin': 8115, 'kian': 8116, 'supercars': 8117, 'gurgaon': 8118, 'location': 8119, '9:15': 8120, 'satire': 8121, 'peanut': 8122, 'viners': 8123, 'palembang': 8124, 'sorrryyy': 8125, 'fany': 8126, 'boner': 8127, 'mercy': 8128, 'yuki': 8129, '2500k': 8130, 'jake': 8131, 'gyllenhaal': 8132, 'impact': 8133, \"ledger's\": 8134, 'b4': 8135, 'deplete': 8136, 'mbasa': 8137, 'aah': 8138, 'pa-copy': 8139, 'biome': 8140, 'mosque': 8141, 'smelly': 8142, 'annoyed': 8143, \"ciara's\": 8144, \"everything's\": 8145, 'hugs': 8146, 'tall': 8147, 'intention': 8148, 'ambs': 8149, \"harry's\": 8150, 'mayday': 8151, 'parade': 8152, 'lyf': 8153, '13th': 8154, 'animal': 8155, 'chris': 8156, 'brown': 8157, 'risky': 8158, 'cologne': 8159, 'duo': 8160, 'ballad': 8161, 'bish': 8162, 'intern': 8163, 'yumyum': 8164, \"cathy's\": 8165, 'missyou': 8166, 'bishes': 8167, 'ruby': 8168, 'pora': 8169, 'karlia': 8170, 'khatam': 8171, 'bandi': 8172, '👑': 8173, 'pyaari': 8174, 'gawd': 8175, 'massis': 8176, 'thatselfiethough': 8177, 'loop': 8178, 'aishhh': 8179, 'viewer': 8180, 'toffee': 8181, 'honesty': 8182, 'cheatday': 8183, 'protein': 8184, 'sissi': 8185, 'tote': 8186, 'slowly': 8187, 'breaking': 8188, 'church': 8189, 'pll': 8190, 'sel': 8191, 'serbia': 8192, 'serbian': 8193, 'selenators': 8194, 'motavators': 8195, 'zayyyn': 8196, 'happend': 8197, 'imperative': 8198, 'panas': 8199, 'sake': 8200, 'hamstring': 8201, 'rodwell': 8202, 'trace': 8203, 'tp': 8204, 'powder': 8205, 'wider': 8206, 'waking': 8207, 'bruno': 8208, '1.8': 8209, 'ed': 8210, 'croke': 8211, 'toll': 8212, 'shape': 8213, 'unluckiest': 8214, 'bettor': 8215, 'nstp': 8216, 'sem': 8217, 'tan': 8218, 'chipotle': 8219, 'chick-fil-a': 8220, 'stole': 8221, 'ramadhan': 8222, 'stexpert': 8223, 'ripstegi': 8224, 'nickyyy': 8225, '¿': 8226, 'emotion': 8227, 'centralise': 8228, 'discontinue': 8229, 'disappointment': 8230, 'sniff': 8231, \"i'ts\": 8232, 'therese': 8233, 'cred': 8234, 't_t': 8235, 'eliminate': 8236, 'teamzipal': 8237, 'smtm': 8238, 'assingnment': 8239, 'editied': 8240, 'nakaka': 8241, 'beastmode': 8242, 'gaaawd': 8243, 'colombia': 8244, 'yots': 8245, 'labyo': 8246, 'pano': 8247, 'nalamannn': 8248, 'hardheaded': 8249, \"zach's\": 8250, 'xpress': 8251, 'hopkins': 8252, 'melatonin': 8253, '2-4': 8254, 'hahaah': 8255, 'frequently': 8256, 'jail': 8257, 'weirddd': 8258, 'donghyuk': 8259, 'stans': 8260, 'beks': 8261, 'reynoldsgrl': 8262, 'ole': 8263, 'beardy': 8264, 'kaussies': 8265, 'pixels': 8266, 'bummer': 8267, 'fightingmcirene': 8268, \"michael's\": 8269, 'exercising': 8270, 'miserable': 8271, '💦': 8272, '💃🏽': 8273, 'shouldve': 8274, 'saffron': 8275, 'peasant': 8276, 'wouldve': 8277, 'nfinite': 8278, 'admin_myung': 8279, 'slp': 8280, 'laomma': 8281, 'kebaya': 8282, 'bandung': 8283, '7df89150': 8284, '62': 8285, '08962464174': 8286, 'laomma_couture': 8287, 'haizzz': 8288, 'urghhh': 8289, 'sat': 8290, 'working-on-a-tight-schedule': 8291, 'ganbarimasu': 8292, 'livid': 8293, 'whammy': 8294, 'quuuee': 8295, 'friooo': 8296, 'raining': 8297, 'stereo': 8298, 'chwang': 8299, 'lorm': 8300, '823': 8301, 'unhappy': 8302, 'lolzz': 8303, 'dats': 8304, 'corey': 8305, 'mahirap': 8306, 'noodle': 8307, 'veeerry': 8308, 'orig': 8309, 'starholicxx': 8310, '07:17': 8311, '@the': 8312, 'notr': 8313, 'hwy': 8314, 'niall': 8315, 'fraud': 8316, 'diplomacy': 8317, 'survival': 8318, 'zero': 8319, 'tolerant': 8320, 'pier': 8321, 'approach': 8322, 'rattle': 8323, 'robe': 8324, 'emphasis': 8325, 'abby.can': 8326, 'persuade': 8327, 'lyric': 8328, \"emily's\": 8329, 'elect': 8330, 'kamiss': 8331, 'mwa': 8332, 'cafe': 8333, 'melbourne': 8334, 'anyonneee': 8335, 'fricken': 8336, 'rito': 8337, 'friendzone': 8338, 'panel': 8339, 'hsm': 8340, 'canarios': 8341, 'ukiss': 8342, 'kurt': 8343, \"fatma'm\": 8344, 'lmfao': 8345, 'flapjack': 8346, 'countthecost': 8347, 'ihop': 8348, 'infra': 8349, 'lq': 8350, 'sotired': 8351, 'mybrainneedstoshutoff': 8352, 'maccies': 8353, '510': 8354, 'silicon': 8355, 'kbye': 8356, 'ini': 8357, 'citizen': 8358, 'ranking': 8359, 'mcountdown': 8360, '5h': 8361, 'thapelo': 8362, 'civ': 8363, 'wooden': 8364, 'mic': 8365, 'embarrassing': 8366, 'organization': 8367, 'translate': 8368, 'mecha-totems': 8369, 'nak': 8370, 'tgk': 8371, 'jokid': 8372, 'rent': 8373, 'inconsiderate': 8374, 'softball': 8375, 'tomcat': 8376, 'chel': 8377, 'jemma': 8378, 'matchy': 8379, 'elsa': 8380, 'postpone': 8381, 'karin': 8382, 'vist': 8383, 'unhealthy': 8384, 'propa': 8385, 'knockin': 8386, 'pre-holiday': 8387, 'meany': 8388, 'deathbybaconsmell': 8389, 'inital': 8390, 'destination': 8391, 'victoria': 8392, 'luna': 8393, 'krystal': 8394, 'sarajevo': 8395, 'haix': 8396, 'sp': 8397, 'wii': 8398, 'bayonetta': 8399, 'doable': 8400, 'drove': 8401, 'moved': 8402, 'agency': 8403, 'story.miss': 8404, 'everone': 8405, 'jps': 8406, 'mamabear': 8407, 'imintoher': 8408, 'underrated': 8409, \"slovakia's\": 8410, 'D:': 8411, 'saklap': 8412, 'rizal': 8413, 'lib': 8414, 'advisory': 8415, 'period': 8416, 'dit': 8417, 'dus': 8418, 'harsh': 8419, 'ohgod': 8420, 'abligaverins': 8421, 'sexygirlbypreciouslemmy': 8422, 'ripsandrabland': 8423, 'cri': 8424, 'edel': 8425, 'salam': 8426, 'mubark': 8427, 'dong': 8428, 'tammirossm': 8429, 'speck': 8430, 'abbymill': 8431, 'ion': 8432, '5min': 8433, 'hse': 8434, 'noob': 8435, 'fck': 8436, 'nae': 8437, 'whit': 8438, 'van': 8439, 'bristol': 8440, 'subserver': 8441, 'oo': 8442, 'tub': 8443, 'penyfan': 8444, 'breconbeacons': 8445, 'tittheir': 8446, '42': 8447, 'hottie': 8448, 'fuzzy': 8449, 'antonio': 8450, 'kang': 8451, 'junhee': 8452, 'couldve': 8453, 'pz': 8454, 'somerset': 8455, 'sunburnt': 8456, 'safer': 8457, 'k3g': 8458, 'input': 8459, 'gamestomp': 8460, 'desc': 8461, \"angelo's\": 8462, 'yna': 8463, 'fiver': 8464, 'sakho': 8465, 'threat': 8466, 'goalscorer': 8467, '10:59': 8468, '11.00': 8469, 'sham': 8470, 'tricky': 8471, 'baao': 8472, 'nisrina': 8473, 'bcs': 8474, 'ladygaga': 8475, \"you's\": 8476, 'marrish': 8477, \"otp's\": 8478, 'edomnt': 8479, 'qih': 8480, 'shxbs': 8481, 'chilton': 8482, 'creepy': 8483, 'boohoo': 8484, 'roar': 8485, 'victory': 8486, 'tweepsmatchout': 8487, 'nein': 8488, '404': 8489, 'willlow': 8490, 'sowwy': 8491, '3000': 8492, 'gear': 8493, '0.001': 8494, 'mode': 8495, 'madi': 8496, '11:11': 8497, 'shanzay': 8498, 'salabraty': 8499, 'journo': 8500, 'lure': 8501, 'mashaket': 8502, 'bapak': 8503, 'prima': 8504, 'mune': 8505, '874': 8506, 'plisss': 8507, 'sunway': 8508, 'petaling': 8509, 'jaya': 8510, 'selangor': 8511, 'huhuu': 8512, 'margo': 8513, 'konga': 8514, 'wa': 8515, 'ode': 8516, 'disvirgined': 8517, 'negotiate': 8518, 'bride': 8519, 'yulin': 8520, 'imma': 8521, 'syawal': 8522, 'lapar': 8523, 'foundation': 8524, 'facil': 8525, 'dh': 8526, 'chalet': 8527, 'suay': 8528, 'anot': 8529, 'bugger': 8530, 'एक': 8531, 'बार': 8532, 'फिर': 8533, 'सेँ': 8534, 'धोखा': 8535, 'chandauli': 8536, 'majhwar': 8537, 'tito': 8538, 'titas': 8539, 'critical': 8540, 'narcos': 8541, 'regens': 8542, 'unfaved': 8543, 'benadryl': 8544, 'arent': 8545, 'yg': 8546, 'gg': 8547, 'sxrew': 8548, 'dissappeared': 8549, 'swap': 8550, 'ishal': 8551, 'thaanks': 8552, 'jhezz': 8553, 'defence': 8554, 'defensive': 8555, 'nrltigersroosters': 8556, 'indiana': 8557, 'hibbs': 8558, 'biblethump': 8559, 'rlyyy': 8560, 'septum': 8561, 'pierce': 8562, 'venomous': 8563, 'carriage': 8564, 'fur-trimmed': 8565, 'stetson': 8566, 'error': 8567, '59': 8568, 'xue': 8569, 'midori': 8570, 'disabled': 8571, 'sakit': 8572, 'mateo': 8573, 'bartender': 8574, 'despair': 8575, 'insta': 8576, 'iwantin': 8577, '___': 8578, 'fault': 8579, 'help@veryhq.co.uk': 8580, 'benedictervention': 8581, '221b': 8582, 'popcorn': 8583, 'joyce': 8584, 'ooops': 8585, 'paalam': 8586, 'sazballs': 8587, 'incident': 8588, 'aaahh': 8589, \"stomach's\": 8590, 'growl': 8591, 'beard': 8592, 'nooope': 8593, 'hundred': 8594, 'meg': 8595, \"verity's\": 8596, 'rupert': 8597, 'pleaaase': 8598, '👆🏻': 8599, 'woaah': 8600, 'solvo': 8601, 'twin': 8602, 'lego': 8603, 'barefooted': 8604, 'twelvyy': 8605, 'boaz': 8606, 'myhill': 8607, 'takeover': 8608, 'wba': 8609, \"taeyeon's\": 8610, 'derp': 8611, 'pd': 8612, 'zoom': 8613, \"sunny's\": 8614, 'besst': 8615, 'plague': 8616, 'pit': 8617, 'frail': 8618, 'twurkin': 8619, 'razzist': 8620, 'tumblr': 8621, 'shek': 8622, '609': 8623, 'mugshot': 8624, 'plsss': 8625, 'taissa': 8626, 'farmiga': 8627, 'ahs': 8628, 'danielle': 8629, 'software': 8630, 'restore': 8631, 'momo': 8632, 'pharma': 8633, 'immovable': 8634, 'messy': 8635, 'anshe': 8636, 'f1': 8637, 'rand': 8638, 'bein': 8639, 'tla': 8640, 'tweng': 8641, 'gene': 8642, 'up.come': 8643, 'county': 8644, 'minhyuks': 8645, '1900': 8646, '😪': 8647, 'hz': 8648, 'emta': 8649, 'hatigii': 8650, 'b2aa': 8651, 'anesthesia': 8652, 'penrith': 8653, 'plain': 8654, 'untouched': 8655, 'brienne': 8656, 'lsh': 8657, 'gunna': 8658, 'former': 8659, 'darn': 8660, 'juudiciary': 8661, \"horton's\": 8662, 'dunkin': 8663, 'socialise': 8664, 'cara': 8665, \"delevingne's\": 8666, 'lace': 8667, 'fank': 8668, 'takfaham': 8669, 'older': 8670, 'ufff': 8671, 'sr': 8672, 'dard': 8673, 'katekyn': 8674, 'ehh': 8675, 'hacharatt': 8676, 'niwll': 8677, 'depend': 8678, 'successful': 8679, 'goa': 8680, 'linis': 8681, 'kasi': 8682, 'sweating': 8683, 'init': 8684, 'rhd': 8685, 'celebration': 8686, 'physical': 8687, 'wae': 8688, 'subsidized': 8689, '20th': 8690, 'youngjae': 8691, 'harumph': 8692, 'soggy': 8693, 'weeding': 8694, 'sakura': 8695, 'flavour': 8696, 'chokkie': 8697, '🌸': 8698, 'unavailable': 8699, 'richard': 8700, 'satya': 8701, 'aditya': 8702, '🍜': 8703, 'vibrate': 8704, 'cu': 8705, 'dhaka': 8706, 'cornettos': 8707, 'nosebleed': 8708, 'nintendo': 8709, 'wew': 8710, 'ramos': 8711, 'ground': 8712, 'shawn': 8713, 'mend': 8714, 'dinghy': 8715, 'skye': 8716, 'colleague': 8717, 'gagal': 8718, 'txt': 8719, 'sims': 8720, 'nooot': 8721, 'notch': 8722, 'thts': 8723, 'everyones': 8724, 'starve': 8725, '\\U000fe196': 8726, 'pyjama': 8727, 'suks': 8728, 'sleeping': 8729, 'swifties': 8730, 'sorna': 8731, 'lurgy': 8732, '6gb': 8733, 'fenestoscope': 8734, 'etienne': 8735, 'bandana': 8736, 'vagina': 8737, 'suriya': 8738, 'dangle': 8739, 'mjhe': 8740, 'aaj': 8741, 'kisi': 8742, 'kiya': 8743, 'eyesight': 8744, '25x30': 8745, 'aftenoon': 8746, 'booore': 8747, 'boyfriend': 8748, 'garage': 8749, 'gws': 8750, 'anatomy': 8751, 'no1': 8752, \"morisette's\": 8753, 'non-trial': 8754, 'sayhername': 8755, 'lootcrate': 8756, 'inca': 8757, 'trail': 8758, 'sandboarding': 8759, 'derby': 8760, 'unable': 8761, 'signature': 8762, 'dish': 8763, 'unfamiliar': 8764, 'shed': 8765, \"old's\": 8766, '14518344': 8767, '61': 8768, 'thirdwheeling': 8769, 'lovebird': 8770, 'imo': 8771, '@juliettemaughan': 8772, 'sensiesha': 8773, 'eldest': 8774, '😟': 8775, 'keedz': 8776, 'taybigail': 8777, 'tournament': 8778, 'ps4': 8779, 'kink': 8780, 'losing': 8781, 'streak': 8782, 'srsky': 8783, 'tdc': 8784, 'in-sensitiveness': 8785, 'cooperate': 8786, 'conversion': 8787, 'thurston': 8788, 'collins': 8789, 'quietly': 8790, 'kennel': 8791, '911': 8792, 'pluckersss': 8793, 'gion': 8794, '886': 8795, 'kidschoiceawards': 8796, 'ming': 8797, 'pbr': 8798, 'shoutout': 8799, 'periscope': 8800, 'uts': 8801, 'shawty': 8802, 'naw': 8803, \"sterling's\": 8804, '9muses': 8805, 'hrryok': 8806, 'wnt': 8807, '9:30': 8808, '9:48': 8809, '9/11': 8810, 'bueno': 8811, 'receptionist': 8812, 'ella': 8813, 'ketchup': 8814, 'tasteless': 8815, 'deantd': 8816, 'justgotkanekified': 8817, 'babes': 8818, 'notgonnabeactivefor': 8819, '2weeksdontmissittoomuch': 8820, '2013': 8821, 'vlog': 8822, 'turtle': 8823, 'cnn': 8824, 'strapline': 8825, 'theatre': 8826, 'guncontrol': 8827, \"thát's\": 8828, 'powerpoint': 8829, 'expectation': 8830, 'diner': 8831, 'no-no': 8832, 'hinde': 8833, 'circuit': 8834, 'secondary': 8835, 'sodders': 8836, 'mobitel': 8837, 'playstation': 8838, 'exp': 8839, 'misspell': 8840, 'hyungwon': 8841, 'needicecreamnow': 8842, 'repeatedly': 8843, 'nu-uh': 8844, 'jace': 8845, 'most': 8846, 'urgh': 8847, \"grigson's\": 8848, 'carrot': 8849, '>:-(': 8850, 'ughh': 8851, 'otter': 8852, 'protection': 8853, 'argh': 8854, 'pon': 8855, 'otl': 8856, 'sleepover': 8857, 'jesse': 8858, 'fabina': 8859, 'meant': 8860, 'gardening': 8861, \"barrista's\": 8862, 'pup': 8863, 'brolly': 8864, 'dey': 8865, 'bitin': 8866, 'pretzel': 8867, 'bb17': 8868, 'bblf': 8869, 'fuckin': 8870, 'vanilla': 8871, 'latte': 8872, 'skulker': 8873, 'thread': 8874, 'hungrrryyy': 8875, 'icloud': 8876, 'ipod': 8877, 'hallyu': 8878, 'über': 8879, 'okie': 8880, '8p': 8881, 'harlo': 8882, 'torrentialrain': 8883, 'lloyd': 8884, 'knowww': 8885, 'runny': 8886, 'sweater': 8887, 'intolerant': 8888, 'xenophobes': 8889, 'wtfff': 8890, 'tone': 8891, '1pm': 8892, 'pish': 8893, 'comparison': 8894, 'remastered': 8895, 'fe14': 8896, 'cornetto': 8897, 'strawberry': 8898, 'kapatidkongpogi': 8899, 'mel': 8900, 'carmen': 8901, 'login': 8902, '00128835': 8903, 'wingstop': 8904, 'budge': 8905, 'fuq': 8906, 'ilhoon': 8907, 'getthescoop': 8908, 'hearess': 8909, '677': 8910, 'txt_shot': 8911, 'unfollowing': 8912, 'standby': 8913, 'inatall': 8914, 'zenmate': 8915, 'namechecking': 8916, 'whistle': 8917, 'junmyeon': 8918, 'ddy': 8919, 'arini': 8920, 'je': 8921, 'igbo': 8922, 'blamehoney': 8923, 'whhr': 8924, 'snuggle': 8925, 'usage': 8926, 'warning': 8927, 'tweeting': 8928, 'animator': 8929, 'vertigo': 8930, 'panic': 8931, 'dual': 8932, 'carriageway': 8933, 'aragalang': 8934, '08': 8935, 'tams': 8936, 'theo': 8937, 'anymoreee': 8938, 'cactus': 8939, 'sorrry': 8940, 'bowel': 8941, 'tumour': 8942, 'puffy': 8943, 'eyelid': 8944, 'musicas': 8945, 'campsite': 8946, 'miah': 8947, 'hahays': 8948, 'churro': 8949, 'montana': 8950, 'reign': 8951, 'example': 8952, 'inflation': 8953, 'sic': 8954, 'reset': 8955, 'entlerbountly': 8956, 'dirtykik': 8957, 'sexcam': 8958, 'spray': 8959, 'postcode': 8960, 'kafi': 8961, 'mene': 8962, 'koi': 8963, 'rewert': 8964, 'bunta': 8965, 'warnaaa': 8966, 'torture': 8967, 'iran': 8968, 'irandeal': 8969, 'us-iran': 8970, 'nuclear': 8971, \"mit's\": 8972, 'severely': 8973, 'li': 8974, 's2e12': 8975, 'rumpy': 8976, 'gallon': 8977, 'responsibility': 8978, 'dandia': 8979, 'rbi': 8980, 'cage': 8981, 'parrot': 8982, '1ly': 8983, 'commission': 8984, 'cag': 8985, 'ily.melanie': 8986, 'unlike': 8987, 'talent': 8988, 'deepxcape': 8989, 'doin': 8990, '5:08': 8991, 'thesis': 8992, 'gtg': 8993, 'compete': 8994, 'vv': 8995, 'respect': 8996, 'nys': 8997, 'opt-outed': 8998, 'vam': 8999, 'testing': 9000, 'speced': 9001, 'ell': 9002, 'sexyamelie': 9003, 'fineandyu': 9004, 'imsorry': 9005, 'koe': 9006, 'emyu': 9007, 'confetti': 9008, 'sini': 9009, 'dipoppo': 9010, 'bestweekend': 9011, 'okay-ish': 9012, 'html': 9013, 'geneva': 9014, 'patml': 9015, '482': 9016, 'abouty': 9017, '797': 9018, 'reaally': 9019, 'meter': 9020, 'unanswered': 9021, 'bri': 9022, 'magcon': 9023, 'merch': 9024, 'sinuend': 9025, 'laper': 9026, 'rage': 9027, 'brendon': 9028, \"urie's\": 9029, 'sumer': 9030, 'repackage': 9031, \":'D\": 9032, 'yongbe': 9033, 'suede': 9034, 'warm-up': 9035, 'signing': 9036, 'rub': 9037, 'belly': 9038, 'jannatul': 9039, 'ferdous': 9040, 'ami': 9041, 'ekta': 9042, 'kharap': 9043, 'manush': 9044, 'mart': 9045, 'gua': 9046, 'can': 9047, \"khloe's\": 9048, 'nhe': 9049, 'yar': 9050, 'minkyuk': 9051, 'hols': 9052, 'grown': 9053, 'sensor': 9054, 'broker': 9055, 'wna': 9056, 'flaviana': 9057, 'chickmt': 9058, '123': 9059, 'letsfootball': 9060, 'atk': 9061, 'greymind': 9062, 'gayle': 9063, 'mood-dump': 9064, 'livestream': 9065, 'felton': 9066, 'verity': 9067, \"standen's\": 9068, '😆': 9069, 'takoyaki': 9070, 'aisyah': 9071, 'ffvi': 9072, 'youtu.be/2_gpctsojkw': 9073, '50p': 9074, 'grate': 9075, 'sparse': 9076, 'lagi': 9077, 'rider': 9078, 'hueee': 9079, 'thingy': 9080, 'george': 9081, 'chew': 9082, 'stella': 9083, 'theaccidentalcouple': 9084, 'smooth': 9085, 'handover': 9086, 'spick': 9087, 'offense': 9088, 'bebii': 9089, 'happenend': 9090, 'dr': 9091, 'balm': 9092, 'hmph': 9093, 'bubba': 9094, 'floor': 9095, 'oi': 9096, 'bengali': 9097, 'insecure': 9098, 'masterchef': 9099, 'whatchya': 9100, 'petrol': 9101, 'diesel': 9102, 'cock': 9103, 'nyquil': 9104, 'poootek': 9105, '1,500': 9106, 'bobble': 9107, 'leak': 9108, 'thermos': 9109, 'tae': 9110, 'confusing': 9111, 'kita': 9112, 'ia': 9113, 'developed': 9114, 'corrupted': 9115, 'anything.surely': 9116, 'october': 9117, 'ene': 9118, '3k': 9119, 'zehr': 9120, 'khany': 9121, 'grocery': 9122, 'hubba': 9123, 'gum': 9124, 'closet': 9125, 'jhalak': 9126, 'bakwas': 9127, '. ...': 9128, 'seehiah': 9129, 'goy': 9130, 'nachos': 9131, 'braid': 9132, 'initial': 9133, 'ruth': 9134, 'boong': 9135, 'gta': 9136, 'cwnt': 9137, 'trivia': 9138, 'bdays': 9139, 'rohingya': 9140, 'muslims': 9141, 'indict': 9142, 'trafficking': 9143, 'thailand': 9144, 'rumble': 9145, 'kumble': 9146, 'scold': 9147, 'phrase': 9148, 'tfw': 9149, 'jest': 9150, 'relaxes': 9151, 'offend': 9152, 'sleepingwithsirens': 9153, '17th': 9154, 'bringmethehorizon': 9155, 'carva': 9156, 'regularly': 9157, 'sympathis': 9158, 'revamps': 9159, 'mosquito': 9160, 'headphone': 9161, 'breathing': 9162, 'wacha': 9163, 'niende': 9164, '2hrs': 9165, '13m': 9166, 'kk': 9167, 'calibraksaep': 9168, 'darlin': 9169, 'stunning': 9170, \"doedn't\": 9171, 'meaningful': 9172, 'horrific': 9173, 'scoups': 9174, 'royally': 9175, 'sweedy': 9176, 'nams': 9177, \"sacconejoly's\": 9178, 'bethesda': 9179, 'fallout': 9180, 'likee': 9181, 'minecon': 9182, 'kateee': 9183, 'iloveyouu': 9184, 'linux': 9185, 'nawwwe': 9186, 'chikka': 9187, 'ug': 9188, 'rata': 9189, 'soonest': 9190, 'mwamwa': 9191, 'faggot': 9192, 'opener': 9193, 'fyi': 9194, 'mehendi': 9195, 'dash': 9196, 'bookmark': 9197, 'whay': 9198, 'shaa': 9199, 'pramis': 9200, '😚': 9201, 'ngee': 9202, 'ann': 9203, 'crikey': 9204, 'snit': 9205, 'tiring': 9206, 'nathanielhinanakit': 9207, 'naya': 9208, 'spinny': 9209, 'loading': 9210, 'wheel': 9211, 'notifs': 9212, 'albeit': 9213, 'disappointing': 9214, 'athlete': 9215, 'racing': 9216, 'stripe': 9217, 'gfriend': 9218, 'screenshots': 9219, 'fugly': 9220, 'jongdae': 9221, 'tlists': 9222, 'recommended': 9223, 'budget': 9224, 'pabebegirls': 9225, 'pabebe': 9226, 'sandra': 9227, 'bland': 9228, 'storify': 9229, 'mtvhottest': 9230, 'gaga': 9231, '😵': 9232, 'hulkamania': 9233, 'unloved': 9234, 'ihhh': 9235, 'stackare': 9236, 'remedy': 9237, 'ov': 9238, 'raiz': 9239, 'nvr': 9240, 'gv': 9241, 'up.wt': 9242, 'wt': 9243, 'thr': 9244, 'soln': 9245, \"sister's\": 9246, 'pipe': 9247, 'lawn': 9248, \"cupid's\": 9249, 'retainer': 9250, 'clown': 9251, 'lipstick': 9252, 'haiss': 9253, 'todayy': 9254, 'thoo': 9255, 'everday': 9256, 'hangout': 9257, 'steven': 9258, 'william': 9259, 'umboh': 9260, 'jadines': 9261, 'thiz': 9262, 'iz': 9263, 'emeged': 9264, 'kennat': 9265, 'abi': 9266, 'arctic': 9267, 'chicsirific': 9268, 'structured': 9269, 'cumbia': 9270, 'badlife': 9271, '4-5': 9272, 'kaslkdja': 9273, '3wks': 9274, 'feverfew': 9275, 'weddingflowers': 9276, 'diyflowers': 9277, 'fitnes': 9278, 'wolverine': 9279, 'innocent': 9280, '🙏🏻': 9281, '🎂': 9282, 'mememe': 9283, 'krystoria': 9284, 'snob': 9285, 'zumba': 9286, 'greekcrisis': 9287, 'remain': 9288, 'artistic': 9289, 'dutch': 9290, 'legible': 9291, 'israeli': 9292, 'passport': 9293, 'froze': 9294, '23rd': 9295, 'stomachache': 9296, 'ཀ': 9297, 'agains': 9298, 'otani': 9299, '3-0': 9300, 'niaaa': 9301, '2/4': 9302, 'scheme': 9303, 'fckin': 9304, 'vin': 9305, 'plss': 9306, 'rply': 9307, 'rat': 9308, 'mac': 9309, 'backup': 9310, 'actual': 9311, 'lunes': 9312, 'martes': 9313, 'robinhood': 9314, 'robinhoodies': 9315, '🚙': 9316, 'docopenhagen': 9317, 'setter': 9318, 'swipe': 9319, 'bbygurl': 9320, 'caribbean': 9321, '6yrs': 9322, 'takraw': 9323, 'fersuree': 9324, 'angie': 9325, 'sheriff': 9326, 'aaages': 9327, \"i'mo\": 9328, 'sulk': 9329, 'selfish': 9330, 'nonce': 9331, 'bison': 9332, 'motivate': 9333, \"q'don\": 9334, 'cheat': 9335, 'stomping': 9336, 'aaaaaaaaah': 9337, 'kanye': 9338, 'jdjdjdjd': 9339, \"jimin's\": 9340, 'fancafe': 9341, 'flipping': 9342, 'waffle': 9343, '87.7': 9344, '2fm': 9345, 'himseek': 9346, 'kissme': 9347, 'glo': 9348, 'cory': 9349, 'monteith': 9350, 'hashbrowns': 9351, 'pgs': 9352, 'msc': 9353, 'hierro': 9354, 'shirleycam': 9355, 'looks': 9356, 'gilet': 9357, 'cheek': 9358, 'squishy': 9359, 'donating': 9360, 'lahhh': 9361, 'eon': 9362, 'sunrise': 9363, 'beety': 9364, '697': 9365, 'getaway': 9366, 'criminal': 9367, 'amiibo': 9368, 'habe': 9369, 'siannn': 9370, 'chuckin': 9371, 'ampsha': 9372, 'nia': 9373, 'strap': 9374, 'dz9055': 9375, 'entlead': 9376, '590': 9377, 'nudes': 9378, '07:02': 9379, 'ifsc': 9380, 'mayor': 9381, 'biodiversity': 9382, 'taxonomic': 9383, 'collaboration': 9384, 'specie': 9385, 'collar': 9386, '3:03': 9387, 'belt': 9388, 'smith': 9389, 'eyeliner': 9390, 'therefore': 9391, 'netherlands': 9392, 'el': 9393, 'jeb': 9394, 'blacklivesmatter': 9395, 'slogan': 9396, 'msnbc': 9397, 'jebbush': 9398, 'famish': 9399, 'marino': 9400, 'qualify': 9401, 'suzy': 9402, 'skirt': 9403, 'tama': 9404, 'warrior': 9405, 'wound': 9406, 'iraq': 9407, 'camara': 9408, 'coverall': 9409, 'sneezy': 9410, 'rogerwatch': 9411, 'stalker': 9412, 'velvet': 9413, 'tradition': 9414, 'beheaviour': 9415, \"robert's\": 9416, '.\\n.': 9417, 'aaron': 9418, 'jelouse': 9419, 'mtg': 9420, 'thoughtseized': 9421, 'playables': 9422, 'oldie': 9423, 'goody': 9424, 'mcg': 9425, 'inspirit': 9426, 'ised': 9427, 'assume': 9428, 'waisted': 9429, 'guinness': 9430, 'venue': 9431, 'pepper': 9432, 'thessidew': 9433, '877': 9434, 'genesis': 9435, 'november': 9436, 'mash': 9437, 'whattsap': 9438, 'inuyasha': 9439, 'outfwith': 9440, 'myungsoo': 9441, 'yeol': 9442, 'satisfied': 9443, 'challo': 9444, 'pliss': 9445, 'juliana': 9446, 'enroll': 9447, 'darlene': 9448, 'emoji': 9449, 'brisbane': 9450, 'merlin': 9451, 'nawwwee': 9452, 'hyperbullies': 9453, 'tong': 9454, 'nga': 9455, 'seatmates': 9456, 'rajud': 9457, 'ore': 9458, 'kaylas': 9459, 'ericavan': 9460, 'jong': 9461, 'dongwoo': 9462, 'photocards': 9463, 'wh': 9464, 'dw': 9465, 'tumor': 9466, 'vivian': 9467, 'mmsmalubhangsakit': 9468, 'jillcruz': 9469, 'qt': 9470, '19th': 9471, 'co-worker': 9472, 'starving': 9473, 'unsettled': 9474, 'gh': 9475, '18c': 9476, 'rlly': 9477, 'hamster': 9478, 'sheeran': 9479, 'preform': 9480, 'monash': 9481, 'hitmarker': 9482, 'glitch': 9483, 'safaa': 9484, \"selena's\": 9485, 'galat': 9486, 'tum': 9487, 'ab': 9488, 'lrka': 9489, 'bna': 9490, 'bhook': 9491, 'afterschool': 9492, 'bilal': 9493, 'ashraf': 9494, 'icu': 9495, 'annnd': 9496, 'winchester': 9497, '{:': 9498, 'dms': 9499, 'grepe': 9500, 'grepein': 9501, 'panem': 9502, 'sulli': 9503, 'injured': 9504, 'cpm': 9505, 'condemn': 9506, 'political': 9507, '✔': 9508, 'occur': 9509, 'mentality': 9510, 'unagi': 9511, '7elw': 9512, 'mesh': 9513, 'beyt': 9514, '3a2ad': 9515, 'fluent': 9516, 'varsity': 9517, 'sengenza': 9518, 'typos': 9519, 'movnat': 9520, 'yield': 9521, 'nbheroes': 9522, 'agover': 9523, 'brasileirao': 9524, 'abusive': 9525, 'unfollower': 9526, 'unparents': 9527, 'bianca': 9528, 'bun': 9529, 'dislike': 9530, 'burdensome': 9531, 'amelia': 9532, 'melon': 9533, 'soccer': 9534}\n"
          ]
        }
      ],
      "source": [
        "def build_vocabulary(corpus):\n",
        "    '''Function that builds a vocabulary from the given corpus\n",
        "    Input: \n",
        "        - corpus (list): the corpus\n",
        "    Output:\n",
        "        - vocab (dict): Dictionary of all the words in the corpus.\n",
        "                The keys are the words and the values are integers.\n",
        "    '''\n",
        "\n",
        "    # The vocabulary includes special tokens like padding token and token for unknown words\n",
        "    # Keys are words and values are distinct integers (increasing by one from 0)\n",
        "    vocab = {'': 0, '[UNK]': 1} \n",
        "    c = 2\n",
        "    # For each tweet in the training set\n",
        "    for tweet in corpus:\n",
        "        # For each word in the tweet\n",
        "        for word in tweet:\n",
        "            # If the word is not in vocabulary yet, add it to vocabulary\n",
        "            if word not in vocab:\n",
        "                vocab[word] = c\n",
        "                c += 1\n",
        "   \n",
        "    return vocab\n",
        "\n",
        "\n",
        "vocab = build_vocabulary(train_x)\n",
        "num_words = len(vocab)\n",
        "\n",
        "print(f\"Vocabulary contains {num_words} words\\n\")\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name=\"2-3\"></a>\n",
        "### 2.3 - Convert a Tweet to a Tensor\n",
        "\n",
        "Next, you will write a function that will convert each tweet to a tensor (a list of integer IDs representing the processed tweet).\n",
        "- You already transformed each tweet to a list of tokens with the `process_tweet` function in order to make a vocabulary.\n",
        "- Now you will transform the tokens to integers and pad the tensors so they all have equal length.\n",
        "- Note, the returned data type will be a **regular Python `list()`**\n",
        "    - You won't use TensorFlow in this function\n",
        "    - You also won't use a numpy array\n",
        "- For words in the tweet that are not in the vocabulary, set them to the unique ID for the token `[UNK]`.\n",
        "\n",
        "##### Example\n",
        "You had the original tweet:\n",
        "```CPP\n",
        "'@happypuppy, is Maria happy?'\n",
        "```\n",
        "\n",
        "The tweet is already converted into a list of tokens (including only relevant words).\n",
        "```CPP\n",
        "['maria', 'happy']\n",
        "```\n",
        "\n",
        "Now you will convert each word into its unique integer.\n",
        "\n",
        "```CPP\n",
        "[1, 55]\n",
        "```\n",
        "- Notice that the word \"maria\" is not in the vocabulary, so it is assigned the unique integer associated with the `[UNK]` token, because it is considered \"unknown.\"\n",
        "\n",
        "After that, you will pad the tweet with zeros so that all the tweets have the same length.\n",
        "\n",
        "```CPP\n",
        "[1, 56, 0, 0, ... , 0]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([4.760e+03, 3.373e+03, 1.605e+03, 2.090e+02, 1.300e+01, 3.800e+01,\n",
              "        1.000e+00, 0.000e+00, 0.000e+00, 1.000e+00]),\n",
              " array([ 0. ,  5.1, 10.2, 15.3, 20.4, 25.5, 30.6, 35.7, 40.8, 45.9, 51. ]),\n",
              " <BarContainer object of 10 artists>)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Tweet lengths\n",
        "plt.hist([len(t) for t in train_x + val_x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The length of the longest tweet is 51 tokens.\n"
          ]
        }
      ],
      "source": [
        "def max_length(training_x, validation_x):\n",
        "    \"\"\"Computes the length of the longest tweet in the training and validation sets.\n",
        "\n",
        "    Args:\n",
        "        training_x (list): The tweets in the training set.\n",
        "        validation_x (list): The tweets in the validation set.\n",
        "\n",
        "    Returns:\n",
        "        int: Length of the longest tweet.\n",
        "    \"\"\"\n",
        "    max_len = 0\n",
        "    for tweet in (training_x+validation_x):\n",
        "        if max_len < len(tweet):\n",
        "            max_len = len(tweet)\n",
        "    \n",
        "    return max_len\n",
        "\n",
        "max_len = max_length(train_x, val_x)\n",
        "print(f'The length of the longest tweet is {max_len} tokens.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def padded_sequence(tweet, vocab_dict, max_len, unk_token='[UNK]'):\n",
        "    \"\"\"Transform sequences of words into padded sequences of numbers.\n",
        "\n",
        "    Args:\n",
        "        tweet (list): A single tweet encoded as a list of strings.\n",
        "        vocab_dict (dict): Vocabulary.\n",
        "        max_len (int): Length of the longest tweet.\n",
        "        unk_token (str, optional): Unknown token. Defaults to '[UNK]'.\n",
        "\n",
        "    Returns:\n",
        "        list: Padded tweet encoded as a list of int.\n",
        "    \"\"\"\n",
        "    # Find the ID of the UNK token, to use it when encountering a new word\n",
        "    unk_ID = vocab_dict.get(unk_token, None)\n",
        "    \n",
        "    # Convert the words to integers by looking up the vocab_dict\n",
        "    int_sequence = [vocab_dict.get(word, unk_ID) for word in tweet]\n",
        "    \n",
        "    # Pad the sequence with zeroes up to the length max_len\n",
        "    padded_sequence = int_sequence + [0] * (max_len - len(int_sequence))\n",
        "    \n",
        "    return padded_sequence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pad the train and validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_x_padded = [padded_sequence(x, vocab, max_len) for x in train_x]\n",
        "val_x_padded = [padded_sequence(x, vocab, max_len) for x in val_x]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the structure of the neural network layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Dense Class \n",
        "\n",
        "Implement the weight initialization in the `__init__` method.\n",
        "- Weights are initialized with a random key.\n",
        "- The shape of the weights (num_rows, num_cols) should equal the number of columns in the input data (this is in the last column) and the number of units respectively.\n",
        "    - The number of rows in the weight matrix should equal the number of columns in the input data `x`.  Since `x` may have 2 dimensions if it represents a single training example (row, col), or three dimensions (batch_size, row, col), get the last dimension from the tuple that holds the dimensions of x.\n",
        "    - The number of columns in the weight matrix is the number of units chosen for that dense layer.\n",
        "- The values generated should have a mean of 0 and standard deviation of `stdev`.\n",
        "    - To initialize random weights, a random generator is created using `random_generator = np.random.default_rng(seed=random_seed)`. This part is implemented for you. You will use `random_generator.normal(...)` to create your random weights. Check [here](https://numpy.org/doc/stable/reference/random/generator.html) how the random generator works.\n",
        "\n",
        "Implement the `forward` function of the Dense class. \n",
        "- The forward function multiplies the input to the layer (`x`) by the weight matrix (`W`)\n",
        "\n",
        "$$\\mathrm{forward}(\\mathbf{x},\\mathbf{W}) = \\mathbf{xW} $$\n",
        "\n",
        "- You can use `numpy.dot` to perform the matrix multiplication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Dense():\n",
        "    \"\"\"\n",
        "    A dense (fully-connected) layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_units, input_shape, activation, stdev=0.1, random_seed=42):\n",
        "        \"\"\"\n",
        "        Initialize the dense layer.\n",
        "\n",
        "        Args:\n",
        "        n_units (int): Number of units in the layer.\n",
        "        input_shape (tuple): Shape of the input data.\n",
        "        activation (callable): Activation function to use.\n",
        "        stdev (float, optional): Standard deviation for weight initialization. Defaults to 0.1.\n",
        "        random_seed (int, optional): Seed for random weight initialization. Defaults to 42.\n",
        "        \"\"\"\n",
        "        # Set the number of units in this layer\n",
        "        self.n_units = n_units\n",
        "        # Set the random key for initializing weights\n",
        "        self.random_generator = np.random.default_rng(seed=random_seed)\n",
        "        self.activation = activation\n",
        "        \n",
        "        # Get the last dimension of the input data\n",
        "        input_dim = input_shape[-1]\n",
        "        \n",
        "        # Generate the weight matrix from a normal distribution with a standard deviation of 'stdev'\n",
        "        self.weights = self.random_generator.normal(loc=0, scale=stdev, size=(input_dim, n_units))\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"\n",
        "        Call function for the layer.\n",
        "\n",
        "        Args:\n",
        "        x (numpy.ndarray): Input data.\n",
        "\n",
        "        Returns:\n",
        "        numpy.ndarray: Output of the layer after forward pass.\n",
        "        \"\"\"\n",
        "        return self.forward(x)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the layer.\n",
        "\n",
        "        Args:\n",
        "        x (numpy.ndarray): Input data.\n",
        "\n",
        "        Returns:\n",
        "        numpy.ndarray: Output of the layer.\n",
        "        \"\"\"\n",
        "        # Matrix multiply x and the weight matrix\n",
        "        dense = np.dot(x, self.weights)\n",
        "        \n",
        "        # Apply the activation function\n",
        "        if self.activation is not None:\n",
        "            dense = self.activation(dense)\n",
        "        \n",
        "        return dense\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    '''Relu activation function implementation\n",
        "    Input: \n",
        "        - x (numpy array)\n",
        "    Output:\n",
        "        - activation (numpy array): input with negative values set to zero\n",
        "    '''\n",
        "    activation = np.maximum(0,x)\n",
        "    return activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weights are:\n",
            " [[ 0.03047171 -0.10399841  0.07504512  0.09405647 -0.19510352 -0.13021795\n",
            "   0.01278404 -0.03162426 -0.00168012 -0.08530439]\n",
            " [ 0.0879398   0.07777919  0.00660307  0.11272412  0.04675093 -0.08592925\n",
            "   0.03687508 -0.09588826  0.08784503 -0.00499259]\n",
            " [-0.01848624 -0.06809295  0.12225413 -0.01545295 -0.04283278 -0.03521336\n",
            "   0.05323092  0.03654441  0.04127326  0.0430821 ]]\n",
            "Foward function output is: [[0.21436609 0.         3.25266507 0.59085808 0.         0.\n",
            "  1.61446659 0.17914382 1.64338651 0.87149558]]\n"
          ]
        }
      ],
      "source": [
        "## Examaple\n",
        "# random_key = np.random.get_prng()  # sets random seed\n",
        "z = np.array([[2.0, 7.0, 25.0]]) # input array\n",
        "\n",
        "# Testing your Dense layer \n",
        "dense_layer = Dense(n_units=10, input_shape=z.shape, activation=relu)  #sets  number of units in dense layer\n",
        "\n",
        "print(\"Weights are:\\n\",dense_layer.weights) #Returns randomly generated weights\n",
        "print(\"Foward function output is:\", dense_layer(z)) # Returns multiplied values of units and weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model\n",
        "\n",
        "Now you will implement a classifier using neural networks. Here is the model architecture you will be implementing. \n",
        "\n",
        "\n",
        "For the model implementation, you will use `TensorFlow` module, imported as `tf`. Your model will consist of layers and activation functions that you implemented above, but you will take them directly from the tensorflow library.\n",
        "\n",
        "You will use the [tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) module, which allows you to stack the layers in a sequence as you want them in the model. You will use the following layers:\n",
        "- [tf.keras.layers.Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)\n",
        "    - Turns positive integers (word indices) into vectors of fixed size. You can imagine it as creating one-hot vectors out of indices and then running them through a fully-connected (dense) layer.\n",
        "- [tf.keras.layers.GlobalAveragePooling1D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling1D)\n",
        "- [tf.keras.layers.Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)\n",
        "    - Regular fully connected layer\n",
        "    \n",
        "Please use the `help` function to view documentation for each layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First you need to create the model. The `tf.keras.Sequential` has been implemented for you. Within it you should put the following layers:\n",
        "- `tf.keras.layers.Embedding` with the size `num_words` times `embeding_dim` and the `input_length` set to the length of the input sequences (which is the length of the longest tweet).\n",
        "- `tf.keras.layers.GlobalAveragePooling1D` with no extra parameters.\n",
        "- `tf.keras.layers.Dense` with the size of one (this is your classification output) and `'sigmoid'` activation passed to the  `activation` keyword parameter.\n",
        "Make sure to separate the layers with a comma.\n",
        "\n",
        "Then you need to compile the model. Here you can look at all the parameters you can set when compiling the model:  [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model). In this notebook, you just need to set the loss to `'binary_crossentropy'` (because you are doing binary classification with a sigmoid function at the output), the optimizer to `'adam'` and the metrics to `'accuracy'` (so that you can track the accuracy on the training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_model(num_words, embedding_dim, max_len):\n",
        "    \"\"\"\n",
        "    Creates a text classifier model\n",
        "    \n",
        "    Args:\n",
        "        num_words (int): size of the vocabulary for the Embedding layer input\n",
        "        embedding_dim (int): dimensionality of the Embedding layer output\n",
        "        max_len (int): length of the input sequences\n",
        "    \n",
        "    Returns:\n",
        "        model (tf.keras Model): the text classifier model\n",
        "    \"\"\"\n",
        "    tf.random.set_seed(123)\n",
        "    \n",
        "    # Create the model\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(num_words, embedding_dim, input_length=max_len),\n",
        "        tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    # Compile the model\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\hp\\anaconda3\\envs\\WebApp\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\hp\\anaconda3\\envs\\WebApp\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = create_model(num_words=num_words, embedding_dim=16, max_len=max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The data is prepared for training!\n",
            "\n",
            "Training:\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:From c:\\Users\\hp\\anaconda3\\envs\\WebApp\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\hp\\anaconda3\\envs\\WebApp\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            "250/250 [==============================] - 10s 23ms/step - loss: 0.6808 - accuracy: 0.6628 - val_loss: 0.6629 - val_accuracy: 0.9920\n",
            "Epoch 2/20\n",
            "250/250 [==============================] - 5s 18ms/step - loss: 0.6273 - accuracy: 0.9628 - val_loss: 0.5925 - val_accuracy: 0.9770\n",
            "Epoch 3/20\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.5353 - accuracy: 0.9874 - val_loss: 0.4952 - val_accuracy: 0.9885\n",
            "Epoch 4/20\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.4296 - accuracy: 0.9902 - val_loss: 0.3953 - val_accuracy: 0.9930\n",
            "Epoch 5/20\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.3334 - accuracy: 0.9944 - val_loss: 0.3096 - val_accuracy: 0.9920\n",
            "Epoch 6/20\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.2556 - accuracy: 0.9944 - val_loss: 0.2428 - val_accuracy: 0.9955\n",
            "Epoch 7/20\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 0.1965 - accuracy: 0.9958 - val_loss: 0.1903 - val_accuracy: 0.9945\n",
            "Epoch 8/20\n",
            "250/250 [==============================] - 2s 8ms/step - loss: 0.1523 - accuracy: 0.9962 - val_loss: 0.1514 - val_accuracy: 0.9950\n",
            "Epoch 9/20\n",
            "250/250 [==============================] - 3s 14ms/step - loss: 0.1200 - accuracy: 0.9964 - val_loss: 0.1222 - val_accuracy: 0.9950\n",
            "Epoch 10/20\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.0957 - accuracy: 0.9970 - val_loss: 0.0995 - val_accuracy: 0.9950\n",
            "Epoch 11/20\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.0775 - accuracy: 0.9969 - val_loss: 0.0825 - val_accuracy: 0.9960\n",
            "Epoch 12/20\n",
            "250/250 [==============================] - 2s 8ms/step - loss: 0.0635 - accuracy: 0.9971 - val_loss: 0.0689 - val_accuracy: 0.9960\n",
            "Epoch 13/20\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 0.0528 - accuracy: 0.9975 - val_loss: 0.0584 - val_accuracy: 0.9965\n",
            "Epoch 14/20\n",
            "250/250 [==============================] - 2s 6ms/step - loss: 0.0442 - accuracy: 0.9979 - val_loss: 0.0500 - val_accuracy: 0.9965\n",
            "Epoch 15/20\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 0.0376 - accuracy: 0.9980 - val_loss: 0.0431 - val_accuracy: 0.9960\n",
            "Epoch 16/20\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 0.0322 - accuracy: 0.9980 - val_loss: 0.0375 - val_accuracy: 0.9960\n",
            "Epoch 17/20\n",
            "250/250 [==============================] - 2s 6ms/step - loss: 0.0277 - accuracy: 0.9980 - val_loss: 0.0327 - val_accuracy: 0.9960\n",
            "Epoch 18/20\n",
            "250/250 [==============================] - 2s 6ms/step - loss: 0.0241 - accuracy: 0.9984 - val_loss: 0.0290 - val_accuracy: 0.9965\n",
            "Epoch 19/20\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 0.0212 - accuracy: 0.9984 - val_loss: 0.0260 - val_accuracy: 0.9955\n",
            "Epoch 20/20\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.0187 - accuracy: 0.9984 - val_loss: 0.0233 - val_accuracy: 0.9955\n"
          ]
        }
      ],
      "source": [
        "# Prepare the data\n",
        "train_x_prepared = np.array(train_x_padded)\n",
        "val_x_prepared = np.array(val_x_padded)\n",
        "\n",
        "train_y_prepared = np.array(train_y)\n",
        "val_y_prepared = np.array(val_y)\n",
        "\n",
        "print('The data is prepared for training!\\n')\n",
        "\n",
        "# Fit the model\n",
        "print('Training:')\n",
        "history = model.fit(train_x_prepared, train_y_prepared, epochs=20, validation_data=(val_x_prepared, val_y_prepared))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def plot_metrics(history, metric):\n",
        "    plt.plot(history.history[metric])\n",
        "    plt.plot(history.history[f'val_{metric}'])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(metric.title())\n",
        "    plt.legend([metric, f'val_{metric}'])\n",
        "    plt.show()\n",
        "    \n",
        "plot_metrics(history, \"accuracy\")\n",
        "plot_metrics(history, \"loss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can see that already after just a few epochs the model reached very high accuracy on both sets. But if you zoom in, you can see that the performance was still slightly improving on the training set through all 20 epochs, while it stagnated a bit earlier on the validation set. The loss on the other hand kept decreasing through all 20 epochs, which means that the model also got more confident in its predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Predict on Data\n",
        "\n",
        "Now you can use the model for predictions on unseen tweets as `model.predict()`. This is as simple as passing an array of sequences you want to predict to the mentioned method.\n",
        "In the cell below you prepare an extract of positive and negative samples from the validation set (remember, the positive examples are at the beginning and the negative are at the end) for the demonstration and predict their values with the model. Note that in the ideal case you should have another test set from which you would draw this data to inspect the model performance. But for the demonstration here the validation set will do just as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 347ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0.89997554],\n",
              "       [0.99416125],\n",
              "       [0.9971019 ],\n",
              "       [0.95091194],\n",
              "       [0.9976281 ],\n",
              "       [0.996107  ],\n",
              "       [0.99227244],\n",
              "       [0.97869337],\n",
              "       [0.99840784],\n",
              "       [0.9982677 ],\n",
              "       [0.01071535],\n",
              "       [0.04273437],\n",
              "       [0.0131921 ],\n",
              "       [0.01690092],\n",
              "       [0.01781025],\n",
              "       [0.00640504],\n",
              "       [0.01598298],\n",
              "       [0.00832003],\n",
              "       [0.02367026],\n",
              "       [0.03208218]], dtype=float32)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Prepare an example with 10 positive and 10 negative tweets.\n",
        "example_for_prediction = np.append(val_x_prepared[0:10], val_x_prepared[-10:], axis=0)\n",
        "\n",
        "# Make a prediction on the tweets.\n",
        "model.predict(example_for_prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test With Your Own Input\n",
        "\n",
        "Finally you will test with your own input. You will see that deepnets are more powerful than the older methods you have used before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_prediction_from_tweet(tweet, model, vocab, max_len):\n",
        "    tweet = process_tweet_2(tweet)\n",
        "    tweet = padded_sequence(tweet, vocab, max_len)\n",
        "    tweet = np.array([tweet])\n",
        "\n",
        "    prediction = model.predict(tweet, verbose=False)\n",
        "    \n",
        "    return prediction[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "positivety: 0.9203526973724365\n"
          ]
        }
      ],
      "source": [
        "unseen_tweet = 'Oh nice good jobbb I like what you did last week with NLp course good luck'\n",
        "\n",
        "prediction_unseen = get_prediction_from_tweet(unseen_tweet, model, vocab, max_len)\n",
        "print(f\"positivety: {prediction_unseen}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The positivety: 0.08746808767318726\n"
          ]
        }
      ],
      "source": [
        "unseen_tweet = 'No, this is bad soo bad I hate youuuuu'\n",
        "prediction_unseen = get_prediction_from_tweet(unseen_tweet, model, vocab, max_len)\n",
        "print(f\"The positivety: {prediction_unseen}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NLP with Sequencial models : NER LSM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "# set random seeds to make this notebook easier to replicate\n",
        "tf.keras.utils.set_random_seed(33)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploring the Data\n",
        "\n",
        "You will be using a dataset from Kaggle, which it will be preprocessed for you. The original data consists of four columns: the sentence number, the word, the part of speech of the word and the tags.  A few tags you might expect to see are: \n",
        "\n",
        "* geo: geographical entity\n",
        "* org: organization\n",
        "* per: person \n",
        "* gpe: geopolitical entity\n",
        "* tim: time indicator\n",
        "* art: artifact\n",
        "* eve: event\n",
        "* nat: natural phenomenon\n",
        "* O: filler word\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SENTENCE: Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
            "\n",
            "SENTENCE LABEL: O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\n",
            "\n",
            "ORIGINAL DATA:\n",
            "     Sentence #           Word  POS Tag\n",
            "0  Sentence: 1      Thousands  NNS   O\n",
            "1          NaN             of   IN   O\n",
            "2          NaN  demonstrators  NNS   O\n",
            "3          NaN           have  VBP   O\n",
            "4          NaN        marched  VBN   O\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv(r\"data\\NLP with Sequencial models  NER LSM\\ner_dataset.csv\", encoding = \"ISO-8859-1\") \n",
        "train_sents = open(r'data\\NLP with Sequencial models  NER LSM\\small\\sentences.txt', 'r').readline()\n",
        "train_labels = open(r'data\\NLP with Sequencial models  NER LSM\\small\\labels.txt', 'r').readline()\n",
        "print('SENTENCE:', train_sents)\n",
        "print('SENTENCE LABEL:', train_labels)\n",
        "print('ORIGINAL DATA:\\n', data.head())\n",
        "del(data, train_sents, train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(file_path):\n",
        "    with open(file_path,'r') as file:\n",
        "        data = np.array([line.strip() for line in file.readlines()])\n",
        "    return data\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_sentences = load_data(r'data\\NLP with Sequencial models  NER LSM\\large\\train\\sentences.txt')\n",
        "train_labels = load_data(r'data\\NLP with Sequencial models  NER LSM\\large\\train\\labels.txt')\n",
        "\n",
        "val_sentences = load_data(r'data\\NLP with Sequencial models  NER LSM\\large\\val\\sentences.txt')\n",
        "val_labels = load_data(r'data\\NLP with Sequencial models  NER LSM\\large\\val\\labels.txt')\n",
        "\n",
        "test_sentences = load_data(r'data\\NLP with Sequencial models  NER LSM\\large\\test\\sentences.txt')\n",
        "test_labels = load_data(r'data\\NLP with Sequencial models  NER LSM\\large\\test\\labels.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Encoding\n",
        "\n",
        "### Encoding the sentences\n",
        "\n",
        "In this section, you will use [`tf.keras.layers.TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) to transform the sentences into integers, so they can be fed into the model you will build later on. \n",
        "\n",
        "You can use `help(tf.keras.layers.TextVectorization)` to further investigate the object and its parameters. \n",
        "\n",
        "The parameter you will need to pass explicitly is `standardize`. This will tell how the parser splits the sentences. By default, `standardize = 'lower_and_strip_punctuation'`, this means the parser will remove all punctuation and make everything lowercase. Note that this may influence the NER task, since an upper case in the middle of a sentence may indicate an entity. Furthermore, the sentences in the dataset are already split into tokens, and all tokens, including punctuation, are separated by a whitespace. The punctuations are also labeled. That said, you will use `standardize = None` so everything will just be split into single tokens and then mapped to a positive integer.\n",
        "\n",
        "Note that `tf.keras.layers.TextVectorization` will also pad the sentences. In this case, it will always pad using the largest sentence in the set you call it with. You will be calling it for the entire training/validation/test set, but padding won't impact at all the model's output, as you will see later on.\n",
        "\n",
        "After instantiating the object, you will need to adapt it to the **sentences training set**, so it will map every token in the training set to an integer. Also, it will by default create two tokens: one for unknown tokens and another for the padding token. Tensorflow maps in the following way:\n",
        "\n",
        "1. padding token: \"\", integer mapped: 0\n",
        "1. unknown token \"[UNK]\", integer mapped: 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_sentence_vectorizer(sentences):\n",
        "    tf.keras.utils.set_random_seed(33) \n",
        "    \"\"\"\n",
        "    Create a TextVectorization layer for sentence tokenization and adapt it to the provided sentences.\n",
        "\n",
        "    Parameters:\n",
        "    sentences (list of str): Sentences for vocabulary adaptation.\n",
        "\n",
        "    Returns:\n",
        "    sentence_vectorizer (tf.keras.layers.TextVectorization): TextVectorization layer for sentence tokenization.\n",
        "    vocab (list of str): Extracted vocabulary.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define TextVectorization object with the appropriate standardize parameter\n",
        "    sentence_vectorizer = tf.keras.layers.TextVectorization(output_mode='int', standardize=None)\n",
        "    # Adapt the sentence vectorization object to the given sentences\n",
        "    sentence_vectorizer.adapt(sentences)\n",
        "    # Get the vocabulary\n",
        "    vocab = sentence_vectorizer.get_vocabulary()\n",
        "    \n",
        "    return sentence_vectorizer, vocab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test vocab size: 4650\n",
            "Sentence: I like learning new NLP models !\n",
            "Sentence vectorized: [ 296  314    1   59    1    1 4649]\n"
          ]
        }
      ],
      "source": [
        "test_vectorizer, test_vocab = get_sentence_vectorizer(train_sentences[:1000])\n",
        "print(f\"Test vocab size: {len(test_vocab)}\")\n",
        "\n",
        "sentence = \"I like learning new NLP models !\"\n",
        "sentence_vectorized = test_vectorizer(sentence)\n",
        "print(f\"Sentence: {sentence}\\nSentence vectorized: {sentence_vectorized}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentence_vectorizer, vocab = get_sentence_vectorizer(train_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoding the labels\n",
        "\n",
        "In this section you will encode the labels. The process is a bit simpler than encoding the sentences, because there are only a few tags, compared with words in the vocabulary. Note, also, that there will be one extra tag to represent the padded token that some sentences may have included. Padding will not interfere at all in this task, as you will see further on. Run the next cell to print one example of a tag related to one sentence.\n",
        "\n",
        "Because there is no meaning in having an UNK token for labels and the padding token will be another number different from 0 (you will see why soon), TextVectorization is not a good choice.\n",
        "\n",
        "You will need also to pad the labels, because the number of labels must match the number of words. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence: Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
            "Labels: O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\n"
          ]
        }
      ],
      "source": [
        "print(f\"Sentence: {train_sentences[0]}\")\n",
        "print(f\"Labels: {train_labels[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_tags(labels):\n",
        "    tag_set = set() # Define an empty set\n",
        "    for el in labels:\n",
        "        for tag in el.split(\" \"):\n",
        "            tag_set.add(tag)\n",
        "    tag_list = list(tag_set) \n",
        "    tag_list.sort()\n",
        "    return tag_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['B-art', 'B-eve', 'B-geo', 'B-gpe', 'B-nat', 'B-org', 'B-per', 'B-tim', 'I-art', 'I-eve', 'I-geo', 'I-gpe', 'I-nat', 'I-org', 'I-per', 'I-tim', 'O']\n"
          ]
        }
      ],
      "source": [
        "tags = get_tags(train_labels)\n",
        "print(tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_tag_map(tags):\n",
        "    tag_map = {}\n",
        "    for i,tag in enumerate(tags):\n",
        "        tag_map[tag] = i \n",
        "    return tag_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `tag_map` is a dictionary that maps the tags that you could have to numbers. Run the cell below to see the possible classes you will be predicting. The prepositions in the tags mean:\n",
        "* I: Token is inside an entity.\n",
        "* B: Token begins an entity.\n",
        "\n",
        "If you had the sentence \n",
        "\n",
        "**\"Sharon flew to Miami on Friday\"**\n",
        "\n",
        "The tags would look like:\n",
        "\n",
        "```\n",
        "Sharon B-per\n",
        "flew   O\n",
        "to     O\n",
        "Miami  B-geo\n",
        "on     O\n",
        "Friday B-tim\n",
        "```\n",
        "\n",
        "where you would have three tokens beginning with B-, since there are no multi-token entities in the sequence. But if you added Sharon's last name to the sentence:\n",
        "\n",
        "**\"Sharon Floyd flew to Miami on Friday\"**\n",
        "\n",
        "```\n",
        "Sharon B-per\n",
        "Floyd  I-per\n",
        "flew   O\n",
        "to     O\n",
        "Miami  B-geo\n",
        "on     O\n",
        "Friday B-tim\n",
        "```\n",
        "\n",
        "Your tags would change to show first \"Sharon\" as B-per, and \"Floyd\" as I-per, where I- indicates an inner token in a multi-token sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'B-art': 0, 'B-eve': 1, 'B-geo': 2, 'B-gpe': 3, 'B-nat': 4, 'B-org': 5, 'B-per': 6, 'B-tim': 7, 'I-art': 8, 'I-eve': 9, 'I-geo': 10, 'I-gpe': 11, 'I-nat': 12, 'I-org': 13, 'I-per': 14, 'I-tim': 15, 'O': 16}\n"
          ]
        }
      ],
      "source": [
        "tag_map = make_tag_map(tags)\n",
        "print(tag_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Padding the labels\n",
        "\n",
        "In this section, you will pad the labels. TextVectorization already padded the sentences, so you must ensure that the labels are properly padded as well. This is not a hard task for two main reasons:\n",
        "\n",
        "1. Tensorflow has built-in functions for padding\n",
        "1. Padding will be performed uniformly per dataset (train, validation and test) using the maximum sentence length in each dataset and the size of each sentence is exactly the same as the size of their respective labels.\n",
        "\n",
        "You will pad the vectorized labels with the value -1. You will not use 0 to simplify loss masking and evaluation in further steps. This is because to properly classify one token, a log softmax transformation will be performed and the index with greater value will be the index label. Since index starts at 0, it is better to keep the label 0 as a valid index, even though it is possible to also use 0 as a mask value for labels, but it would require some tweaks in the model architecture or in the loss computation.\n",
        "\n",
        "Tensorflow provides the function [`tf.keras.utils.pad_sequences`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences). The arguments you will need are:\n",
        "\n",
        "- `sequences`: An array with the labels.\n",
        "- `padding`: The position where padding will take place, the standard is `pre`, meaning the sequences will be padded at the beginning. You need to pass the argument `post`.\n",
        "- `value`: Padding value. The default value is  0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def label_vectorizer(labels, tag_map):\n",
        "    \"\"\"\n",
        "    Convert list of label strings to padded label IDs using a tag mapping.\n",
        "\n",
        "    Parameters:\n",
        "    labels (list of str): List of label strings.\n",
        "    tag_map (dict): Dictionary mapping tags to IDs.\n",
        "    Returns:\n",
        "    label_ids (numpy.ndarray): Padded array of label IDs.\n",
        "    \"\"\"\n",
        "    label_ids = [] # It can't be a numpy array yet, since each sentence has a different size\n",
        "\n",
        "    # Each element in labels is a string of tags so for each of them:\n",
        "    for element in labels:\n",
        "        # Split it into single tokens. You may use .split function for strings. Be aware to split it by a blank space!\n",
        "        tokens = element.split()\n",
        "\n",
        "        # Use the dictionary tag_map passed as an argument to the label_vectorizer function\n",
        "        # to make the correspondence between tags and numbers. \n",
        "        element_ids = [tag_map[token] for token in tokens]\n",
        "\n",
        "        # Append the found ids corresponding to the current element to label_ids list\n",
        "        label_ids.append(element_ids)\n",
        "        \n",
        "    # Pad the elements\n",
        "    label_ids = tf.keras.utils.pad_sequences(label_ids, padding='post', value=-1)\n",
        "\n",
        "\n",
        "    return label_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence: The party is divided over Britain 's participation in the Iraq conflict and the continued deployment of 8,500 British troops in that country .\n",
            "Labels: O O O O O B-gpe O O O O B-geo O O O O O O O B-gpe O O O O O\n",
            "Vectorized labels: [[16 16 16 16 16  3 16 16 16 16  2 16 16 16 16 16 16 16  3 16 16 16 16 16]]\n"
          ]
        }
      ],
      "source": [
        "print(f\"Sentence: {train_sentences[5]}\")\n",
        "print(f\"Labels: {train_labels[5]}\")\n",
        "print(f\"Vectorized labels: {label_vectorizer([train_labels[5]], tag_map)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building the Dataset\n",
        "\n",
        "In this section, you will build the dataset for training, validation and testing. You will be using [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) class, which provides an optimized way to handle data to feed into a tensorflow model. It may be not as straightforward as a pandas dataset, but it avoids keeping all the data in memory, thus it makes the training faster.\n",
        "\n",
        "You will be using the `tf.data.Dataset.from_tensor_slices` function that converts any iterable into a Tensorflow dataset. You can pass a tuple of `(sentences,labels)` and Tensorflow will understand that each sentence is mapped to its respective label, therefore it is expected that if a tuple of arrays is passed, both arrays have the same length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_dataset(sentences, labels, sentence_vectorizer, tag_map):\n",
        "    sentences_ids = sentence_vectorizer(sentences)\n",
        "    labels_ids = label_vectorizer(labels, tag_map = tag_map)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((sentences_ids, labels_ids))\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = generate_dataset(train_sentences,train_labels, sentence_vectorizer, tag_map)\n",
        "val_dataset = generate_dataset(val_sentences,val_labels,  sentence_vectorizer, tag_map)\n",
        "test_dataset = generate_dataset(test_sentences, test_labels,  sentence_vectorizer, tag_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of outputs is 17\n",
            "Num of vocabulary words in the training set: 29847\n",
            "The training size is 33570\n",
            "The validation size is 7194\n",
            "An example of the first sentence is\n",
            "\t [1046    6 1121   18 1832  232  543    7  528    2  158    5   60    9\n",
            "  648    2  922    6  192   87   22   16   54    3    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0]\n",
            "An example of its corresponding label is\n",
            "\t [16 16 16 16 16 16  2 16 16 16 16 16  2 16 16 16 16 16  3 16 16 16 16 16\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1]\n"
          ]
        }
      ],
      "source": [
        "# Exploring information about the training data\n",
        "print(f'The number of outputs is {len(tags)}')\n",
        "# The number of vocabulary tokens (including <PAD>)\n",
        "g_vocab_size = len(vocab)\n",
        "print(f\"Num of vocabulary words in the training set: {g_vocab_size}\")\n",
        "print('The training size is', len(train_dataset))\n",
        "print('The validation size is', len(val_dataset))\n",
        "print('An example of the first sentence is\\n\\t', next(iter(train_dataset))[0].numpy())\n",
        "print('An example of its corresponding label is\\n\\t', next(iter(train_dataset))[1].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Considerations about RNNs and LSTMs inputs\n",
        "\n",
        "Tensorflow implementation of RNNs (in particular LSTMs) allow you to pass a variable size of input sentences, however this cannot be done **in the same batch**. You must assure that, for each batch, the shapes for our input tensors are the same. \n",
        "\n",
        "A second point here is that, for this purpose, the size of the padding should not influence the final result. Therefore, it does not matter if you perform the padding for each batch or in the entire dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Building the Model\n",
        "\n",
        "### 4.1 Model structure\n",
        "\n",
        "\n",
        "Concretely, your inputs will be sentences represented as tensors that are fed to a model with:\n",
        "\n",
        "* An Embedding layer,\n",
        "* A LSTM layer\n",
        "* A Dense layer\n",
        "* A log softmax layer.\n",
        "\n",
        "You may choose between outputting only the very last LSTM output for each sentence, but you may also request the LSTM to output every value for a sentence - this is what you want. You will need every output, because the idea is to label every token in the sentence and not to predict the next token or even make an overall classification task for that sentence. \n",
        "\n",
        "This implies that when you input a single sentence, such as `[452, 3400, 123, 0, 0, 0]`, the expected output should be an array for each word ID, with a length equal to the number of tags. This output is obtained by applying the LogSoftfmax function for each of the `len(tags)` values. So, in the case of the example array with a shape of `(6,)`, the output should be an array with a shape of `(6, len(tags))`.\n",
        "\n",
        "In your case, you've seen that each sentence in the training set is 104 values long, so in a batch of, say, 64 tensors, the model shoud input a tensor of shape `(64,104)` and output another tensor with shape `(64,104,17)`. \n",
        "\n",
        "Good news! We won't make you implement the LSTM cell drawn above. You will be in charge of the overall architecture of the model. \n",
        "\n",
        "   \n",
        "- [tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential): Combinator that applies layers serially (by function composition) - **this is not properly a layer (it is under `tensorflow.keras` only and not under `tensorflow.keras.layers`)**. It is in fact a Tensorflow [model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) object.\n",
        "    - You can add the layers to a `Sequential` layer by calling the method `.add(layer)`.  \n",
        "    - You may skip the input shape and pass it in the first layer you instantiate, if necessary (RNNs usually don't need to fix an input length).\n",
        "\n",
        "\n",
        "-  [tf.keras.layers.Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding): Initializes the embedding layer. An embedding layer in tensorflow will input only **positive integers**.\n",
        "    - `Embedding(input_dim, output_dim, mask_zero = False)`.\n",
        "    - `input_dim` is the expected range of integers for each tensor in the batch. Note that the input_dim is not related to array size, but to the possible range of integers expected in the input. Usually this is the vocabulary size, but it may differ by 1, depending on further parameters.  See below. \n",
        "    - `output_dim` is the number of elements in the word embedding (some choices for a word embedding size range from 150 to 300, for example). Each word processed will be assigned an array of size `output_dim`. So if one array of shape (3,) is passed (example of such an array `[100,203,204]`), then the Embedding layer should have output shape (3,output_dim).\n",
        "    - `mask_zero` is a boolean telling whether 0 is a mask value or not. If `mask_zero = True`, then some considerations must be done: \n",
        "                1. The value 0 should be reserved as the mask value, as it will be ignored in training.\n",
        "                2. You need to add 1 in `input_dim`, since now Tensorflow will consider that one extra 0 value may show up in each sentence.\n",
        "\n",
        "-  [tf.keras.layers.LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM): An LSTM layer. \n",
        "    - `LSTM(units, return_sequences)` Builds an LSTM layer with hidden state and cell sizes equal to `units`. The arguments you will need: \n",
        "            1. `units`: It is the number of `LSTM` cells you will create to pass every input to. In this case, set the `units` as the Embedding `output_dim`. This is just a choice, in fact there is no static rule preventing one from choosing any amount of LSTM units. \n",
        "            2. `return_sequences`: A boolean, telling whether you want to return every output value from the LSTM cells. If `return_sequences = False`, then the LSTM output shape will be `(batch_size, units)`. Otherwise, it is `(batch_size, sentence_length, units)`, since there will be an output for each word in the sentence.\n",
        "\n",
        "\n",
        "-  [tf.keras.layers.Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense): A dense layer.\n",
        "    - `Dense(units, activation)`: The parameters for this layer are: \n",
        "            1. `units`: It is the number of units chosen for this dense layer, i.e., it is the dimensionality of the output space. In this case, each value passed through the Dense layer must be mapped into a vector with length `num_of_classes` (in this case, `len(tags)`). \n",
        "            2. `activation`: This is the activation that will be performed after computing the values in the Dense layer. Since the Dense layer comes before the LogSoftmax step, you can pass the LogSoftmax function as activation function here. **You can find the implementation for LogSoftmax under `tf.nn`. So you may call it as `tf.nn.log_softmax`. See its documentation [here](https://www.tensorflow.org/api_docs/python/tf/nn/log_softmax).\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def NER(len_tags, vocab_size, embedding_dim=50):\n",
        "    \"\"\"\n",
        "    Create a Named Entity Recognition (NER) model.\n",
        "\n",
        "    Parameters:\n",
        "    len_tags (int): Number of NER tags (output classes).\n",
        "    vocab_size (int): Vocabulary size.\n",
        "    embedding_dim (int, optional): Dimension of embedding and LSTM layers (default is 50).\n",
        "\n",
        "    Returns:\n",
        "    model (Sequential): NER model.\n",
        "    \"\"\"\n",
        "    model = tf.keras.Sequential(name='sequential') \n",
        "    # Add the tf.keras.layers.Embedding layer. Do not forget to mask out the zeros!\n",
        "    model.add(tf.keras.layers.Embedding(input_dim=vocab_size + 1, output_dim=embedding_dim, mask_zero=True))\n",
        "    # Add the LSTM layer. Make sure you are passing the right dimension (defined in the docstring above) \n",
        "    # and returning every output for the tf.keras.layers.LSTM layer and not the very last one.\n",
        "    model.add(tf.keras.layers.LSTM(units=embedding_dim, return_sequences=True))\n",
        "    # Add the final tf.keras.layers.Dense with the appropriate activation function. Remember you must pass the activation function itself ant not its call!\n",
        "    # You must use tf.nn.log_softmax instead of tf.nn.log_softmax().\n",
        "    model.add(tf.keras.layers.Dense(units=len_tags, activation=tf.nn.log_softmax))\n",
        "\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Masked loss and metrics\n",
        "\n",
        "Before training the model, you need to create your own function to compute the accuracy. Tensorflow has built-in accuracy metrics but you cannot pass values to be ignored. This will impact the calculations, since you must remove the padded values. Before diving into the exercise, let's just make some points clear.\n",
        "\n",
        "Usually, the metric that inputs true labels and predicted labels and outputs how many times the predicted and true labels match is called `accuracy`. In some cases, however, there is one more step before getting the predicted labels. This may happen if, instead of passing the predicted labels, a vector of probabilities is passed. In such case, there is a need to perform an `argmax` for each prediction to find the appropriate predicted label. Such situations happen very often, therefore Tensorflow has a set of functions, with prefix `Sparse`, that performs this operation in the backend. Unfortunately, it does not provide values to ignore in the accuracy case. This is what you will work on now. \n",
        "\n",
        "Note that the model's prediction has 3 axes: \n",
        "- the number of examples (batch size)\n",
        "- the number of words in each example (padded to be as long as the longest sentence in the batch)\n",
        "- the number of possible targets (the 17 named entity tags)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def masked_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate the masked sparse categorical cross-entropy loss.\n",
        "\n",
        "    Parameters:\n",
        "    y_true (tensor): True labels.\n",
        "    y_pred (tensor): Predicted logits.\n",
        "    \n",
        "    Returns:\n",
        "    loss (tensor): Calculated loss.\n",
        "    \"\"\"\n",
        "    # Create a mask where true labels are not equal to -1\n",
        "    mask = tf.math.not_equal(y_true, -1)\n",
        "    # Cast mask to float32\n",
        "    mask = tf.cast(mask, tf.float32)\n",
        "    # Replace padded values in y_true with a valid class label (e.g., 0)\n",
        "    y_true = tf.where(tf.equal(y_true, -1), tf.zeros_like(y_true), y_true)\n",
        "    # Calculate the loss for each item in the batch\n",
        "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
        "    # Cast the loss tensor to float32 (to match the data type of the mask)\n",
        "    loss = tf.cast(loss, tf.float32)\n",
        "    # Apply the mask to the loss\n",
        "    masked_loss = loss * mask\n",
        "    # Compute the mean loss\n",
        "    masked_loss = tf.reduce_mean(masked_loss)\n",
        "    return masked_loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(1.0508584, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "true_labels = [0,1,2,0]\n",
        "predicted_logits = [[0.1,0.6,0.3] , [0.2,0.7,0.1], [0.1, 0.5,0.4], [0.4,0.4,0.2]]\n",
        "print(masked_loss(true_labels, predicted_logits))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You will maked a masked version of the accuracy function. You will need to perform an argmax to get the predicted label for each element in the batch. Remember to provide the appropriate axis in the argmax function. Furthermore, remember to use only tensorflow operations. Even though numpy has every function you will need, to pass it as a loss function and/or metric function, you must use tensorflow operations, due to internal optimizations that Tensorflow performs for reliable fitting. The following tensorflow functions are already loaded in memory, so you can directly call them.\n",
        "\n",
        "1. tf.equal, equivalent to np.equal\n",
        "2. tf.cast, equivalent to np.astype\n",
        "3. tf.reduce_sum, equiavalent to np.sum\n",
        "4. tf.math.argmax, equivalent to np.argmax\n",
        "5. You may need tf.float32 while casting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def masked_accuracy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate masked accuracy for predicted labels.\n",
        "\n",
        "    Parameters:\n",
        "    y_true (tensor): True labels.\n",
        "    y_pred (tensor): Predicted logits.\n",
        "\n",
        "    Returns:\n",
        "    accuracy (tensor): Masked accuracy.\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    # Calculate the loss for each item in the batch.\n",
        "    # You must always cast the tensors to the same type in order to use them in training. Since you will make divisions, it is safe to use tf.float32 data type.\n",
        "    y_true = tf.cast(y_true, tf.float32) \n",
        "    # Create the mask, i.e., the values that will be ignored\n",
        "    mask = tf.cast(tf.math.not_equal(y_true, -1), tf.float32)\n",
        "    # Perform argmax to get the predicted values\n",
        "    y_pred_class = tf.cast(tf.math.argmax(y_pred, axis=-1), tf.float32)\n",
        "    # Compare the true values with the predicted ones\n",
        "    matches_true_pred = tf.equal(y_true, y_pred_class)\n",
        "    matches_true_pred = tf.cast(matches_true_pred, tf.float32)\n",
        "    # Multiply the matches tensor with the masks\n",
        "    matches_true_pred *= mask\n",
        "    # Compute masked accuracy (quotient between the total matches and the total valid values, i.e., the amount of non-masked values)\n",
        "    masked_acc = tf.reduce_sum(matches_true_pred) / tf.reduce_sum(mask)\n",
        "\n",
        "    return masked_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(0.5, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "true_labels = [0,1,2,0]\n",
        "predicted_logits = [[0.1,0.6,0.3] , [0.2,0.7,0.1], [0.1, 0.5,0.4], [0.4,0.4,0.2]]\n",
        "print(masked_accuracy(true_labels, predicted_logits))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, None, 50)          1492400   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, None, 50)          20200     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, None, 17)          867       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1513467 (5.77 MB)\n",
            "Trainable params: 1513467 (5.77 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = NER(len(tag_map), len(vocab))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A note on padding\n",
        "\n",
        "You will check now how padding does not affect the model's output. Of course the output dimension will change. If ten zeros are added at the end of the tensor, then the resulting output dimension will have 10 more elements (more specifically, 10 more arrays of length 17 each). However, those are removed from any calculation further on, so it won't impact at all the model's performance and training. You will be using the function tf.expand_dims."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = tf.expand_dims(np.array([545, 467, 896]), axis = 0) # Expanding dims is needed to pass it to the model, \n",
        "                                                        # since it expects batches and not single prediction arrays\n",
        "    \n",
        "x_padded = tf.expand_dims(np.array([545, 467, 896, 0, 0, 0]), axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x shape: (1, 3, 17)\n",
            "x_padded shape: (1, 6, 17)\n"
          ]
        }
      ],
      "source": [
        "pred_x = model(x)\n",
        "pred_x_padded = model(x_padded)\n",
        "print(f'x shape: {pred_x.shape}\\nx_padded shape: {pred_x_padded.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the last three elements of `pred_x_padded` are removed, both `pred_x` and `pred_x_padded[:3]` must have the same elements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.allclose(pred_x, pred_x[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "masked_loss is the same: False\n",
            "masked_accuracy is the same: True\n"
          ]
        }
      ],
      "source": [
        "y_true = tf.expand_dims([16, 6, 12], axis = 0)\n",
        "y_true_padded = tf.expand_dims([16,6,12,-1,-1,-1], axis = 0) # Remember you mapped the padded values to -1 in the labels\n",
        "print(f\"masked_loss is the same: {np.allclose(masked_loss(y_true,pred_x), masked_loss(y_true_padded,pred_x_padded))}\")\n",
        "print(f\"masked_accuracy is the same: {np.allclose(masked_accuracy(y_true,pred_x), masked_accuracy(y_true_padded,pred_x_padded))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.01), \n",
        "              loss = masked_loss,\n",
        "               metrics = [masked_accuracy])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training the Model\n",
        "\n",
        "You will now train the model. \n",
        "\n",
        "\n",
        "- You will train it with `shuffle = True`, over 2 epochs and passing the validation dataset as `validation_data`.\n",
        "- You will run into an error if you just pass the datasets as they are right now, because they are not prepared in batches. You must use the method `.batch` that returns a dataset already divided in batches\n",
        "\n",
        "\n",
        "**NOTE**: The fitting takes about 1 minute to run. Only the first epoch is slow, the following ones are much faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "WARNING:tensorflow:From c:\\Users\\hp\\anaconda3\\envs\\WebApp\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            "525/525 [==============================] - 112s 186ms/step - loss: 0.0559 - masked_accuracy: 0.9308 - val_loss: 0.0425 - val_masked_accuracy: 0.9575\n",
            "Epoch 2/2\n",
            "525/525 [==============================] - 104s 198ms/step - loss: 0.0231 - masked_accuracy: 0.9658 - val_loss: 0.0406 - val_masked_accuracy: 0.9594\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x1e918f702e0>"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.keras.utils.set_random_seed(33) ## Setting again a random seed to ensure reproducibility\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "model.fit(train_dataset.batch(BATCH_SIZE),\n",
        "          validation_data = val_dataset.batch(BATCH_SIZE),\n",
        "          shuffle=True,\n",
        "          epochs = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute Accuracy\n",
        "\n",
        "You will now evaluate on the test set. Previously, you have seen the accuracy on the training set and the validation (noted as eval) set. You will now evaluate on your test set. You already have a function to compute the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "225/225 [==============================] - 9s 26ms/step\n"
          ]
        }
      ],
      "source": [
        "# Convert the sentences into ids\n",
        "test_sentences_id = sentence_vectorizer(test_sentences)\n",
        "# Convert the labels into token ids\n",
        "test_labels_id = label_vectorizer(test_labels,tag_map)\n",
        "# Rename to prettify next function call\n",
        "y_true = test_labels_id \n",
        "y_pred = model.predict(test_sentences_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model's accuracy in test set is: 0.9586\n"
          ]
        }
      ],
      "source": [
        "print(f\"The model's accuracy in test set is: {masked_accuracy(y_true,y_pred).numpy():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing with your Own Sentence\n",
        "\n",
        "In this section you will make a predictor function to predict the NER labels for any sentence. \n",
        "\n",
        "You will make a function `predict` that inputs one arbitrary sentence, a trained NER model, the sentence_vectorizer and the tag mapping and return a list of predicted NER labels. Remember that the sentences in pre-processing were already separated by token, so you do not need to worry about separating tokens such as commas or dots. You will just pass one sentence in the desired format, e.g., sentence = \"I like apples , oranges and grapes .\"\n",
        "\n",
        "To get a single prediction from a tensorflow model, you will need to make some changes in the input array, since tensorflow expects a batch of sentences. You can use the function `tf.expand_dims` to do this. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(sentence, model, sentence_vectorizer, tag_map):\n",
        "    \"\"\"\n",
        "    Predict NER labels for a given sentence using a trained model.\n",
        "\n",
        "    Parameters:\n",
        "    sentence (str): Input sentence.\n",
        "    model (tf.keras.Model): Trained NER model.\n",
        "    sentence_vectorizer (tf.keras.layers.TextVectorization): Sentence vectorization layer.\n",
        "    tag_map (dict): Dictionary mapping tag IDs to labels.\n",
        "\n",
        "    Returns:\n",
        "    predictions (list): Predicted NER labels for the sentence.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert the sentence into ids\n",
        "    sentence_vectorized = sentence_vectorizer([sentence])\n",
        "    # Get the model output\n",
        "    output = model.predict(sentence_vectorized)\n",
        "    # Get the predicted labels for each token, using argmax function and specifying the correct axis to perform the argmax\n",
        "    outputs = np.argmax(output, axis=-1)\n",
        "    # Next line is just to adjust outputs dimension. Since this function expects only one input to get a prediction, outputs will be something like [[1,2,3]]\n",
        "    # so to avoid heavy notation below, let's transform it into [1,2,3]\n",
        "    outputs = outputs[0] \n",
        "    # Get a list of all keys, remember that the tag_map was built in a way that each label id matches its index in a list\n",
        "    labels = list(tag_map.keys()) \n",
        "    pred = [] \n",
        "    # Iterating over every predicted token in outputs list\n",
        "    for tag_idx in outputs:\n",
        "        pred_label = labels[tag_idx]\n",
        "        pred.append(pred_label)\n",
        "    \n",
        "    \n",
        "    return pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 7s 7s/step\n",
            "Peter B-per\n",
            "Parker I-per\n",
            "White B-org\n",
            "House I-org\n",
            "U.S B-org\n",
            "Sunday B-tim\n",
            "morning I-tim\n",
            "White B-org\n",
            "House I-org\n"
          ]
        }
      ],
      "source": [
        "# New york times news:\n",
        "sentence = \"Peter Parker , the White House director of trade and manufacturing policy of U.S , said in an interview on Sunday morning that the White House was working to prepare for the possibility of a second wave of the coronavirus in the fall , though he said it wouldn ’t necessarily come\"\n",
        "predictions = predict(sentence, model, sentence_vectorizer, tag_map)\n",
        "for x,y in zip(sentence.split(' '), predictions):\n",
        "    if y != 'O':\n",
        "        print(x,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NLP with Sequencial models : Siamese Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importing the Data\n",
        "[One Shot Learning](https://medium.com/@saba99/one-shot-learning-312e385993bc)\n",
        "\n",
        "### Loading in the data\n",
        "\n",
        "You will be using the 'Quora question answer' dataset to build a model that can identify similar questions. This is a useful task because you don't want to have several versions of the same question posted. Several times when teaching I end up responding to similar questions on piazza, or on other community forums. This data set has already been labeled for you. Run the cell below to import some of the packages you will be using. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random as rnd\n",
        "import tensorflow as tf\n",
        "\n",
        "# Set random seeds\n",
        "rnd.seed(34)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of question pairs:  404351\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>qid1</th>\n",
              "      <th>qid2</th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "      <th>is_duplicate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
              "      <td>What would happen if the Indian government sto...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>How can I increase the speed of my internet co...</td>\n",
              "      <td>How can Internet speed be increased by hacking...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
              "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
              "      <td>Which fish would survive in salt water?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  qid1  qid2                                          question1  \\\n",
              "0   0     1     2  What is the step by step guide to invest in sh...   \n",
              "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
              "2   2     5     6  How can I increase the speed of my internet co...   \n",
              "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
              "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
              "\n",
              "                                           question2  is_duplicate  \n",
              "0  What is the step by step guide to invest in sh...             0  \n",
              "1  What would happen if the Indian government sto...             0  \n",
              "2  How can Internet speed be increased by hacking...             0  \n",
              "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
              "4            Which fish would survive in salt water?             0  "
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv(r\"data\\NLP with Sequencial models  Siamese Networks\\questions.csv\")\n",
        "N = len(data)\n",
        "print('Number of question pairs: ', N)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set: 300000 Test set: 10240\n"
          ]
        }
      ],
      "source": [
        "N_train = 300000\n",
        "N_test = 10240\n",
        "data_train = data[:N_train]\n",
        "data_test = data[N_train:N_train + N_test]\n",
        "print(\"Train set:\", len(data_train), \"Test set:\", len(data_test))\n",
        "del (data)  # remove to free memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following cells are in charge of selecting only duplicate questions from the training set, which will give you a smaller dataset. First find the indexes with duplicate questions.\n",
        "\n",
        "You will start by identifying the indexes in the training set which correspond to duplicate questions. For this you will define a boolean variable `td_index`, which has value `True` if the index corresponds to duplicate questions and `False` otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of duplicate questions:  111486\n",
            "Indexes of first ten duplicate questions: [5, 7, 11, 12, 13, 15, 16, 18, 20, 29]\n"
          ]
        }
      ],
      "source": [
        "td_index = data_train['is_duplicate'] == 1\n",
        "td_index = [i for i, x in enumerate(td_index) if x]\n",
        "print('Number of duplicate questions: ', len(td_index))\n",
        "print('Indexes of first ten duplicate questions:', td_index[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?\n",
            "I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?\n",
            "is_duplicate:  1\n"
          ]
        }
      ],
      "source": [
        "print(data_train['question1'][5])\n",
        "print(data_train['question2'][5])\n",
        "print('is_duplicate: ', data_train['is_duplicate'][5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, keep only the rows in the original training set that correspond to the rows where `td_index` is `True`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "Q1_train = np.array(data_train['question1'][td_index])\n",
        "Q2_train = np.array(data_train['question2'][td_index])\n",
        "\n",
        "Q1_test = np.array(data_test['question1'])\n",
        "Q2_test = np.array(data_test['question2'])\n",
        "y_test  = np.array(data_test['is_duplicate'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAINING QUESTIONS:\n",
            "\n",
            "Question 1:  Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?\n",
            "Question 2:  I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me? \n",
            "\n",
            "Question 1:  What would a Trump presidency mean for current international master’s students on an F1 visa?\n",
            "Question 2:  How will a Trump presidency affect the students presently in US or planning to study in US? \n",
            "\n",
            "TESTING QUESTIONS:\n",
            "\n",
            "Question 1:  How do I prepare for interviews for cse?\n",
            "Question 2:  What is the best way to prepare for cse? \n",
            "\n",
            "is_duplicate = 0 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('TRAINING QUESTIONS:\\n')\n",
        "print('Question 1: ', Q1_train[0])\n",
        "print('Question 2: ', Q2_train[0], '\\n')\n",
        "print('Question 1: ', Q1_train[5])\n",
        "print('Question 2: ', Q2_train[5], '\\n')\n",
        "\n",
        "print('TESTING QUESTIONS:\\n')\n",
        "print('Question 1: ', Q1_test[0])\n",
        "print('Question 2: ', Q2_test[0], '\\n')\n",
        "print('is_duplicate =', y_test[0], '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of duplicate questions:  111486\n",
            "The length of the training set is:   89188\n",
            "The length of the validation set is:  22298\n"
          ]
        }
      ],
      "source": [
        "# Splitting the data\n",
        "cut_off = int(len(Q1_train) * 0.8)\n",
        "train_Q1, train_Q2 = Q1_train[:cut_off], Q2_train[:cut_off]\n",
        "val_Q1, val_Q2 = Q1_train[cut_off:], Q2_train[cut_off:]\n",
        "print('Number of duplicate questions: ', len(Q1_train))\n",
        "print(\"The length of the training set is:  \", len(train_Q1))\n",
        "print(\"The length of the validation set is: \", len(val_Q1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Learning question encoding\n",
        "\n",
        "The next step is to learn how to encode each of the questions as a list of numbers (integers). You will be learning how to encode each word of the selected duplicate pairs with an index. \n",
        "\n",
        "You will start by learning a word dictionary, or vocabulary, containing all the words in your training dataset, which you will use to encode each word of the selected duplicate pairs with an index. \n",
        "\n",
        "For this task you will be using the [`TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) layer from Keras. which will take care of everything for you. Begin by setting a seed, so we all get the same encoding.\n",
        "\n",
        "The vocabulary is learned using the `.adapt()`. This will analyze the dataset, determine the frequency of individual string values, and create a vocabulary from them. If you need, you can later access the vocabulary by using `.get_vocabulary()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\hp\\anaconda3\\envs\\WebApp\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\hp\\anaconda3\\envs\\WebApp\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tf.random.set_seed(0)\n",
        "text_vectorization = tf.keras.layers.TextVectorization(output_mode='int',split='whitespace', standardize='strip_punctuation')\n",
        "text_vectorization.adapt(np.concatenate((Q1_train,Q2_train)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, it is set to split text on whitespaces and it's stripping the punctuation from text. You can check how big your vocabulary is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 36224\n"
          ]
        }
      ],
      "source": [
        "print(f'Vocabulary size: {text_vectorization.vocabulary_size()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also call `text_vectorization` to see what the encoding looks like for the first questions of the training and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "first question in the train set:\n",
            "\n",
            "Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me? \n",
            "\n",
            "encoded version:\n",
            "tf.Tensor(\n",
            "[ 6984     6   178    10  8988  2442 35393   761    13  6636 28205    31\n",
            "    28   483    45    98], shape=(16,), dtype=int64) \n",
            "\n",
            "first question in the test set:\n",
            "\n",
            "How do I prepare for interviews for cse? \n",
            "\n",
            "encoded version:\n",
            "tf.Tensor([    4     8     6   160    17  2079    17 11775], shape=(8,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "print('first question in the train set:\\n')\n",
        "print(Q1_train[0], '\\n') \n",
        "print('encoded version:')\n",
        "print(text_vectorization(Q1_train[0]),'\\n')\n",
        "\n",
        "print('first question in the test set:\\n')\n",
        "print(Q1_test[0], '\\n')\n",
        "print('encoded version:')\n",
        "print(text_vectorization(Q1_test[0]) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Defining the Siamese model\n",
        "\n",
        "### Understanding the Siamese Network \n",
        "\n",
        "A Siamese network is a neural network which uses the same weights while working in tandem on two different input vectors to compute comparable output vectors. The Siamese network you are about to implement looks something like this:\n",
        "\n",
        "\n",
        "You get the question, get it vectorized and embedded, run it through an LSTM layer, normalize $v_1$ and $v_2$, and finally get the corresponding cosine similarity for each pair of questions (remember that each question is a single string). Because of the implementation of the loss function you will see in the next section, you are not going to have the cosine similarity as output of your Siamese network, but rather $v_1$ and $v_2$. You will add the cosine distance step once you reach the classification step. \n",
        "\n",
        "To train the model, you will use the triplet loss (explained below). This loss makes use of a baseline (anchor) input that is compared to a positive (truthy) input and a negative (falsy) input. The (cosine) distance from the baseline input to the positive input is minimized, and the distance from the baseline input to the negative  input is maximized. Mathematically, you are trying to maximize the following.\n",
        "\n",
        "$$\\mathcal{L}(A, P, N)=\\max \\left(\\|\\mathrm{f}(A)-\\mathrm{f}(P)\\|^{2}-\\|\\mathrm{f}(A)-\\mathrm{f}(N)\\|^{2}+\\alpha, 0\\right),$$\n",
        "\n",
        "where $A$ is the anchor input, for example $q1_1$, $P$ is the duplicate input, for example, $q2_1$, and $N$ is the negative input (the non duplicate question), for example $q2_2$.<br>\n",
        "$\\alpha$ is a margin; you can think about it as a safety net, or by how much you want to push the duplicates from the non duplicates. This is the essence of the triplet loss. However, as you will see in the next section, you will be using a pretty smart trick to improve your training, known as hard negative mining. \n",
        "<br>\n",
        "\n",
        "\n",
        "- [`tf.keras.models.Sequential`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential): groups a linear stack of layers into a tf.keras.Model.\n",
        "    - You can pass in the layers as arguments to `Serial`, separated by commas, or simply instantiate the `Sequential`model and use the `add` method to add layers.\n",
        "    - For example: `Sequential(Embeddings(...), AveragePooling1D(...), Dense(...), Softmax(...))` or \n",
        "    \n",
        "    `model = Sequential()\n",
        "     model.add(Embeddings(...))\n",
        "     model.add(AveragePooling1D(...))\n",
        "     model.add(Dense(...))\n",
        "     model.add(Softmax(...))`\n",
        "\n",
        "-  [`tf.keras.layers.Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) : Maps positive integers into vectors of fixed size. It will have shape (vocabulary length X dimension of output vectors). The dimension of output vectors (called `d_feature`in the model) is the number of elements in the word embedding. \n",
        "    - `Embedding(input_dim, output_dim)`.\n",
        "    - `input_dim` is the number of unique words in the given vocabulary.\n",
        "    - `output_dim` is the number of elements in the word embedding (some choices for a word embedding size range from 150 to 300, for example).\n",
        "    \n",
        "\n",
        "\n",
        "-  [`tf.keras.layers.LSTM`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) : The LSTM layer. The number of units should be specified and should match the number of elements in the word embedding. \n",
        "    - `LSTM(units)` Builds an LSTM layer of n_units.\n",
        "    \n",
        "    \n",
        "    \n",
        "- [`tf.keras.layers.GlobalAveragePooling1D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling1D) : Computes global average pooling, which essentially takes the mean across a desired axis. GlobalAveragePooling1D uses one tensor axis to form groups of values and replaces each group with the mean value of that group. \n",
        "    - `GlobalAveragePooling1D()` takes the mean.\n",
        "\n",
        "\n",
        "\n",
        "- [`tf.keras.layers.Lambda`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn): Layer with no weights that applies the function f, which should be specified using a lambda syntax. You will use this layer to apply normalization with the function\n",
        "    - `tfmath.l2_normalize(x)`\n",
        "\n",
        "\n",
        "\n",
        "- [`tf.keras.layers.Input`](https://www.tensorflow.org/api_docs/python/tf/keras/Input): it is used to instantiate a Keras tensor. Remember to set correctly the dimension and type of the input, which are batches of questions. For this, keep in mind that each question is a single string. \n",
        "    - `Input(input_shape,dtype=None,...)`\n",
        "    - `input_shape`: Shape tuple (not including the batch axis)\n",
        "    - `dtype`: (optional) data type of the input\n",
        "\n",
        "\n",
        "\n",
        "- [`tf.keras.layers.Concatenate`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Concatenate): Layer that concatenates a list of inputs. This layer will concatenate the normalized outputs of each LSTM into a single output for the model. \n",
        "    - `Concatenate()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: Siamese\n",
        "def Siamese(text_vectorizer, vocab_size=36224, d_feature=128):\n",
        "    \"\"\"Returns a Siamese model.\n",
        "\n",
        "    Args:\n",
        "        text_vectorizer (TextVectorization): TextVectorization instance, already adapted to your training data.\n",
        "        vocab_size (int, optional): Length of the vocabulary. Defaults to 36224, which is the vocabulary size for your case.\n",
        "        d_model (int, optional): Depth of the model. Defaults to 128.\n",
        "        d_feature : length of Embedding vectors.\n",
        "    Returns:\n",
        "        tf.model.Model: A Siamese model. \n",
        "    \n",
        "    \"\"\"\n",
        "    branch = tf.keras.models.Sequential(name='sequential') \n",
        "    # Add the text_vectorizer layer. This is the text_vectorizer you instantiated and trained before \n",
        "    branch.add(text_vectorizer)\n",
        "    # Add the Embedding layer. Remember to call it 'embedding' using the parameter `name`\n",
        "    branch.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=d_feature, name='embedding'))\n",
        "    # Add the LSTM layer, recall from W2 that you want to the LSTM layer to return sequences, ot just one value. \n",
        "    # Remember to call it 'LSTM' using the parameter `name`\n",
        "    branch.add(tf.keras.layers.LSTM(units=d_feature, return_sequences=True, name='LSTM'))\n",
        "    # Add the GlobalAveragePooling1D layer. Remember to call it 'mean' using the parameter `name`\n",
        "    branch.add(tf.keras.layers.GlobalAveragePooling1D(name='mean'))\n",
        "    # Add the normalizing layer using the Lambda function. Remember to call it 'out' using the parameter `name`\n",
        "    branch.add(tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1), name='out'))\n",
        "    \n",
        "    # Define both inputs. Remember to call then 'input_1' and 'input_2' using the `name` parameter. \n",
        "    # Be mindful of the data type and size\n",
        "    input1 = tf.keras.layers.Input(shape=(1,), dtype='string', name='input_1')\n",
        "    input2 = tf.keras.layers.Input(shape=(1,), dtype='string', name='input_2')\n",
        "    # Define the output of each branch of your Siamese network. Remember that both branches have the same coefficients, \n",
        "    # but they each receive different inputs.\n",
        "    branch1 = branch(input1)\n",
        "    branch2 = branch(input2)\n",
        "    # Define the Concatenate layer. You should concatenate columns, you can fix this using the `axis`parameter. \n",
        "    # This layer is applied over the outputs of each branch of the Siamese network\n",
        "    conc = tf.keras.layers.Concatenate(axis=1, name='conc_1_2')([branch1, branch2]) \n",
        "    \n",
        "    return tf.keras.models.Model(inputs=[input1, input2], outputs=conc, name=\"SiameseModel\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"SiameseModel\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " sequential (Sequential)     (None, 128)                  4768256   ['input_1[0][0]',             \n",
            "                                                                     'input_2[0][0]']             \n",
            "                                                                                                  \n",
            " conc_1_2 (Concatenate)      (None, 256)                  0         ['sequential[0][0]',          \n",
            "                                                                     'sequential[1][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4768256 (18.19 MB)\n",
            "Trainable params: 4768256 (18.19 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVe  (None, None)              0         \n",
            " ctorization)                                                    \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 128)         4636672   \n",
            "                                                                 \n",
            " LSTM (LSTM)                 (None, None, 128)         131584    \n",
            "                                                                 \n",
            " mean (GlobalAveragePooling  (None, 128)               0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " out (Lambda)                (None, 128)               0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4768256 (18.19 MB)\n",
            "Trainable params: 4768256 (18.19 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# check your model\n",
        "model = Siamese(text_vectorization, vocab_size=text_vectorization.vocabulary_size())\n",
        "model.build(input_shape=None)\n",
        "model.summary()\n",
        "model.get_layer(name='sequential').summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Hard Negative Mining\n",
        "\n",
        "\n",
        "You will now implement the `TripletLoss` with hard negative mining.<br>\n",
        "As explained in the lecture, you will be using all the questions from each batch to compute this loss. Positive examples are questions $q1_i$, and $q2_i$, while all the other combinations $q1_i$, $q2_j$ ($i\\neq j$), are considered negative examples. The loss will be composed of two terms. One term utilizes the mean of all the non duplicates, the second utilizes the *closest negative*. Our loss expression is then:\n",
        " \n",
        "\\begin{align}\n",
        " \\mathcal{Loss_1(A,P,N)} &=\\max \\left( -cos(A,P)  + mean_{neg} +\\alpha, 0\\right) \\\\\n",
        " \\mathcal{Loss_2(A,P,N)} &=\\max \\left( -cos(A,P)  + closest_{neg} +\\alpha, 0\\right) \\\\\n",
        "\\mathcal{Loss(A,P,N)} &= mean(Loss_1 + Loss_2) \\\\\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "Further, two sets of instructions are provided. The first set, found just below, provides a brief description of the task. If that set proves insufficient, a more detailed set can be displayed.  \n",
        "\n",
        "\n",
        "Here is a list of things you should do: <br>\n",
        "\n",
        "- As this will be run inside Tensorflow, use all operation supplied by `tf.math` or `tf.linalg`, instead of `numpy` functions. You will also need to explicitly use `tf.shape` to get the batch size from the inputs. This is to make it compatible with the Tensor inputs it will receive when doing actual training and testing. \n",
        "- Use [`tf.linalg.matmul`](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul) to calculate the similarity matrix $v_2v_1^T$ of dimension `batch_size` x `batch_size`. \n",
        "- Take the score of the duplicates on the diagonal with [`tf.linalg.diag_part`](https://www.tensorflow.org/api_docs/python/tf/linalg/diag_part). \n",
        "- Use the `TensorFlow` functions [`tf.eye`](https://www.tensorflow.org/api_docs/python/tf/eye) and [`tf.math.reduce_max`](https://www.tensorflow.org/api_docs/python/tf/math/reduce_max) for the identity matrix and the maximum respectively. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: TripletLossFn\n",
        "def TripletLossFn(v1, v2, margin=0.25):\n",
        "    \"\"\"Custom Loss function.\n",
        "\n",
        "    Args:\n",
        "        v1 (numpy.ndarray or Tensor): Array with dimension (batch_size, model_dimension) associated to Q1.\n",
        "        v2 (numpy.ndarray or Tensor): Array with dimension (batch_size, model_dimension) associated to Q2.\n",
        "        margin (float, optional): Desired margin. Defaults to 0.25.\n",
        "\n",
        "    Returns:\n",
        "        triplet_loss (numpy.ndarray or Tensor)\n",
        "    \"\"\"\n",
        "\n",
        "    # use `tf.linalg.matmul` to take the dot product of the two batches. \n",
        "    # Don't forget to transpose the second argument using `transpose_b=True`\n",
        "    scores = tf.linalg.matmul(v2, v1, transpose_b=True)\n",
        "    # calculate new batch size and cast it as the same datatype as scores. \n",
        "    batch_size = tf.cast(tf.shape(v1)[0], scores.dtype) \n",
        "    # use `tf.linalg.diag_part` to grab the cosine similarity of all positive examples\n",
        "    positive = tf.linalg.diag_part(scores)\n",
        "    # subtract the diagonal from scores. You can do this by creating a diagonal matrix with the values \n",
        "    # of all positive examples using `tf.linalg.diag`\n",
        "    negative_zero_on_duplicate = scores - tf.linalg.diag(positive)\n",
        "    # use `tf.math.reduce_sum` on `negative_zero_on_duplicate` for `axis=1` and divide it by `(batch_size - 1)`\n",
        "    mean_negative = tf.math.reduce_sum(negative_zero_on_duplicate, axis=1) / (batch_size - 1)\n",
        "    # create a composition of two masks: \n",
        "    # the first mask to extract the diagonal elements (make sure you use the variable batch_size here), \n",
        "    # the second mask to extract elements in the negative_zero_on_duplicate matrix that are larger than the elements in the diagonal \n",
        "    mask_exclude_positives = tf.cast((tf.eye(batch_size, dtype=tf.bool)) | (negative_zero_on_duplicate > tf.expand_dims(positive, axis=1)),\n",
        "                                    scores.dtype)\n",
        "    # multiply `mask_exclude_positives` with 2.0 and subtract it out of `negative_zero_on_duplicate`\n",
        "    negative_without_positive = negative_zero_on_duplicate - mask_exclude_positives * 2.0\n",
        "    # take the row by row `max` of `negative_without_positive`. \n",
        "    # Hint: `tf.math.reduce_max(negative_without_positive, axis = 1)`\n",
        "    closest_negative = tf.math.reduce_max(negative_without_positive, axis=1)\n",
        "    # compute `tf.maximum` among 0.0 and `A`\n",
        "    # A = subtract `positive` from `margin` and add `closest_negative` \n",
        "    triplet_loss1 = tf.maximum(0.0, margin - positive + closest_negative)\n",
        "    # compute `tf.maximum` among 0.0 and `B`\n",
        "    # B = subtract `positive` from `margin` and add `mean_negative` \n",
        "    triplet_loss2 = tf.maximum(0.0, margin - positive + mean_negative)\n",
        "    # add the two losses together and take the `tf.math.reduce_sum` of it\n",
        "    triplet_loss = tf.reduce_sum(triplet_loss1 + triplet_loss2)\n",
        "\n",
        "\n",
        "    return triplet_loss\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Triplet Loss: 0.7035076825158911\n"
          ]
        }
      ],
      "source": [
        "v1 = np.array([[0.26726124, 0.53452248, 0.80178373],[0.5178918 , 0.57543534, 0.63297887]])\n",
        "v2 = np.array([[ 0.26726124,  0.53452248,  0.80178373],[-0.5178918 , -0.57543534, -0.63297887]])\n",
        "print(\"Triplet Loss:\", TripletLossFn(v1,v2).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To recognize it as a loss function, keras needs it to have two inputs: true labels, and output labels. You will not be using the true labels, but you still need to pass some dummy variable with size `(batch_size,)` for TensorFlow to accept it as a valid loss.\n",
        "\n",
        "Additionally, the `out` parameter must coincide with the output of your Siamese network, which is the concatenation of the processing of each of the inputs, so you need to extract $v_1$ and $v_2$ from there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def TripletLoss(labels, out, margin=0.25):\n",
        "    _, out_size = out.shape # get embedding size\n",
        "    v1 = out[:,:int(out_size/2)] # Extract v1 from out\n",
        "    v2 = out[:,int(out_size/2):] # Extract v2 from out\n",
        "    return TripletLossFn(v1, v2, margin=margin)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n",
        "\n",
        "Now it's time to finally train your model. As usual, you have to define the cost function and the optimizer. You also have to build the actual model you will be training. \n",
        "\n",
        "To pass the input questions for training and validation you will use the iterator produced by [`tensorflow.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset). Run the next cell to create your train and validation datasets. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(((train_Q1, train_Q2),tf.constant([1]*len(train_Q1))))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(((val_Q1, val_Q2),tf.constant([1]*len(val_Q1))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: train_model\n",
        "def train_model(Siamese, TripletLoss, text_vectorizer, train_dataset, val_dataset, d_feature=128, lr=0.01, train_steps=5):\n",
        "    \"\"\"Training the Siamese Model\n",
        "\n",
        "    Args:\n",
        "        Siamese (function): Function that returns the Siamese model.\n",
        "        TripletLoss (function): Function that defines the TripletLoss loss function.\n",
        "        text_vectorizer: trained instance of `TextVecotrization` \n",
        "        train_dataset (tf.data.Dataset): Training dataset\n",
        "        val_dataset (tf.data.Dataset): Validation dataset\n",
        "        d_feature (int, optional) = size of the encoding. Defaults to 128.\n",
        "        lr (float, optional): learning rate for optimizer. Defaults to 0.01\n",
        "        train_steps (int): number of epochs\n",
        "        \n",
        "    Returns:\n",
        "        tf.keras.Model\n",
        "    \"\"\"\n",
        "    # Instantiate your Siamese model\n",
        "    model = Siamese(text_vectorizer,\n",
        "                    vocab_size = text_vectorizer.vocabulary_size(), \n",
        "                    d_feature = d_feature)\n",
        "    # Compile the model\n",
        "    model.compile(loss=TripletLoss,\n",
        "                  optimizer = tf.keras.optimizers.Adam(lr)\n",
        "            )\n",
        "    # Train the model \n",
        "    model.fit(train_dataset,\n",
        "              epochs = train_steps,\n",
        "              validation_data = val_dataset,\n",
        "             )\n",
        "             \n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now call the `train_model` function. You will be using a batch size of 256. \n",
        "\n",
        "To create the data generators you will be using the method `batch` for `Dataset` object. You will also call the `shuffle` method, to shuffle the dataset on each iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "349/349 [==============================] - 170s 452ms/step - loss: 29.2656 - val_loss: 12.2289\n",
            "Epoch 2/2\n",
            "349/349 [==============================] - 110s 315ms/step - loss: 8.5262 - val_loss: 9.5722\n"
          ]
        }
      ],
      "source": [
        "train_steps = 2\n",
        "batch_size = 256\n",
        "train_generator = train_dataset.shuffle(len(train_Q1),\n",
        "                                        seed=7, \n",
        "                                        reshuffle_each_iteration=True).batch(batch_size=batch_size)\n",
        "val_generator = val_dataset.shuffle(len(val_Q1), \n",
        "                                   seed=7,\n",
        "                                   reshuffle_each_iteration=True).batch(batch_size=batch_size)\n",
        "model = train_model(Siamese, TripletLoss,text_vectorization, \n",
        "                                            train_generator, \n",
        "                                            val_generator, \n",
        "                                            train_steps=train_steps,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model.save(\"trained_model.keras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation  \n",
        "\n",
        "### Evaluating your siamese network\n",
        "\n",
        "In this section you will learn how to evaluate a Siamese network. You will start by loading a pretrained model, and then you will use it to predict. For the prediction you will need to take the output of your model and compute the cosine loss between each pair of questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"SiameseModel\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " sequential (Sequential)     (None, 128)                  4768256   ['input_1[0][0]',             \n",
            "                                                                     'input_2[0][0]']             \n",
            "                                                                                                  \n",
            " conc_1_2 (Concatenate)      (None, 256)                  0         ['sequential[0][0]',          \n",
            "                                                                     'sequential[1][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4768256 (18.19 MB)\n",
            "Trainable params: 4768256 (18.19 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.models.load_model(r'data\\NLP with Sequencial models  Siamese Networks\\trained_model.keras', safe_mode=False, compile=False)\n",
        "\n",
        "# Show the model architecture\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Classify\n",
        "\n",
        "To determine the accuracy of the model, you will use the test set that was configured earlier. While in training you used only positive examples, the test data, `Q1_test`, `Q2_test` and `y_test`, is set up as pairs of questions, some of which are duplicates and some are not. \n",
        "This routine will run all the test question pairs through the model, compute the cosine similarity of each pair, threshold it and compare the result to `y_test` - the correct response from the data set. The results are accumulated to produce an accuracy; the confusion matrix is also computed to have a better understanding of the errors.\n",
        "\n",
        "\n",
        "\n",
        " - Use a `tensorflow.data.Dataset` to go through the data in chunks with size batch_size. This time you don't need the labels, so you can just replace them by `None`,\n",
        " - use `predict` on the chunks of data.\n",
        " - compute `v1`, `v2` using the model output,\n",
        " - for each element of the batch\n",
        "        - compute the cosine similarity of each pair of entries, `v1[j]`,`v2[j]`\n",
        "        - determine if `d > threshold`\n",
        "        - increment accuracy if that result matches the expected results (`y_test[j]`)\n",
        "  \n",
        "   Instead of running a for loop, you will vectorize all these operations to make things more efficient,\n",
        " - compute the final accuracy and confusion matrix and return. For the confusion matrix you can use the [`tf.math.confusion_matrix`](https://www.tensorflow.org/api_docs/python/tf/math/confusion_matrix) function. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def classify(test_Q1, test_Q2, y_test, threshold, model, batch_size=64, verbose=True):\n",
        "    \"\"\"Function to test the accuracy of the model.\n",
        "\n",
        "    Args:\n",
        "        test_Q1 (numpy.ndarray): Array of Q1 questions. Each element of the array would be a string.\n",
        "        test_Q2 (numpy.ndarray): Array of Q2 questions. Each element of the array would be a string.\n",
        "        y_test (numpy.ndarray): Array of actual target.\n",
        "        threshold (float): Desired threshold\n",
        "        model (tensorflow.Keras.Model): The Siamese model.\n",
        "        batch_size (int, optional): Size of the batches. Defaults to 64.\n",
        "\n",
        "    Returns:\n",
        "        float: Accuracy of the model\n",
        "        numpy.array: confusion matrix\n",
        "    \"\"\"\n",
        "    y_pred = []\n",
        "    test_gen = tf.data.Dataset.from_tensor_slices(((test_Q1, test_Q2), None)).batch(batch_size=batch_size)\n",
        "    pred = model.predict(test_gen)\n",
        "    _, n_feat = pred.shape\n",
        "    v1 = pred[:, :n_feat//2]\n",
        "    v2 = pred[:, n_feat//2:]\n",
        "    \n",
        "    # Compute the cosine similarity. Using `tf.math.reduce_sum`. \n",
        "    # Don't forget to use the appropriate axis argument.\n",
        "    d = tf.math.reduce_sum(tf.multiply(v1, v2), axis=1)\n",
        "    # Check if d>threshold to make predictions\n",
        "    y_pred = tf.cast(d > threshold, tf.float64)\n",
        "    # take the average of correct predictions to get the accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred, y_test), tf.float64))\n",
        "    # compute the confusion matrix using `tf.math.confusion_matrix`\n",
        "    cm = tf.math.confusion_matrix(y_test, y_pred)\n",
        "    return accuracy, cm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20/20 [==============================] - 15s 591ms/step\n",
            "Accuracy 0.72216796875\n",
            "Confusion matrix:\n",
            "[[4394 1988]\n",
            " [ 857 3001]]\n"
          ]
        }
      ],
      "source": [
        "# this takes around 1 minute\n",
        "accuracy, cm = classify(Q1_test,Q2_test, y_test, 0.7, model,  batch_size = 512) \n",
        "print(\"Accuracy\", accuracy.numpy())\n",
        "print(f\"Confusion matrix:\\n{cm.numpy()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing with your own questions\n",
        "\n",
        "In this final section you will test the model with your own questions. You will write a function `predict` which takes two questions as input and returns `True` or `False` depending on whether the question pair is a duplicate or not.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(question1, question2, threshold, model, verbose=False):\n",
        "    \"\"\"Function for predicting if two questions are duplicates.\n",
        "\n",
        "    Args:\n",
        "        question1 (str): First question.\n",
        "        question2 (str): Second question.\n",
        "        threshold (float): Desired Similarity threshold.\n",
        "        model (tensorflow.keras.Model): The Siamese model.\n",
        "        verbose (bool, optional): If the results should be printed out. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the questions are duplicates, False otherwise.\n",
        "    \"\"\"\n",
        "    generator = tf.data.Dataset.from_tensor_slices((([question1], [question2]), None)).batch(batch_size=1)\n",
        "\n",
        "    # Call the predict method of your model and save the output into v1v2\n",
        "    v1v2 = model.predict(generator)\n",
        "    d_feature = v1v2.shape[1] // 2  # Calculate the dimensionality of the output vectors\n",
        "    # Extract v1 and v2 from the model output\n",
        "    v1 = v1v2[:, :d_feature]\n",
        "    v2 = v1v2[:, d_feature:]\n",
        "    # Take the dot product to compute cos similarity of each pair of entries, v1, v2\n",
        "    # Since v1 and v2 are both vectors, use the function tf.math.reduce_sum instead of tf.linalg.matmul\n",
        "    d = tf.math.reduce_sum(tf.multiply(v1, v2), axis=1)\n",
        "    # Is d greater than the threshold?\n",
        "    result = d > threshold\n",
        "    \n",
        "    if(verbose):\n",
        "        print(\"Q1  = \", question1, \"\\nQ2  = \", question2)\n",
        "        print(\"Similarity   = \", d.numpy()[0])\n",
        "        print(\"Duplicates ? :\", result.numpy()[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feel free to try with your own questions\n",
        "question1 = \"When will I see you?\"\n",
        "question2 = \"When can I see you again?\"\n",
        "# 1 means it is duplicated, 0 otherwise\n",
        "predict(question1 , question2, 0.7, model, verbose = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NLP with Attention models : Translation Machine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to\n",
            "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Setting this env variable prevents TF warnings from showing up\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from collections import Counter\n",
        "from utils import (sentences, train_data, val_data, english_vectorizer, portuguese_vectorizer, \n",
        "                   masked_loss, masked_acc, tokens_to_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preparation\n",
        "\n",
        "The text pre-processing bits have already been taken care of (if you are interested in this be sure to check the `utils.py` file). The steps performed can be summarized as:\n",
        "\n",
        "- Reading the raw data from the text files\n",
        "- Cleaning the data (using lowercase, adding space around punctuation, trimming whitespaces, etc)\n",
        "- Splitting it into training and validation sets\n",
        "- Adding the start-of-sentence and end-of-sentence tokens to every sentence\n",
        "- Tokenizing the sentences\n",
        "- Creating a Tensorflow dataset out of the tokenized sentences\n",
        "\n",
        "Take a moment to inspect the raw sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English (to translate) sentence:\n",
            "\n",
            "No matter how much you try to convince people that chocolate is vanilla, it'll still be chocolate, even though you may manage to convince yourself and a few others that it's vanilla.\n",
            "\n",
            "Portuguese (translation) sentence:\n",
            "\n",
            "Não importa o quanto você tenta convencer os outros de que chocolate é baunilha, ele ainda será chocolate, mesmo que você possa convencer a si mesmo e poucos outros de que é baunilha.\n"
          ]
        }
      ],
      "source": [
        "portuguese_sentences, english_sentences = sentences\n",
        "\n",
        "print(f\"English (to translate) sentence:\\n\\n{english_sentences[-5]}\\n\")\n",
        "print(f\"Portuguese (translation) sentence:\\n\\n{portuguese_sentences[-5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You don't have much use for the raw sentences so delete them to save memory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "del portuguese_sentences\n",
        "del english_sentences\n",
        "del sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that you imported an `english_vectorizer` and a `portuguese_vectorizer` from `utils.py`. These were created using [tf.keras.layers.TextVectorization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) and they provide interesting features such as ways to visualize the vocabulary and convert text into tokenized ids and vice versa. In fact, you can inspect the first ten words of the vocabularies for both languages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 10 words of the english vocabulary:\n",
            "\n",
            "['', '[UNK]', '[SOS]', '[EOS]', '.', 'tom', 'i', 'to', 'you', 'the']\n",
            "\n",
            "First 10 words of the portuguese vocabulary:\n",
            "\n",
            "['', '[UNK]', '[SOS]', '[EOS]', '.', 'tom', 'que', 'o', 'nao', 'eu']\n"
          ]
        }
      ],
      "source": [
        "print(f\"First 10 words of the english vocabulary:\\n\\n{english_vectorizer.get_vocabulary()[:10]}\\n\")\n",
        "print(f\"First 10 words of the portuguese vocabulary:\\n\\n{portuguese_vectorizer.get_vocabulary()[:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that the first 4 words are reserved for special words. In order, these are:\n",
        "\n",
        "- the empty string\n",
        "- a special token to represent an unknown word\n",
        "- a special token to represent the start of a sentence\n",
        "- a special token to represent the end of a sentence\n",
        "\n",
        "You can see how many words are in a vocabulary by using the `vocabulary_size` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Portuguese vocabulary is made up of 12000 words\n",
            "English vocabulary is made up of 12000 words\n"
          ]
        }
      ],
      "source": [
        "# Size of the vocabulary\n",
        "vocab_size_por = portuguese_vectorizer.vocabulary_size()\n",
        "vocab_size_eng = english_vectorizer.vocabulary_size()\n",
        "\n",
        "print(f\"Portuguese vocabulary is made up of {vocab_size_por} words\")\n",
        "print(f\"English vocabulary is made up of {vocab_size_eng} words\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can define [tf.keras.layers.StringLookup](https://www.tensorflow.org/api_docs/python/tf/keras/layers/StringLookup) objects that will help you map from words to ids and vice versa. Do this for the portuguese vocabulary since this will be useful later on when you decode the predictions from your model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This helps you convert from words to ids\n",
        "word_to_id = tf.keras.layers.StringLookup(\n",
        "    vocabulary=portuguese_vectorizer.get_vocabulary(), \n",
        "    mask_token=\"\", \n",
        "    oov_token=\"[UNK]\"\n",
        ")\n",
        "\n",
        "# This helps you convert from ids to words\n",
        "id_to_word = tf.keras.layers.StringLookup(\n",
        "    vocabulary=portuguese_vectorizer.get_vocabulary(),\n",
        "    mask_token=\"\",\n",
        "    oov_token=\"[UNK]\",\n",
        "    invert=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Try it out for the special tokens and a random word:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The id for the [UNK] token is 1\n",
            "The id for the [SOS] token is 2\n",
            "The id for the [EOS] token is 3\n",
            "The id for baunilha (vanilla) is 7079\n"
          ]
        }
      ],
      "source": [
        "unk_id = word_to_id(\"[UNK]\")\n",
        "sos_id = word_to_id(\"[SOS]\")\n",
        "eos_id = word_to_id(\"[EOS]\")\n",
        "baunilha_id = word_to_id(\"baunilha\")\n",
        "\n",
        "print(f\"The id for the [UNK] token is {unk_id}\")\n",
        "print(f\"The id for the [SOS] token is {sos_id}\")\n",
        "print(f\"The id for the [EOS] token is {eos_id}\")\n",
        "print(f\"The id for baunilha (vanilla) is {baunilha_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally take a look at how the data that is going to be fed to the neural network looks like. Both `train_data` and `val_data` are of type `tf.data.Dataset` and are already arranged in batches of 64 examples. To get the first batch out of a tf dataset you can use the `take` method. To get the first example out of the batch you can slice the tensor and use the `numpy` method for nicer printing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized english sentence:\n",
            "[   2  210    9  146  123   38    9 1672    4    3    0    0    0    0]\n",
            "\n",
            "\n",
            "Tokenized portuguese sentence (shifted to the right):\n",
            "[   2 1085    7  128   11  389   37 2038    4    0    0    0    0    0\n",
            "    0]\n",
            "\n",
            "\n",
            "Tokenized portuguese sentence:\n",
            "[1085    7  128   11  389   37 2038    4    3    0    0    0    0    0\n",
            "    0]\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for (to_translate, sr_translation), translation in train_data.take(1):\n",
        "    print(f\"Tokenized english sentence:\\n{to_translate[0, :].numpy()}\\n\\n\")\n",
        "    print(f\"Tokenized portuguese sentence (shifted to the right):\\n{sr_translation[0, :].numpy()}\\n\\n\")\n",
        "    print(f\"Tokenized portuguese sentence:\\n{translation[0, :].numpy()}\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are a couple of important details to notice.\n",
        "\n",
        "- Padding has already been applied to the tensors and the value used for this is 0\n",
        "- Each example consists of 3 different tensors:\n",
        "    - The sentence to translate\n",
        "    - The shifted-to-the-right translation\n",
        "    - The translation\n",
        "    \n",
        "The first two can be considered as the features, while the third one as the target. By doing this your model can perform Teacher Forcing as you saw in the lectures.\n",
        "\n",
        "Now it is time to begin coding!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NMT model with attention\n",
        "\n",
        "The model you will build uses an encoder-decoder architecture. This Recurrent Neural Network (RNN) takes in a tokenized version of a sentence in its encoder, then passes it on to the decoder for translation. As mentioned in the lectures, just using a a regular sequence-to-sequence model with LSTMs will work effectively for short to medium sentences but will start to degrade for longer ones. You can picture it like the figure below where all of the context of the input sentence is compressed into one vector that is passed into the decoder block. You can see how this will be an issue for very long sentences (e.g. 100 tokens or more) because the context of the first parts of the input will have very little effect on the final vector passed to the decoder.\n",
        "\n",
        "\n",
        "Adding an attention layer to this model avoids this problem by giving the decoder access to all parts of the input sentence. To illustrate, let's just use a 4-word input sentence as shown below. Remember that a hidden state is produced at each timestep of the encoder (represented by the orange rectangles). These are all passed to the attention layer and each are given a score given the current activation (i.e. hidden state) of the decoder. For instance, let's consider the figure below where the first prediction \"como\" is already made. To produce the next prediction, the attention layer will first receive all the encoder hidden states (i.e. orange rectangles) as well as the decoder hidden state when producing the word \"como\" (i.e. first green rectangle). Given this information, it will score each of the encoder hidden states to know which one the decoder should focus on to produce the next word. As a result of training, the model might have learned that it should align to the second encoder hidden state and subsequently assigns a high probability to the word \"você\". If we are using greedy decoding, we will output the said word as the next symbol, then restart the process to produce the next word until we reach an end-of-sentence prediction.\n",
        "\n",
        "\n",
        "There are different ways to implement attention and the one we'll use for this assignment is the Scaled Dot Product Attention which has the form:\n",
        "\n",
        "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
        "\n",
        "You will dive deeper into this equation in the next week but for now, you can think of it as computing scores using queries (Q) and keys (K), followed by a multiplication of values (V) to get a context vector at a particular timestep of the decoder. This context vector is fed to the decoder RNN to get a set of probabilities for the next predicted word. The division by square root of the keys dimensionality ($\\sqrt{d_k}$) is for improving model performance and you'll also learn more about it next week. For our machine translation application, the encoder activations (i.e. encoder hidden states) will be the keys and values, while the decoder activations (i.e. decoder hidden states) will be the queries.\n",
        "\n",
        "You will see in the upcoming sections that this complex architecture and mechanism can be implemented with just a few lines of code. \n",
        "\n",
        "First you will define two important global variables:\n",
        "\n",
        "- The size of the vocabulary\n",
        "- The number of units in the LSTM layers (the same number will be used for all LSTM layers)\n",
        "\n",
        "In this assignment, the vocabulary sizes for English and Portuguese are the same. Therefore, we use a single constant VOCAB_SIZE throughout the notebook. While in other settings, vocabulary sizes could differ, that is not the case in our assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 12000\n",
        "UNITS = 256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encoder\n",
        "\n",
        "Your first exercise is to code the encoder part of the neural network. For this, complete the `Encoder` class below. Notice that in the constructor (the `__init__` method) you need to define all of the sublayers of the encoder and then use these sublayers during the forward pass (the `call` method).\n",
        "\n",
        "The encoder consists of the following layers:\n",
        "\n",
        "- [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding). For this layer you need to define the appropriate `input_dim` and `output_dim` and let it know that you are using '0' as padding, which can be done by using the appropriate value for the `mask_zero` parameter.\n",
        "    \n",
        "+ [Bidirectional](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional) [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM). In TF you can implement bidirectional behaviour for RNN-like layers. This part is already taken care of but you will need to specify the appropriate type of layer as well as its parameters. In particular you need to set the appropriate number of units and make sure that the LSTM returns the full sequence and not only the last output, which can be done by using the appropriate value for the `return_sequences` parameter.\n",
        "\n",
        "\n",
        "You need to define the forward pass using the syntax of TF's [functional API](https://www.tensorflow.org/guide/keras/functional_api). What this means is that you chain function calls together to define your network like this:\n",
        "\n",
        "```python\n",
        "encoder_input = keras.Input(shape=(28, 28, 1), name=\"original_img\")\n",
        "x = layers.Conv2D(16, 3, activation=\"relu\")(encoder_input)\n",
        "x = layers.MaxPooling2D(3)(x)\n",
        "x = layers.Conv2D(16, 3, activation=\"relu\")(x)\n",
        "encoder_output = layers.GlobalMaxPooling2D()(x)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, units):\n",
        "        \"\"\"Initializes an instance of this class\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the vocabulary\n",
        "            units (int): Number of units in the LSTM layer\n",
        "        \"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(  \n",
        "            input_dim=vocab_size,\n",
        "            output_dim=units,\n",
        "            mask_zero=True\n",
        "        )  \n",
        "\n",
        "        self.rnn = tf.keras.layers.Bidirectional(  \n",
        "            layer=tf.keras.layers.LSTM(\n",
        "                units=units,\n",
        "                return_sequences=True\n",
        "            ),  \n",
        "            merge_mode=\"sum\"\n",
        "        )  \n",
        "\n",
        "    def call(self, context):\n",
        "        \"\"\"Forward pass of this layer\n",
        "\n",
        "        Args:\n",
        "            context (tf.Tensor): The sentence to translate\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: Encoded sentence to translate\n",
        "        \"\"\"\n",
        "        # Pass the context through the embedding layer\n",
        "        x = self.embedding(context)\n",
        "\n",
        "        # Pass the output of the embedding through the RNN\n",
        "        x = self.rnn(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor of sentences in english has shape: (64, 14)\n",
            "\n",
            "Encoder output has shape: (64, 14, 256)\n"
          ]
        }
      ],
      "source": [
        "# Do a quick check of your implementation\n",
        "\n",
        "# Create an instance of your class\n",
        "encoder = Encoder(VOCAB_SIZE, UNITS)\n",
        "\n",
        "# Pass a batch of sentences to translate from english to portuguese\n",
        "encoder_output = encoder(to_translate)\n",
        "\n",
        "print(f'Tensor of sentences in english has shape: {to_translate.shape}\\n')\n",
        "print(f'Encoder output has shape: {encoder_output.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CrossAttention\n",
        "\n",
        "Your next exercise is to code the layer that will perform cross attention between the original sentences and the translations. For this, complete the `CrossAttention` class below. Notice that in the constructor (the `__init__` method) you need to define all of the sublayers and then use these sublayers during the forward pass (the `call` method). For this particular case some of these bits are already taken care of.\n",
        "\n",
        "The cross attention consists of the following layers:\n",
        "\n",
        "- [MultiHeadAttention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention). For this layer you need to define the appropriate `key_dim`, which is the size of the key and query tensors. You will also need to set the number of heads to 1 since you aren't implementing multi head attention but attention between two tensors. The reason why this layer is preferred over [Attention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention) is that it allows simpler code during the forward pass.\n",
        "    \n",
        "A couple of things to notice:\n",
        "- You need a way to pass both the output of the attention alongside the shifted-to-the-right translation (since this cross attention happens in the decoder side). For this you will use an [Add](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Add) layer so that the original dimension is preserved, which would not happen if you use something like a [Concatenate](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Concatenate) layer.\n",
        "\n",
        "+ Layer normalization is also performed for better stability of the network by using a [LayerNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization) layer.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        \"\"\"Initializes an instance of this class\n",
        "\n",
        "        Args:\n",
        "            units (int): Number of units in the LSTM layer\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "\n",
        "        self.mha = (\n",
        "            tf.keras.layers.MultiHeadAttention(\n",
        "                key_dim=units,\n",
        "                num_heads=1\n",
        "            )\n",
        "        )\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "        self.add = tf.keras.layers.Add()\n",
        "\n",
        "    def call(self, context, target):\n",
        "        \"\"\"Forward pass of this layer\n",
        "\n",
        "        Args:\n",
        "            context (tf.Tensor): Encoded sentence to translate\n",
        "            target (tf.Tensor): The embedded shifted-to-the-right translation\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: Cross attention between context and target\n",
        "        \"\"\"\n",
        "        ### START CODE HERE ###\n",
        "\n",
        "        # Call the MH attention by passing in the query and value\n",
        "        # For this case the query should be the translation and the value the encoded sentence to translate\n",
        "        # Hint: Check the call arguments of MultiHeadAttention in the docs\n",
        "        attn_output = self.mha(\n",
        "            query=target,\n",
        "            value=context\n",
        "        )\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        x = self.add([target, attn_output])\n",
        "\n",
        "        x = self.layernorm(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor of contexts has shape: (64, 14, 256)\n",
            "Tensor of translations has shape: (64, 15, 256)\n",
            "Tensor of attention scores has shape: (64, 15, 256)\n"
          ]
        }
      ],
      "source": [
        "# Do a quick check of your implementation\n",
        "\n",
        "# Create an instance of your class\n",
        "attention_layer = CrossAttention(UNITS)\n",
        "\n",
        "# The attention layer expects the embedded sr-translation and the context\n",
        "# The context (encoder_output) is already embedded so you need to do this for sr_translation:\n",
        "sr_translation_embed = tf.keras.layers.Embedding(VOCAB_SIZE, output_dim=UNITS, mask_zero=True)(sr_translation)\n",
        "\n",
        "# Compute the cross attention\n",
        "attention_result = attention_layer(encoder_output, sr_translation_embed)\n",
        "\n",
        "print(f'Tensor of contexts has shape: {encoder_output.shape}')\n",
        "print(f'Tensor of translations has shape: {sr_translation_embed.shape}')\n",
        "print(f'Tensor of attention scores has shape: {attention_result.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decoder\n",
        "\n",
        "\n",
        "Now you will implement the decoder part of the neural network by completing the `Decoder` class below. Notice that in the constructor (the `__init__` method) you need to define all of the sublayers of the decoder and then use these sublayers during the forward pass (the `call` method).\n",
        "\n",
        "The decoder consists of the following layers:\n",
        "\n",
        "- [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding). For this layer you need to define the appropriate `input_dim` and `output_dim` and let it know that you are using '0' as padding, which can be done by using the appropriate value for the `mask_zero` parameter.\n",
        "  \n",
        "  \n",
        "+ Pre-attention [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM). Unlike in the encoder in which you used a Bidirectional LSTM, here you will use a vanilla LSTM. Don't forget to set the appropriate number of units and make sure that the LSTM returns the full sequence and not only the last output, which can be done by using the appropriate value for the `return_sequences` parameter. It is very important that this layer returns the state since this will be needed for inference so make sure to set the `return_state` parameter accordingly. Notice that LSTM layers return state as a tuple of two tensors called `memory_state` and `carry_state`, **however these names have been changed to better reflect what you have seen in the lectures to `hidden_state` and `cell_state` respectively**.\n",
        "\n",
        "- The attention layer that performs cross attention between the sentence to translate and the right-shifted translation. Here you need to use the `CrossAttention` layer you defined in the previous exercise.\n",
        "\n",
        "+ Post-attention [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM). Another LSTM layer. For this one you don't need it to return the state.\n",
        "\n",
        "- Finally a [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layer. This one should have the same number of units as the size of the vocabulary since you expect it to compute the logits for every possible word in the vocabulary. Make sure to use a `logsoftmax` activation function for this one, which you can get as [tf.nn.log_softmax](https://www.tensorflow.org/api_docs/python/tf/nn/log_softmax).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, units):\n",
        "        \"\"\"Initializes an instance of this class\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the vocabulary\n",
        "            units (int): Number of units in the LSTM layer\n",
        "        \"\"\"\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "\n",
        "        # The embedding layer\n",
        "        self.embedding = tf.keras.layers.Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=units,\n",
        "            mask_zero=True\n",
        "        )\n",
        "\n",
        "        # The RNN before attention\n",
        "        self.pre_attention_rnn = tf.keras.layers.LSTM(\n",
        "            units=units,\n",
        "            return_sequences=True,\n",
        "            return_state=True\n",
        "        )\n",
        "\n",
        "        # The attention layer\n",
        "        self.attention = CrossAttention(units)\n",
        "\n",
        "        # The RNN after attention\n",
        "        self.post_attention_rnn = tf.keras.layers.LSTM(\n",
        "            units=units,\n",
        "            return_sequences=True\n",
        "        )\n",
        "\n",
        "        # The dense layer with logsoftmax activation\n",
        "        self.output_layer = tf.keras.layers.Dense(\n",
        "            units=vocab_size,\n",
        "            activation=tf.nn.log_softmax\n",
        "        )\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    def call(self, context, target, state=None, return_state=False):\n",
        "        \"\"\"Forward pass of this layer\n",
        "\n",
        "        Args:\n",
        "            context (tf.Tensor): Encoded sentence to translate\n",
        "            target (tf.Tensor): The shifted-to-the-right translation\n",
        "            state (list[tf.Tensor, tf.Tensor], optional): Hidden state of the pre-attention LSTM. Defaults to None.\n",
        "            return_state (bool, optional): If set to true return the hidden states of the LSTM. Defaults to False.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: The log_softmax probabilities of predicting a particular token\n",
        "        \"\"\"\n",
        "        ### START CODE HERE ###\n",
        "\n",
        "        # Get the embedding of the input\n",
        "        x = self.embedding(target)\n",
        "\n",
        "        # Pass the embedded input into the pre attention LSTM\n",
        "        # Hints:\n",
        "        # - The LSTM you defined earlier should return the output alongside the state (made up of two tensors)\n",
        "        # - Pass in the state to the LSTM (needed for inference)\n",
        "        x, hidden_state, cell_state = self.pre_attention_rnn(x, initial_state=state)\n",
        "\n",
        "        # Perform cross attention between the context and the output of the LSTM (in that order)\n",
        "        x = self.attention(context, x)\n",
        "\n",
        "        # Do a pass through the post attention LSTM\n",
        "        x = self.post_attention_rnn(x)\n",
        "\n",
        "        # Compute the logits\n",
        "        logits = self.output_layer(x)\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        if return_state:\n",
        "            return logits, [hidden_state, cell_state]\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor of contexts has shape: (64, 14, 256)\n",
            "Tensor of right-shifted translations has shape: (64, 15)\n",
            "Tensor of logits has shape: (64, 15, 12000)\n"
          ]
        }
      ],
      "source": [
        "# Do a quick check of your implementation\n",
        "\n",
        "# Create an instance of your class\n",
        "decoder = Decoder(VOCAB_SIZE, UNITS)\n",
        "\n",
        "# Notice that you don't need the embedded version of sr_translation since this is done inside the class\n",
        "logits = decoder(encoder_output, sr_translation)\n",
        "\n",
        "print(f'Tensor of contexts has shape: {encoder_output.shape}')\n",
        "print(f'Tensor of right-shifted translations has shape: {sr_translation.shape}')\n",
        "print(f'Tensor of logits has shape: {logits.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Translator\n",
        "\n",
        "Now you have to put together all of the layers you previously coded into an actual model. For this, complete the `Translator` class below. Notice how unlike the Encoder and Decoder classes inherited from `tf.keras.layers.Layer`, the Translator class inherits from `tf.keras.Model`.\n",
        "\n",
        "Remember that `train_data` will yield a tuple with the sentence to translate and the shifted-to-the-right translation, which are the \"features\" of the model. This means that the inputs of your network will be tuples containing context and targets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Translator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, units):\n",
        "        \"\"\"Initializes an instance of this class\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the vocabulary\n",
        "            units (int): Number of units in the LSTM layer\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "\n",
        "        # Define the encoder with the appropriate vocab_size and number of units\n",
        "        self.encoder = Encoder(vocab_size, units)\n",
        "\n",
        "        # Define the decoder with the appropriate vocab_size and number of units\n",
        "        self.decoder = Decoder(vocab_size, units)\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Forward pass of this layer\n",
        "\n",
        "        Args:\n",
        "            inputs (tuple(tf.Tensor, tf.Tensor)): Tuple containing the context (sentence to translate) and the target (shifted-to-the-right translation)\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: The log_softmax probabilities of predicting a particular token\n",
        "        \"\"\"\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "\n",
        "        # In this case inputs is a tuple consisting of the context and the target, unpack it into single variables\n",
        "        context, target = inputs\n",
        "\n",
        "        # Pass the context through the encoder\n",
        "        encoded_context = self.encoder(context)\n",
        "\n",
        "        # Compute the logits by passing the encoded context and the target to the decoder\n",
        "        logits = self.decoder(encoded_context, target)\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor of sentences to translate has shape: (64, 14)\n",
            "Tensor of right-shifted translations has shape: (64, 15)\n",
            "Tensor of logits has shape: (64, 15, 12000)\n"
          ]
        }
      ],
      "source": [
        "# Do a quick check of your implementation\n",
        "\n",
        "# Create an instance of your class\n",
        "translator = Translator(VOCAB_SIZE, UNITS)\n",
        "\n",
        "# Compute the logits for every word in the vocabulary\n",
        "logits = translator((to_translate, sr_translation))\n",
        "\n",
        "print(f'Tensor of sentences to translate has shape: {to_translate.shape}')\n",
        "print(f'Tensor of right-shifted translations has shape: {sr_translation.shape}')\n",
        "print(f'Tensor of logits has shape: {logits.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n",
        "\n",
        "Now that you have an untrained instance of the NMT model, it is time to train it. You can use the `compile_and_train` function below to achieve this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compile_and_train(model, epochs=20, steps_per_epoch=500):\n",
        "    model.compile(optimizer=\"adam\", loss=masked_loss, metrics=[masked_acc, masked_loss])\n",
        "\n",
        "    history = model.fit(\n",
        "        train_data.repeat(),\n",
        "        epochs=epochs,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        validation_data=val_data,\n",
        "        validation_steps=50,\n",
        "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)],\n",
        "    )\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the translator (this takes some minutes so feel free to take a break)\n",
        "\n",
        "trained_translator, history = compile_and_train(translator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using the model for inference \n",
        "\n",
        "\n",
        "Now that your model is trained you can use it for inference. To help you with this the `generate_next_token` function is provided. Notice that this function is meant to be used inside a for-loop, so you feed to it the information of the previous step to generate the information of the next step. In particular you need to keep track of the state of the pre-attention LSTM in the decoder and if you are done with the translation. Also notice that a `temperature` variable is introduced which determines how to select the next token given the predicted logits:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_next_token(decoder, context, next_token, done, state, temperature=0.0):\n",
        "    \"\"\"Generates the next token in the sequence\n",
        "\n",
        "    Args:\n",
        "        decoder (Decoder): The decoder\n",
        "        context (tf.Tensor): Encoded sentence to translate\n",
        "        next_token (tf.Tensor): The predicted next token\n",
        "        done (bool): True if the translation is complete\n",
        "        state (list[tf.Tensor, tf.Tensor]): Hidden states of the pre-attention LSTM layer\n",
        "        temperature (float, optional): The temperature that controls the randomness of the predicted tokens. Defaults to 0.0.\n",
        "\n",
        "    Returns:\n",
        "        tuple(tf.Tensor, np.float, list[tf.Tensor, tf.Tensor], bool): The next token, log prob of said token, hidden state of LSTM and if translation is done\n",
        "    \"\"\"\n",
        "    # Get the logits and state from the decoder\n",
        "    logits, state = decoder(context, next_token, state=state, return_state=True)\n",
        "    \n",
        "    # Trim the intermediate dimension \n",
        "    logits = logits[:, -1, :]\n",
        "        \n",
        "    # If temp is 0 then next_token is the argmax of logits\n",
        "    if temperature == 0.0:\n",
        "        next_token = tf.argmax(logits, axis=-1)\n",
        "        \n",
        "    # If temp is not 0 then next_token is sampled out of logits\n",
        "    else:\n",
        "        logits = logits / temperature\n",
        "        next_token = tf.random.categorical(logits, num_samples=1)\n",
        "    \n",
        "    # Trim dimensions of size 1\n",
        "    logits = tf.squeeze(logits)\n",
        "    next_token = tf.squeeze(next_token)\n",
        "    \n",
        "    # Get the logit of the selected next_token\n",
        "    logit = logits[next_token].numpy()\n",
        "    \n",
        "    # Reshape to (1,1) since this is the expected shape for text encoded as TF tensors\n",
        "    next_token = tf.reshape(next_token, shape=(1,1))\n",
        "    \n",
        "    # If next_token is End-of-Sentence token you are done\n",
        "    if next_token == eos_id:\n",
        "        done = True\n",
        "    \n",
        "    return next_token, logit, state, done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "See how it works by running the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Next token: [[7672]]\n",
            "Logit: -18.8052\n",
            "Done? False\n"
          ]
        }
      ],
      "source": [
        "# PROCESS SENTENCE TO TRANSLATE AND ENCODE\n",
        "\n",
        "# A sentence you wish to translate\n",
        "eng_sentence = \"I love languages\"\n",
        "\n",
        "# Convert it to a tensor\n",
        "texts = tf.convert_to_tensor(eng_sentence)[tf.newaxis]\n",
        "\n",
        "# Vectorize it and pass it through the encoder\n",
        "context = english_vectorizer(texts).to_tensor()\n",
        "context = encoder(context)\n",
        "\n",
        "# SET STATE OF THE DECODER\n",
        "\n",
        "# Next token is Start-of-Sentence since you are starting fresh\n",
        "next_token = tf.fill((1,1), sos_id)\n",
        "\n",
        "# Hidden and Cell states of the LSTM can be mocked using uniform samples\n",
        "state = [tf.random.uniform((1, UNITS)), tf.random.uniform((1, UNITS))]\n",
        "\n",
        "# You are not done until next token is EOS token\n",
        "done = False\n",
        "\n",
        "# Generate next token\n",
        "next_token, logit, state, done = generate_next_token(decoder, context, next_token, done, state, temperature=0.5)\n",
        "print(f\"Next token: {next_token}\\nLogit: {logit:.4f}\\nDone? {done}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## translate\n",
        "\n",
        "Now you can put everything together to translate a given sentence. For this, complete the `translate` function below. This function will take care of the following steps: \n",
        "- Process the sentence to translate and encode it\n",
        "\n",
        "+ Set the initial state of the decoder\n",
        "\n",
        "- Get predictions of the next token (starting with the \\<SOS> token) for a maximum of iterations (in case the \\<EOS> token is never returned)\n",
        "    \n",
        "+ Return the translated text (as a string), the logit of the last iteration (this helps measure how certain was that the sequence was translated in its totality) and the translation in token format.\n",
        "\n",
        "\n",
        "Hints: \n",
        "\n",
        "- The previous cell provides a lot of insights on how this function should work, so if you get stuck refer to it.\n",
        "\n",
        "+ Some useful docs:\n",
        "    + [tf.newaxis](https://www.tensorflow.org/api_docs/python/tf#newaxis)\n",
        "\n",
        "    - [tf.fill](https://www.tensorflow.org/api_docs/python/tf/fill)\n",
        "\n",
        "    + [tf.zeros](https://www.tensorflow.org/api_docs/python/tf/zeros)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def translate(model, text, max_length=50, temperature=0.0):\n",
        "    \"\"\"Translate a given sentence from English to Portuguese\n",
        "\n",
        "    Args:\n",
        "        model (Translator): The trained translator\n",
        "        text (string): The sentence to translate\n",
        "        max_length (int, optional): The maximum length of the translation. Defaults to 50.\n",
        "        temperature (float, optional): The temperature that controls the randomness of the predicted tokens. Defaults to 0.0.\n",
        "\n",
        "    Returns:\n",
        "        tuple(str, np.float, tf.Tensor): The translation, logit that predicted <EOS> token and the tokenized translation\n",
        "    \"\"\"\n",
        "    tokens, logits = [], []\n",
        "    eng_sentence = text\n",
        "    # Convert it to a tensor\n",
        "    texts = tf.convert_to_tensor(eng_sentence)[tf.newaxis]\n",
        "\n",
        "    # Vectorize it and pass it through the encoder\n",
        "    context = english_vectorizer(texts).to_tensor()\n",
        "    context = model.encoder(context)\n",
        "\n",
        "    # SET STATE OF THE DECODER\n",
        "\n",
        "    # Next token is Start-of-Sentence since you are starting fresh\n",
        "    next_token = tf.fill((1,1), sos_id)\n",
        "\n",
        "    # Hidden and Cell states of the LSTM can be mocked using uniform samples\n",
        "    state =[tf.zeros((1, UNITS)), tf.zeros((1, UNITS))]\n",
        "\n",
        "    # You are not done until next token is EOS token\n",
        "    done = False\n",
        "\n",
        "    # Iterate for max_length iterations\n",
        "    for _ in range(max_length):\n",
        "        next_token, logit, state, done = generate_next_token(model.decoder, context, next_token, done, state, temperature)\n",
        "        #print(f\"Next token: {next_token}\\nLogit: {logit:.4f}\\nDone? {done}\")\n",
        " \n",
        "        # If done then break out of the loop\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        # Add next_token to the list of tokens\n",
        "        tokens.append(next_token)\n",
        "\n",
        "        # Add logit to the list of logits\n",
        "        logits.append(logit)\n",
        "\n",
        "    # Concatenate all tokens into a tensor\n",
        "    tokens = tf.concat(tokens, axis=-1)\n",
        "\n",
        "    # Convert the translated tokens into text\n",
        "    translation = tf.squeeze(tokens_to_text(tokens, id_to_word))\n",
        "    translation = translation.numpy().decode()\n",
        "\n",
        "    return translation, logits[-1], tokens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Try your function with temperature of 0, which will yield a deterministic output and is equivalent to a greedy decoding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Temperature: 0.0\n",
            "\n",
            "Original sentence: I love languages\n",
            "Translation: eu adoro idiomas voce .\n",
            "Translation tokens:[[  9 564 850  14   4]]\n",
            "Logit: -1.673\n"
          ]
        }
      ],
      "source": [
        "# Running this cell multiple times should return the same output since temp is 0\n",
        "\n",
        "temp = 0.0 \n",
        "original_sentence = \"I love languages\"\n",
        "\n",
        "translation, logit, tokens = translate(trained_translator, original_sentence, temperature=temp)\n",
        "\n",
        "print(f\"Temperature: {temp}\\n\\nOriginal sentence: {original_sentence}\\nTranslation: {translation}\\nTranslation tokens:{tokens}\\nLogit: {logit:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Try your function with temperature of 0.7 (stochastic output):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Temperature: 0.7\n",
            "\n",
            "Original sentence: I love languages\n",
            "Translation: adoro idiomas voce .\n",
            "Translation tokens:[[564 850  14   4]]\n",
            "Logit: -2.697\n"
          ]
        }
      ],
      "source": [
        "# Running this cell multiple times should return different outputs since temp is not 0\n",
        "# You can try different temperatures\n",
        "\n",
        "temp = 0.7\n",
        "original_sentence = \"I love languages\"\n",
        "\n",
        "translation, logit, tokens = translate(trained_translator, original_sentence, temperature=temp)\n",
        "\n",
        "print(f\"Temperature: {temp}\\n\\nOriginal sentence: {original_sentence}\\nTranslation: {translation}\\nTranslation tokens:{tokens}\\nLogit: {logit:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Minimum Bayes-Risk Decoding\n",
        "\n",
        "As mentioned in the lectures, getting the most probable token at each step may not necessarily produce the best results. Another approach is to do Minimum Bayes Risk Decoding or MBR. The general steps to implement this are:\n",
        "\n",
        "- Take several random samples\n",
        "+ Score each sample against all other samples\n",
        "- Select the one with the highest score\n",
        "\n",
        "You will be building helper functions for these steps in the following sections.\n",
        "\n",
        "With the ability to generate different translations by setting different temperature values you can do what you saw in the lectures and generate a bunch of translations and then determine which one is the best candidate. You will now do this by using the provided `generate_samples` function. This function will return any desired number of candidate translations alongside the log-probability for each one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_samples(model, text, n_samples=4, temperature=0.6):\n",
        "    \n",
        "    samples, log_probs = [], []\n",
        "\n",
        "    # Iterate for n_samples iterations\n",
        "    for _ in range(n_samples):\n",
        "        \n",
        "        # Save the logit and the translated tensor\n",
        "        _, logp, sample = translate(model, text, temperature=temperature)\n",
        "        \n",
        "        # Save the translated tensors\n",
        "        samples.append(np.squeeze(sample.numpy()).tolist())\n",
        "        \n",
        "        # Save the logits\n",
        "        log_probs.append(logp)\n",
        "                \n",
        "    return samples, log_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated tensor: [522, 850, 42, 4] has logit: -1.964\n",
            "Translated tensor: [564, 850, 14, 165, 2454, 4] has logit: -1.611\n",
            "Translated tensor: [255, 850, 21, 847, 4] has logit: -0.356\n",
            "Translated tensor: [9, 522, 850, 21, 519, 4] has logit: -0.197\n"
          ]
        }
      ],
      "source": [
        "samples, log_probs = generate_samples(trained_translator, 'I love languages')\n",
        "\n",
        "for s, l in zip(samples, log_probs):\n",
        "    print(f\"Translated tensor: {s} has logit: {l:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing overlaps\n",
        "\n",
        "Now that you can generate multiple translations it is time to come up with a method to measure the goodness of each one. As you saw in the lectures, one way to achieve this is by comparing each sample against the others. \n",
        "\n",
        "There are several metrics you can use for this purpose, as shown in the lectures and you can try experimenting with any one of these. For this assignment, you will be calculating scores for **unigram overlaps**. \n",
        "\n",
        "One of these metrics is the widely used yet simple [Jaccard similarity](https://en.wikipedia.org/wiki/Jaccard_index) which gets the intersection over union of two sets. The `jaccard_similarity` function returns this metric for any pair of candidate and reference translations:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def jaccard_similarity(candidate, reference):\n",
        "        \n",
        "    # Convert the lists to sets to get the unique tokens\n",
        "    candidate_set = set(candidate)\n",
        "    reference_set = set(reference)\n",
        "    \n",
        "    # Get the set of tokens common to both candidate and reference\n",
        "    common_tokens = candidate_set.intersection(reference_set)\n",
        "    \n",
        "    # Get the set of all tokens found in either candidate or reference\n",
        "    all_tokens = candidate_set.union(reference_set)\n",
        "    \n",
        "    # Compute the percentage of overlap (divide the number of common tokens by the number of all tokens)\n",
        "    overlap = len(common_tokens) / len(all_tokens)\n",
        "        \n",
        "    return overlap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "jaccard similarity between lists: [1, 2, 3] and [1, 2, 3, 4] is 0.750\n"
          ]
        }
      ],
      "source": [
        "l1 = [1, 2, 3]\n",
        "l2 = [1, 2, 3, 4]\n",
        "\n",
        "js = jaccard_similarity(l1, l2)\n",
        "\n",
        "print(f\"jaccard similarity between lists: {l1} and {l2} is {js:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## rouge1_similarity\n",
        "\n",
        "Jaccard similarity is good but a more commonly used metric in machine translation is the ROUGE score. For unigrams, this is called ROUGE-1 and as shown in the lectures, you can output the scores for both precision and recall when comparing two samples. To get the final score, you will want to compute the F1-score as given by:\n",
        "\n",
        "$$score = 2* \\frac{(precision * recall)}{(precision + recall)}$$\n",
        "\n",
        "For the implementation of the `rouge1_similarity` function you want to use the [Counter](https://docs.python.org/3/library/collections.html#collections.Counter) class from the Python standard library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rouge1_similarity(candidate, reference):\n",
        "    \"\"\"Computes the ROUGE 1 score between two token lists\n",
        "\n",
        "    Args:\n",
        "        candidate (list[int]): Tokenized candidate translation\n",
        "        reference (list[int]): Tokenized reference translation\n",
        "\n",
        "    Returns:\n",
        "        float: Overlap between the two token lists\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ###\n",
        "    \n",
        "    # Make a frequency table of the candidate and reference tokens\n",
        "    # Hint: use the Counter class (already imported)\n",
        "    candidate_word_counts = Counter(candidate)\n",
        "    reference_word_counts = Counter(reference)\n",
        "    \n",
        "    # Initialize overlap at 0\n",
        "    overlap = 0\n",
        "    \n",
        "    # Iterate over the tokens in the candidate frequency table\n",
        "    # Hint: Counter is a subclass of dict and you can get the keys \n",
        "    #       out of a dict using the keys method like this: dict.keys()\n",
        "    for token in candidate_word_counts.keys():\n",
        "        \n",
        "        # Get the count of the current token in the candidate frequency table\n",
        "        # Hint: You can access the counts of a token as you would access values of a dictionary\n",
        "        token_count_candidate = candidate_word_counts[token]\n",
        "        \n",
        "        # Get the count of the current token in the reference frequency table\n",
        "        # Hint: You can access the counts of a token as you would access values of a dictionary\n",
        "        token_count_reference = reference_word_counts[token]\n",
        "        \n",
        "        # Update the overlap by getting the minimum between the two token counts above\n",
        "        overlap += min(token_count_candidate, token_count_reference)\n",
        "    \n",
        "    # Compute the precision\n",
        "    # Hint: precision = overlap / (number of tokens in candidate list) \n",
        "    precision = overlap / len(candidate)\n",
        "    \n",
        "    # Compute the recall\n",
        "    # Hint: recall = overlap / (number of tokens in reference list) \n",
        "    recall = overlap / len(reference)\n",
        "    \n",
        "    if precision + recall != 0:\n",
        "        # Compute the Rouge1 Score\n",
        "        # Hint: This is equivalent to the F1 score\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "        \n",
        "        return f1_score\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "        \n",
        "    return 0 # If precision + recall = 0 then return 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rouge 1 similarity between lists: [1, 2, 3] and [1, 2, 3, 4] is 0.857\n"
          ]
        }
      ],
      "source": [
        "l1 = [1, 2, 3]\n",
        "l2 = [1, 2, 3, 4]\n",
        "\n",
        "r1s = rouge1_similarity(l1, l2)\n",
        "\n",
        "print(f\"rouge 1 similarity between lists: {l1} and {l2} is {r1s:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computing the Overall Score\n",
        "\n",
        "\n",
        "You will now build a function to generate the overall score for a particular sample. As mentioned in the lectures, you need to compare each sample with all other samples. For instance, if we generated 30 sentences, we will need to compare sentence 1 to sentences 2 through 30. Then, we compare sentence 2 to sentences 1 and 3 through 30, and so forth. At each step, we get the average score of all comparisons to get the overall score for a particular sample. To illustrate, these will be the steps to generate the scores of a 4-sample list.\n",
        "\n",
        "- Get similarity score between sample 1 and sample 2\n",
        "+ Get similarity score between sample 1 and sample 3\n",
        "- Get similarity score between sample 1 and sample 4\n",
        "+ Get average score of the first 3 steps. This will be the overall score of sample 1\n",
        "- Iterate and repeat until samples 1 to 4 have overall scores.\n",
        "\n",
        "\n",
        "The results will be stored in a dictionary for easy lookups.\n",
        "\n",
        "### average_overlap\n",
        "\n",
        "Complete the `average_overlap` function below which should implement the process described above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def average_overlap(samples, similarity_fn):\n",
        "    \"\"\"Computes the arithmetic mean of each candidate sentence in the samples\n",
        "\n",
        "    Args:\n",
        "        samples (list[list[int]]): Tokenized version of translated sentences\n",
        "        similarity_fn (Function): Similarity function used to compute the overlap\n",
        "\n",
        "    Returns:\n",
        "        dict[int, float]: A dictionary mapping the index of each translation to its score\n",
        "    \"\"\"\n",
        "    # Initialize dictionary\n",
        "    scores = {}\n",
        "    \n",
        "    # Iterate through all samples (enumerate helps keep track of indexes)\n",
        "    for index_candidate, candidate in enumerate(samples):    \n",
        "        \n",
        "        ### START CODE HERE ###\n",
        "                \n",
        "        # Initially overlap is zero\n",
        "        overlap = 0\n",
        "        \n",
        "        # Iterate through all samples (enumerate helps keep track of indexes)\n",
        "        for index_sample, sample in enumerate(samples):\n",
        "\n",
        "            # Skip if the candidate index is the same as the sample index\n",
        "            if index_candidate == index_sample:\n",
        "                continue\n",
        "                \n",
        "            # Get the overlap between candidate and sample using the similarity function\n",
        "            sample_overlap = similarity_fn(candidate, sample)\n",
        "            \n",
        "            # Add the sample overlap to the total overlap\n",
        "            overlap += sample_overlap\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Get the score for the candidate by computing the average\n",
        "        score = overlap / (len(samples) - 1)\n",
        "\n",
        "        # Only use 3 decimal points\n",
        "        score = round(score, 3)\n",
        "        \n",
        "        # Save the score in the dictionary. use index as the key.\n",
        "        scores[index_candidate] = score\n",
        "        \n",
        "    return scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average overlap between lists: [1, 2, 3], [1, 2, 4] and [1, 2, 4, 5] using Jaccard similarity is:\n",
            "\n",
            "{0: 0.45, 1: 0.625, 2: 0.575}\n"
          ]
        }
      ],
      "source": [
        "# Test with Jaccard similarity\n",
        "\n",
        "l1 = [1, 2, 3]\n",
        "l2 = [1, 2, 4]\n",
        "l3 = [1, 2, 4, 5]\n",
        "\n",
        "avg_ovlp = average_overlap([l1, l2, l3], jaccard_similarity)\n",
        "\n",
        "print(f\"average overlap between lists: {l1}, {l2} and {l3} using Jaccard similarity is:\\n\\n{avg_ovlp}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In practice, it is also common to see the weighted mean being used to calculate the overall score instead of just the arithmetic mean. This is implemented in the `weighted_avg_overlap` function below and you can use it in your experiments to see which one will give better results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def weighted_avg_overlap(samples, log_probs, similarity_fn):\n",
        "    \n",
        "    # Scores dictionary\n",
        "    scores = {}\n",
        "    \n",
        "    # Iterate over the samples\n",
        "    for index_candidate, candidate in enumerate(samples):    \n",
        "        \n",
        "        # Initialize overlap and weighted sum\n",
        "        overlap, weight_sum = 0.0, 0.0\n",
        "        \n",
        "        # Iterate over all samples and log probabilities\n",
        "        for index_sample, (sample, logp) in enumerate(zip(samples, log_probs)):\n",
        "\n",
        "            # Skip if the candidate index is the same as the sample index            \n",
        "            if index_candidate == index_sample:\n",
        "                continue\n",
        "                \n",
        "            # Convert log probability to linear scale\n",
        "            sample_p = float(np.exp(logp))\n",
        "\n",
        "            # Update the weighted sum\n",
        "            weight_sum += sample_p\n",
        "\n",
        "            # Get the unigram overlap between candidate and sample\n",
        "            sample_overlap = similarity_fn(candidate, sample)\n",
        "            \n",
        "            # Update the overlap\n",
        "            overlap += sample_p * sample_overlap\n",
        "            \n",
        "        # Compute the score for the candidate\n",
        "        score = overlap / weight_sum\n",
        "\n",
        "        # Only use 3 decimal points\n",
        "        score = round(score, 3)\n",
        "        \n",
        "        # Save the score in the dictionary. use index as the key.\n",
        "        scores[index_candidate] = score\n",
        "    \n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "weighted average overlap using Jaccard similarity is:\n",
            "\n",
            "{0: 0.443, 1: 0.631, 2: 0.558}\n"
          ]
        }
      ],
      "source": [
        "l1 = [1, 2, 3]\n",
        "l2 = [1, 2, 4]\n",
        "l3 = [1, 2, 4, 5]\n",
        "log_probs = [0.4, 0.2, 0.5]\n",
        "\n",
        "w_avg_ovlp = weighted_avg_overlap([l1, l2, l3], log_probs, jaccard_similarity)\n",
        "\n",
        "print(f\"weighted average overlap using Jaccard similarity is:\\n\\n{w_avg_ovlp}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## mbr_decode\n",
        "\n",
        "You will now put everything together in the the `mbr_decode` function below. This final step is not graded as this function is just a wrapper around all the cool stuff you have coded so far! \n",
        "\n",
        "You can use it to play around, trying different numbers of samples, temperatures and similarity functions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mbr_decode(model, text, n_samples=5, temperature=0.6, similarity_fn=jaccard_similarity):\n",
        "    \n",
        "    # Generate samples\n",
        "    samples, log_probs = generate_samples(model, text, n_samples=n_samples, temperature=temperature)\n",
        "    \n",
        "    # Compute the overlap scores\n",
        "    scores = weighted_avg_overlap(samples, log_probs, similarity_fn)\n",
        "\n",
        "    # Decode samples\n",
        "    decoded_translations = [tokens_to_text(s, id_to_word).numpy().decode('utf-8') for s in samples]\n",
        "    \n",
        "    # Find the key with the highest score\n",
        "    max_score_key = max(scores, key=lambda k: scores[k])\n",
        "    \n",
        "    # Get the translation \n",
        "    translation = decoded_translations[max_score_key]\n",
        "    \n",
        "    return translation, decoded_translations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translation candidates:\n",
            "eu adoro idiomas voce .\n",
            "amo idiomas por frequencia .\n",
            "adoro idiomas voce faz um morcego .\n",
            "eu adoro idiomas por frequencia .\n",
            "amo idiomas voce bebe .\n",
            "eu adoro idiomas voce .\n",
            "adoro idiomas ?\n",
            "eu adoro idiomas voce .\n",
            "adoro idiomas voce .\n",
            "amo idiomas voce americano .\n",
            "\n",
            "Selected translation: adoro idiomas voce .\n"
          ]
        }
      ],
      "source": [
        "english_sentence = \"I love languages\"\n",
        "\n",
        "translation, candidates = mbr_decode(trained_translator, english_sentence, n_samples=10, temperature=0.6)\n",
        "\n",
        "print(\"Translation candidates:\")\n",
        "for c in candidates:\n",
        "    print(c)\n",
        "\n",
        "print(f\"\\nSelected translation: {translation}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NLP with Attention models : Transformer Summarizer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to\n",
            "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import utils\n",
        "\n",
        "import textwrap\n",
        "wrapper = textwrap.TextWrapper(width=70)\n",
        "\n",
        "\n",
        "tf.keras.utils.set_random_seed(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import the Dataset\n",
        "You have the dataset saved in a .json file, which you can easily open with pandas. The loading function has already been taken care of in `utils.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dialogue:\n",
            "Lucas: Hey! How was your day?\n",
            "Demi: Hey there! \n",
            "Demi: It was pretty fine, actually, thank you!\n",
            "Demi: I just got promoted! :D\n",
            "Lucas: Whoa! Great news!\n",
            "Lucas: Congratulations!\n",
            "Lucas: Such a success has to be celebrated.\n",
            "Demi: I agree! :D\n",
            "Demi: Tonight at Death & Co.?\n",
            "Lucas: Sure!\n",
            "Lucas: See you there at 10pm?\n",
            "Demi: Yeah! See you there! :D\n",
            "\n",
            "Summary:\n",
            "Demi got promoted. She will celebrate that with Lucas at Death & Co at 10 pm.\n"
          ]
        }
      ],
      "source": [
        "data_dir = r\"data\\NLP with Attention models  Transformer Summarizer\\corpus\"\n",
        "\n",
        "train_data, test_data = utils.get_train_test_data(data_dir)\n",
        "\n",
        "# Take one example from the dataset and print it\n",
        "example_summary, example_dialogue = train_data.iloc[10]\n",
        "print(f\"Dialogue:\\n{example_dialogue}\")\n",
        "print(f\"\\nSummary:\\n{example_summary}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocess the data\n",
        "\n",
        "First you will do some preprocessing of the data and split it into inputs and outputs. Here you also remove some of the characters that are specific to this dataset and add the `[EOS]` (end of sentence) token to the end, like it was discussed in the lecture videos. You will also add a `[SOS]` (start of sentence) token to the beginning of the sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "document, summary = utils.preprocess(train_data)\n",
        "document_test, summary_test = utils.preprocess(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now perform the standard preprocessing with the tensorflow library. You will need to modify the filters, because you dont want the `[EOS]` tokens to be removed.\n",
        "\n",
        "Then create the vocabulary by combining the data in the documents and the summaries and using `.fit_on_texts()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of vocabulary: 34250\n"
          ]
        }
      ],
      "source": [
        "# The [ and ] from default tokens cannot be removed, because they mark the SOS and EOS token.\n",
        "filters = '!\"#$%&()*+,-./:;<=>?@\\\\^_`{|}~\\t\\n'\n",
        "oov_token = '[UNK]'\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token, lower=False)\n",
        "\n",
        "documents_and_summary = pd.concat([document, summary], ignore_index=True)\n",
        "\n",
        "tokenizer.fit_on_texts(documents_and_summary)\n",
        "\n",
        "inputs = tokenizer.texts_to_sequences(document)\n",
        "targets = tokenizer.texts_to_sequences(summary)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(f'Size of vocabulary: {vocab_size}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you can pad the tokenized sequences for the training data.\n",
        "\n",
        "For the purpose of this notebook you need to limit the length of the sequences, as transformers are really big models and are not meant to be trained in such small environments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Limit the size of the input and output data for being able to run it in this environment.\n",
        "encoder_maxlen = 150\n",
        "decoder_maxlen = 50\n",
        "\n",
        "# Pad the sequences.\n",
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=decoder_maxlen, padding='post', truncating='post')\n",
        "\n",
        "inputs = tf.cast(inputs, dtype=tf.int32)\n",
        "targets = tf.cast(targets, dtype=tf.int32)\n",
        "\n",
        "# Create the final training dataset.\n",
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Positional Encoding\n",
        "\n",
        "In sequence to sequence tasks, the relative order of your data is extremely important to its meaning. When you were training sequential neural networks such as RNNs, you fed your inputs into the network in order. Information about the order of your data was automatically fed into your model. However, when you train a Transformer network using multi-head attention, you feed your data into the model all at once. While this dramatically reduces training time, there is no information about the order of your data. This is where positional encoding is useful.\n",
        "\n",
        "You have learned how to implement the positional encoding in one of this week's labs. Here you will use the `positional_encoding` function to create positional encodings for your transformer. The function is already implemented for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def positional_encoding(positions, d_model):\n",
        "    \"\"\"\n",
        "    Precomputes a matrix with all the positional encodings \n",
        "    \n",
        "    Arguments:\n",
        "        positions (int): Maximum number of positions to be encoded \n",
        "        d_model (int): Encoding size \n",
        "    \n",
        "    Returns:\n",
        "        pos_encoding (tf.Tensor): A matrix of shape (1, position, d_model) with the positional encodings\n",
        "    \"\"\"\n",
        "    \n",
        "    position = np.arange(positions)[:, np.newaxis]\n",
        "    k = np.arange(d_model)[np.newaxis, :]\n",
        "    i = k // 2\n",
        "    \n",
        "    # initialize a matrix angle_rads of all the angles \n",
        "    angle_rates = 1 / np.power(10000, (2 * i) / np.float32(d_model))\n",
        "    angle_rads = position * angle_rates\n",
        "  \n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  \n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    \n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    \n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Masking\n",
        "\n",
        "There are two types of masks that are useful when building your Transformer network: the *padding mask* and the *look-ahead mask*. Both help the softmax computation give the appropriate weights to the words in your input sentence. \n",
        "\n",
        "You have already learned how to implement and use them in one of this week's labs. Here they are implemented for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_padding_mask(decoder_token_ids):\n",
        "    \"\"\"\n",
        "    Creates a matrix mask for the padding cells\n",
        "    \n",
        "    Arguments:\n",
        "        decoder_token_ids (matrix like): matrix of size (n, m)\n",
        "    \n",
        "    Returns:\n",
        "        mask (tf.Tensor): binary tensor of size (n, 1, m)\n",
        "    \"\"\"    \n",
        "    seq = 1 - tf.cast(tf.math.equal(decoder_token_ids, 0), tf.float32)\n",
        "  \n",
        "    # add extra dimensions to add the padding to the attention logits. \n",
        "    # this will allow for broadcasting later when comparing sequences\n",
        "    return seq[:, tf.newaxis, :] \n",
        "\n",
        "\n",
        "def create_look_ahead_mask(sequence_length):\n",
        "    \"\"\"\n",
        "    Returns a lower triangular matrix filled with ones\n",
        "    \n",
        "    Arguments:\n",
        "        sequence_length (int): matrix size\n",
        "    \n",
        "    Returns:\n",
        "        mask (tf.Tensor): binary tensor of size (sequence_length, sequence_length)\n",
        "    \"\"\"\n",
        "    mask = tf.linalg.band_part(tf.ones((1, sequence_length, sequence_length)), -1, 0)\n",
        "    return mask "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-Attention\n",
        "    \n",
        "The use of self-attention paired with traditional convolutional networks allows for parallelization which speeds up training. You will implement **scaled dot product attention** which takes in a query, key, value, and a mask as inputs to return rich, attention-based vector representations of the words in your sequence. This type of self-attention can be mathematically expressed as:\n",
        "$$\n",
        "\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\tag{4}\\\n",
        "$$\n",
        "\n",
        "* $Q$ is the matrix of queries \n",
        "* $K$ is the matrix of keys\n",
        "* $V$ is the matrix of values\n",
        "* $M$ is the optional mask you choose to apply \n",
        "* ${d_k}$ is the dimension of the keys, which is used to scale everything down so the softmax doesn't explode\n",
        "### scaled_dot_product_attention \n",
        "\n",
        "Implement the function `scaled_dot_product_attention()` to create attention-based representations.\n",
        "\n",
        "**Reminder**: The boolean mask parameter can be passed in as `none` or as either padding or look-ahead. \n",
        "    \n",
        "* Multiply (1. - mask) by -1e9 before adding it to the scaled attention logits. \n",
        "\n",
        "**Additional Hints**\n",
        "* You may find [tf.matmul](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul) useful for matrix multiplication (check how you can use the parameter transpose_b).\n",
        "* You can use [tf.keras.activations.softmax](https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax) for softmax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"\n",
        "    Calculate the attention weights.\n",
        "      q, k, v must have matching leading dimensions.\n",
        "      k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "      The mask has different shapes depending on its type(padding or look ahead) \n",
        "      but it must be broadcastable for addition.\n",
        "\n",
        "    Arguments:\n",
        "        q (tf.Tensor): query of shape (..., seq_len_q, depth)\n",
        "        k (tf.Tensor): key of shape (..., seq_len_k, depth)\n",
        "        v (tf.Tensor): value of shape (..., seq_len_v, depth_v)\n",
        "        mask (tf.Tensor): mask with shape broadcastable \n",
        "              to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        output -- attention_weights\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ###\n",
        "    \n",
        "    # Multiply q and k transposed.\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    # scale matmul_qk with the square root of dk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:  # Don't replace this None\n",
        "        mask = (1-mask)*-1e9\n",
        "        scaled_attention_logits += mask\n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    # Multiply the attention weights by v\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output:\n",
            " [[[1.   0.62]\n",
            "  [0.62 0.62]\n",
            "  [0.74 0.31]]]\n",
            "\n",
            "Attention weigths:\n",
            " [[[0.   0.38 0.   0.23 0.38]\n",
            "  [0.38 0.   0.   0.23 0.38]\n",
            "  [0.26 0.43 0.   0.16 0.16]]]\n"
          ]
        }
      ],
      "source": [
        "# Test your function!\n",
        "q = np.array([[1, 1, 0, 1], [0, 1, 1, 1], [1, 0, 1, 1]]).astype(np.float32)\n",
        "k = np.array([[1, 1, 0, 1], [1, 0, 1, 1 ], [1, 1, 1, 0], [0, 0, 0, 1], [0, 1, 0, 1]]).astype(np.float32)\n",
        "v = np.array([[0, 0], [1, 0], [1, 0], [1, 1], [1, 1]]).astype(np.float32)\n",
        "mask = np.array([[[0, 1, 0, 1, 1], [1, 0, 0, 1, 1], [1, 1, 0, 1, 1]]])\n",
        "\n",
        "ou, atw = scaled_dot_product_attention(q, k, v, mask)\n",
        "ou = np.around(ou, decimals=2)\n",
        "atw = np.around(atw, decimals=2)\n",
        "\n",
        "print(f\"Output:\\n {ou}\")\n",
        "print(f\"\\nAttention weigths:\\n {atw}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encoder\n",
        "\n",
        "The Transformer Encoder layer pairs self-attention and convolutional neural network style of processing to improve the speed of training and passes K and V matrices to the Decoder, which you'll build later in the assignment. In this section of the assignment, you will implement the Encoder by pairing multi-head attention and a feed forward neural network (Figure 2a). \n",
        "\n",
        "\n",
        "* `MultiHeadAttention` you can think of as computing the self-attention several times to detect different features. \n",
        "* Feed forward neural network contains two Dense layers which we'll implement as the function `FullyConnected`\n",
        "\n",
        "Your input sentence first passes through a *multi-head attention layer*, where the encoder looks at other words in the input sentence as it encodes a specific word. The outputs of the multi-head attention layer are then fed to a *feed forward neural network*. The exact same feed forward network is independently applied to each position.\n",
        "   \n",
        "* For the `MultiHeadAttention` layer, you will use the [MultiHeadAttention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention) implemented in Keras. If you're curious about how to split the query matrix Q, key matrix K, and value matrix V into different heads, you can look through the implementation. \n",
        "* You will also use the [Sequential API](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) with two dense layers to built the feed forward neural network layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def FullyConnected(embedding_dim, fully_connected_dim):\n",
        "    \"\"\"\n",
        "    Returns a sequential model consisting of two dense layers. The first dense layer has\n",
        "    fully_connected_dim neurons and is activated by relu. The second dense layer has\n",
        "    embedding_dim and no activation.\n",
        "\n",
        "    Arguments:\n",
        "        embedding_dim (int): output dimension\n",
        "        fully_connected_dim (int): dimension of the hidden layer\n",
        "\n",
        "    Returns:\n",
        "        _ (tf.keras.Model): sequential model\n",
        "    \"\"\"\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(fully_connected_dim, activation='relu'),  # (batch_size, seq_len, d_model)\n",
        "        tf.keras.layers.Dense(embedding_dim)  # (batch_size, seq_len, d_model)\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoder Layer\n",
        "\n",
        "Now you can pair multi-head attention and feed forward neural network together in an encoder layer! You will also use residual connections and layer normalization to help speed up training.\n",
        "\n",
        "The encoder block  is is already implemented for you. Take a very close look at its implementation, as you will later have to create the decoder yourself, and a lot of the code is very similar. The encoder block performs the following steps: \n",
        "1. It takes the Q, V, K matrices and a boolean mask to a multi-head attention layer. Remember that to compute *self*-attention Q, V and K are the same. You will also perform Dropout in this multi-head attention layer during training. \n",
        "2. There is a skip connection to add your original input `x` and the output of the multi-head attention layer. \n",
        "3. After adding the skip connection, the output passes through the first normalization layer.\n",
        "4. Finally, steps 1-3 are repeated but with the feed forward neural network with a dropout layer instead of the multi-head attention layer. \n",
        "\n",
        "\n",
        "* The `__init__` method creates all the layers that will be accesed by the the `call` method. Wherever you want to use a layer defined inside  the `__init__`  method you will have to use the syntax `self.[insert layer name]`. \n",
        "* You will find the documentation of [MultiHeadAttention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention) helpful. *Note that if query, key and value are the same, then this function performs self-attention.*\n",
        "* The call arguments for `self.mha` are (Where B is for batch_size, T is for target sequence shapes, and S is output_shape):\n",
        " - `query`: Query Tensor of shape (B, T, dim).\n",
        " - `value`: Value Tensor of shape (B, S, dim).\n",
        " - `key`: Optional key Tensor of shape (B, S, dim). If not given, will use the same value for both key and value, which is the most common case.\n",
        " - `attention_mask`: a boolean mask of shape (B, T, S), that prevents attention to certain positions. The boolean mask specifies which query elements can attend to which key elements, 1 indicates attention and 0 indicates no attention. Broadcasting can happen for the missing batch dimensions and the head dimension.\n",
        " - `return_attention_scores`: A boolean to indicate whether the output should be attention output if True, or (attention_output, attention_scores) if False. Defaults to False.\n",
        " - `training`: Python boolean indicating whether the layer should behave in training mode (adding dropout) or in inference mode (no dropout). Defaults to either using the training mode of the parent layer/model, or False (inference) if there is no parent layer. Take a look at [tf.keras.layers.Dropout](https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/keras/layers/Dropout) for more details (Additional reading in [Keras FAQ](https://keras.io/getting_started/faq/#whats-the-difference-between-the-training-argument-in-call-and-the-trainable-attribute))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    The encoder layer is composed by a multi-head self-attention mechanism,\n",
        "    followed by a simple, positionwise fully connected feed-forward network. \n",
        "    This architecture includes a residual connection around each of the two \n",
        "    sub-layers, followed by layer normalization.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim, num_heads, fully_connected_dim,\n",
        "                 dropout_rate=0.1, layernorm_eps=1e-6):\n",
        "        \n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embedding_dim,\n",
        "            dropout=dropout_rate\n",
        "        )\n",
        "\n",
        "        self.ffn = FullyConnected(\n",
        "            embedding_dim=embedding_dim,\n",
        "            fully_connected_dim=fully_connected_dim\n",
        "        )\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
        "\n",
        "        self.dropout_ffn = tf.keras.layers.Dropout(dropout_rate)\n",
        "    \n",
        "    def call(self, x, training, mask):\n",
        "        \"\"\"\n",
        "        Forward pass for the Encoder Layer\n",
        "        \n",
        "        Arguments:\n",
        "            x (tf.Tensor): Tensor of shape (batch_size, input_seq_len, fully_connected_dim)\n",
        "            training (bool): Boolean, set to true to activate\n",
        "                        the training mode for dropout layers\n",
        "            mask (tf.Tensor): Boolean mask to ensure that the padding is not \n",
        "                    treated as part of the input\n",
        "        Returns:\n",
        "            encoder_layer_out (tf.Tensor): Tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
        "        \"\"\"\n",
        "        # calculate self-attention using mha(~1 line).\n",
        "        # Dropout is added by Keras automatically if the dropout parameter is non-zero during training\n",
        "        self_mha_output = self.mha(x, x, x, mask)  # Self attention (batch_size, input_seq_len, fully_connected_dim)\n",
        "        \n",
        "        # skip connection\n",
        "        # apply layer normalization on sum of the input and the attention output to get the  \n",
        "        # output of the multi-head attention layer\n",
        "        skip_x_attention = self.layernorm1(x + self_mha_output)  # (batch_size, input_seq_len, fully_connected_dim)\n",
        "\n",
        "        # pass the output of the multi-head attention layer through a ffn\n",
        "        ffn_output = self.ffn(skip_x_attention)  # (batch_size, input_seq_len, fully_connected_dim)\n",
        "        \n",
        "        # apply dropout layer to ffn output during training\n",
        "        # use `training=training`\n",
        "        ffn_output = self.dropout_ffn(ffn_output, training=training)\n",
        "        \n",
        "        # apply layer normalization on sum of the output from multi-head attention (skip connection) and ffn output\n",
        "        # to get the output of the encoder layer\n",
        "        encoder_layer_out = self.layernorm2(skip_x_attention + ffn_output)  # (batch_size, input_seq_len, embedding_dim)\n",
        "        \n",
        "        return encoder_layer_out\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Full Encoder\n",
        "\n",
        "Now you're ready to build the full Transformer Encoder (Figure 2b), where you will embed your input and add the positional encodings you calculated. You will then feed your encoded embeddings to a stack of Encoder layers. \n",
        "\n",
        "The Encoder class is implemented for you. It performs the following steps: \n",
        "1. Pass the input through the Embedding layer.\n",
        "2. Scale the embedding by multiplying it by the square root of the embedding dimension. \n",
        "3. Add the position encoding: self.pos_encoding `[:, :seq_len, :]` to the embedding.\n",
        "4. Pass the encoded embedding through a dropout layer\n",
        "5. Pass the output of the dropout layer through the stack of encoding layers using a for loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    The entire Encoder starts by passing the input to an embedding layer \n",
        "    and using positional encoding to then pass the output through a stack of\n",
        "    encoder Layers\n",
        "        \n",
        "    \"\"\"  \n",
        "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size,\n",
        "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, self.embedding_dim)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
        "                                                self.embedding_dim)\n",
        "\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(embedding_dim=self.embedding_dim,\n",
        "                                        num_heads=num_heads,\n",
        "                                        fully_connected_dim=fully_connected_dim,\n",
        "                                        dropout_rate=dropout_rate,\n",
        "                                        layernorm_eps=layernorm_eps) \n",
        "                           for _ in range(self.num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "        \n",
        "    def call(self, x, training, mask):\n",
        "        \"\"\"\n",
        "        Forward pass for the Encoder\n",
        "        \n",
        "        Arguments:\n",
        "            x (tf.Tensor): Tensor of shape (batch_size, seq_len, embedding_dim)\n",
        "            training (bool): Boolean, set to true to activate\n",
        "                        the training mode for dropout layers\n",
        "            mask (tf.Tensor): Boolean mask to ensure that the padding is not \n",
        "                    treated as part of the input\n",
        "\n",
        "        Returns:\n",
        "            x (tf.Tensor): Tensor of shape (batch_size, seq_len, embedding_dim)\n",
        "        \"\"\"\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        \n",
        "        # Pass input through the Embedding layer\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, embedding_dim)\n",
        "        # Scale embedding by multiplying it by the square root of the embedding dimension\n",
        "        x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
        "        # Add the position encoding to embedding\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        # Pass the encoded embedding through a dropout layer\n",
        "        # use `training=training`\n",
        "        x = self.dropout(x, training=training)\n",
        "        # Pass the output through the stack of encoding layers \n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, embedding_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decoder\n",
        "\n",
        "Now it is time to implement the decoder. You have seen it in the videos and you can use some help by looking at the encoder implementation above. The Decoder layer takes the K and V matrices generated by the Encoder and computes the second multi-head attention layer with the Q matrix from the output (Figure 3a).\n",
        "\n",
        "\n",
        "### Decoder Layer\n",
        "\n",
        "Again, you'll pair multi-head attention with a feed forward neural network, but this time you'll implement two multi-head attention layers. You will also use residual connections and layer normalization to help speed up training.\n",
        "\n",
        "    \n",
        "1. Block 1 is a multi-head attention layer with a residual connection, and look-ahead mask. Like in the `EncoderLayer`, Dropout is defined within the multi-head attention layer.\n",
        "2. Block 2 will take into account the output of the Encoder, so the multi-head attention layer will receive K and V from the encoder, and Q from the Block 1. You will then apply a normalization layer and a residual connection, just like you did before with the `EncoderLayer`.\n",
        "3. Finally, Block 3 is a feed forward neural network with dropout and normalization layers and a residual connection.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    The decoder layer is composed by two multi-head attention blocks, \n",
        "    one that takes the new input and uses self-attention, and the other \n",
        "    one that combines it with the output of the encoder, followed by a\n",
        "    fully connected block. \n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embedding_dim,\n",
        "            dropout=dropout_rate\n",
        "        )\n",
        "\n",
        "        self.mha2 = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embedding_dim,\n",
        "            dropout=dropout_rate\n",
        "        )\n",
        "\n",
        "        self.ffn = FullyConnected(\n",
        "            embedding_dim=embedding_dim,\n",
        "            fully_connected_dim=fully_connected_dim\n",
        "        )\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
        "\n",
        "        self.dropout_ffn = tf.keras.layers.Dropout(dropout_rate)\n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        \"\"\"\n",
        "        Forward pass for the Decoder Layer\n",
        "        \n",
        "        Arguments:\n",
        "            x (tf.Tensor): Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
        "            enc_output (tf.Tensor): Tensor of shape(batch_size, input_seq_len, fully_connected_dim)\n",
        "            training (bool): Boolean, set to true to activate\n",
        "                        the training mode for dropout layers\n",
        "            look_ahead_mask (tf.Tensor): Boolean mask for the target_input\n",
        "            padding_mask (tf.Tensor): Boolean mask for the second multihead attention layer\n",
        "        Returns:\n",
        "            out3 (tf.Tensor): Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
        "            attn_weights_block1 (tf.Tensor): Tensor of shape (batch_size, num_heads, target_seq_len, target_seq_len)\n",
        "            attn_weights_block2 (tf.Tensor): Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        # enc_output.shape == (batch_size, input_seq_len, fully_connected_dim)\n",
        "        \n",
        "        # BLOCK 1\n",
        "        # calculate self-attention and return attention scores as attn_weights_block1.\n",
        "        # Dropout will be applied during training\n",
        "        mult_attn_out1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask, training=training,return_attention_scores=True)\n",
        "        \n",
        "        # apply layer normalization (layernorm1) to the sum of the attention output and the input\n",
        "        Q1 = self.layernorm1(x + mult_attn_out1)\n",
        "\n",
        "        # BLOCK 2\n",
        "        # calculate cross-attention using the output of the first block as query, and encoder output as key and value.\n",
        "        # Dropout will be applied during training\n",
        "        mult_attn_out2, attn_weights_block2 = self.mha2(Q1, enc_output, enc_output, padding_mask, training=training,return_attention_scores=True)\n",
        "        \n",
        "        # apply layer normalization (layernorm2) to the sum of the attention output and the output of the first block\n",
        "        mult_attn_out2 = self.layernorm2(mult_attn_out2 + Q1)\n",
        "                \n",
        "        # BLOCK 3\n",
        "        # pass the output of the second block through a ffn\n",
        "        ffn_output = self.ffn(mult_attn_out2)\n",
        "        \n",
        "        # apply a dropout layer to the ffn output\n",
        "        # use `training=training`\n",
        "        ffn_output = self.dropout_ffn(ffn_output, training=training)\n",
        "        \n",
        "        # apply layer normalization (layernorm3) to the sum of the ffn output and the output of the second block\n",
        "        out3 = self.layernorm3(ffn_output + mult_attn_out2)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using embedding_dim=12 and num_heads=16:\n",
            "\n",
            "q has shape:(1, 15, 12)\n",
            "Output of encoder has shape:(1, 7, 8)\n",
            "\n",
            "Output of decoder layer has shape:(1, 15, 12)\n",
            "Att Weights Block 1 has shape:(1, 16, 15, 15)\n",
            "Att Weights Block 2 has shape:(1, 16, 15, 7)\n"
          ]
        }
      ],
      "source": [
        "# Test your function!\n",
        "key_dim = 12\n",
        "n_heads = 16\n",
        "\n",
        "decoderLayer_test = DecoderLayer(embedding_dim=key_dim, num_heads=n_heads, fully_connected_dim=32)\n",
        "\n",
        "q = np.ones((1, 15, key_dim))\n",
        "encoder_test_output = tf.convert_to_tensor(np.random.rand(1, 7, 8))\n",
        "look_ahead_mask = create_look_ahead_mask(q.shape[1])\n",
        "\n",
        "out, attn_w_b1, attn_w_b2 = decoderLayer_test(q, encoder_test_output, False, look_ahead_mask, None)\n",
        "\n",
        "print(f\"Using embedding_dim={key_dim} and num_heads={n_heads}:\\n\")\n",
        "print(f\"q has shape:{q.shape}\")\n",
        "print(f\"Output of encoder has shape:{encoder_test_output.shape}\\n\")\n",
        "\n",
        "print(f\"Output of decoder layer has shape:{out.shape}\")\n",
        "print(f\"Att Weights Block 1 has shape:{attn_w_b1.shape}\")\n",
        "print(f\"Att Weights Block 2 has shape:{attn_w_b2.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Full Decoder\n",
        "You're almost there! Time to use your Decoder layer to build a full Transformer Decoder (Figure 3b). You will embed your output and add positional encodings. You will then feed your encoded embeddings to a stack of Decoder layers. \n",
        "\n",
        "1. Pass your generated output through the Embedding layer.\n",
        "2. Scale your embedding by multiplying it by the square root of your embedding dimension. Remember to cast the embedding dimension to data type `tf.float32` before computing the square root.\n",
        "3. Add the position encoding: self.pos_encoding `[:, :seq_len, :]` to your embedding.\n",
        "4. Pass the encoded embedding through a dropout layer, remembering to use the `training` parameter to set the model training mode. \n",
        "5. Pass the output of the dropout layer through the stack of Decoding layers using a for loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    The entire Encoder starts by passing the target input to an embedding layer \n",
        "    and using positional encoding to then pass the output through a stack of\n",
        "    decoder Layers\n",
        "        \n",
        "    \"\"\" \n",
        "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, target_vocab_size,\n",
        "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, self.embedding_dim)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.embedding_dim)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(embedding_dim=self.embedding_dim,\n",
        "                                        num_heads=num_heads,\n",
        "                                        fully_connected_dim=fully_connected_dim,\n",
        "                                        dropout_rate=dropout_rate,\n",
        "                                        layernorm_eps=layernorm_eps) \n",
        "                           for _ in range(self.num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        \"\"\"\n",
        "        Forward pass for the Decoder\n",
        "        \n",
        "        Arguments:\n",
        "            x (tf.Tensor): Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
        "            enc_output (tf.Tensor):  Tensor of shape(batch_size, input_seq_len, fully_connected_dim)\n",
        "            training (bool): Boolean, set to true to activate\n",
        "                        the training mode for dropout layers\n",
        "            look_ahead_mask (tf.Tensor): Boolean mask for the target_input\n",
        "            padding_mask (tf.Tensor): Boolean mask for the second multihead attention layer\n",
        "        Returns:\n",
        "            x (tf.Tensor): Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
        "            attention_weights (dict[str: tf.Tensor]): Dictionary of tensors containing all the attention weights\n",
        "                                each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
        "        \n",
        "        \"\"\"\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "        \n",
        "        # create word embeddings \n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        # scale embeddings by multiplying by the square root of their dimension\n",
        "        x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
        "        \n",
        "        # add positional encodings to word embedding\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        # apply a dropout layer to x\n",
        "        # use `training=training`\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        # use a for loop to pass x through a stack of decoder layers and update attention_weights\n",
        "        for i in range(self.num_layers):\n",
        "            # pass x and the encoder output through a stack of decoder layers and save the attention weights\n",
        "            # of block 1 and 2\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "\n",
        "            #update attention_weights dictionary with the attention weights of block 1 and block 2\n",
        "            attention_weights[f'decoder_layer_{i+1}_block1_self_att'] = block1\n",
        "            attention_weights[f'decoder_layer_{i+1}_block2_decenc_att'] = block2\n",
        "        \n",
        "        return x, attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using num_layers=5, embedding_dim=13 and num_heads=17:\n",
            "\n",
            "x has shape:(3, 4)\n",
            "Output of encoder has shape:(3, 7, 9)\n",
            "\n",
            "Output of decoder has shape:(3, 4, 13)\n",
            "\n",
            "Attention weights:\n",
            "decoder_layer_1_block1_self_att has shape:(3, 17, 4, 4)\n",
            "decoder_layer_1_block2_decenc_att has shape:(3, 17, 4, 7)\n",
            "decoder_layer_2_block1_self_att has shape:(3, 17, 4, 4)\n",
            "decoder_layer_2_block2_decenc_att has shape:(3, 17, 4, 7)\n",
            "decoder_layer_3_block1_self_att has shape:(3, 17, 4, 4)\n",
            "decoder_layer_3_block2_decenc_att has shape:(3, 17, 4, 7)\n",
            "decoder_layer_4_block1_self_att has shape:(3, 17, 4, 4)\n",
            "decoder_layer_4_block2_decenc_att has shape:(3, 17, 4, 7)\n",
            "decoder_layer_5_block1_self_att has shape:(3, 17, 4, 4)\n",
            "decoder_layer_5_block2_decenc_att has shape:(3, 17, 4, 7)\n"
          ]
        }
      ],
      "source": [
        "# Test your function!\n",
        "n_layers = 5\n",
        "emb_d = 13\n",
        "n_heads = 17\n",
        "fully_connected_dim = 16\n",
        "target_vocab_size = 300\n",
        "maximum_position_encoding = 6\n",
        "\n",
        "x = np.array([[3, 2, 1, 1], [2, 1, 1, 0], [2, 1, 1, 0]])\n",
        "\n",
        "encoder_test_output = tf.convert_to_tensor(np.random.rand(3, 7, 9))\n",
        "\n",
        "look_ahead_mask = create_look_ahead_mask(x.shape[1])\n",
        "\n",
        "decoder_test = Decoder(n_layers, emb_d, n_heads, fully_connected_dim, target_vocab_size,maximum_position_encoding)\n",
        "                   \n",
        "outd, att_weights = decoder_test(x, encoder_test_output, False, look_ahead_mask, None)\n",
        "\n",
        "print(f\"Using num_layers={n_layers}, embedding_dim={emb_d} and num_heads={n_heads}:\\n\")\n",
        "print(f\"x has shape:{x.shape}\")\n",
        "print(f\"Output of encoder has shape:{encoder_test_output.shape}\\n\")\n",
        "\n",
        "print(f\"Output of decoder has shape:{outd.shape}\\n\")\n",
        "print(\"Attention weights:\")\n",
        "for name, tensor in att_weights.items():\n",
        "    print(f\"{name} has shape:{tensor.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformer\n",
        "    \n",
        "The flow of data through the Transformer Architecture is as follows:\n",
        "* First your input passes through an Encoder, which is just repeated Encoder layers that you implemented:\n",
        "    - embedding and positional encoding of your input\n",
        "    - multi-head attention on your input\n",
        "    - feed forward neural network to help detect features\n",
        "* Then the predicted output passes through a Decoder, consisting of the decoder layers that you implemented:\n",
        "    - embedding and positional encoding of the output\n",
        "    - multi-head attention on your generated output\n",
        "    - multi-head attention with the Q from the first multi-head attention layer and the K and V from the Encoder\n",
        "    - a feed forward neural network to help detect features\n",
        "* Finally, after the Nth Decoder layer, one dense layer and a softmax are applied to generate prediction for the next output in your sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Complete transformer with an Encoder and a Decoder\n",
        "    \"\"\"\n",
        "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, \n",
        "               target_vocab_size, max_positional_encoding_input,\n",
        "               max_positional_encoding_target, dropout_rate=0.1, layernorm_eps=1e-6):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers=num_layers,\n",
        "                               embedding_dim=embedding_dim,\n",
        "                               num_heads=num_heads,\n",
        "                               fully_connected_dim=fully_connected_dim,\n",
        "                               input_vocab_size=input_vocab_size,\n",
        "                               maximum_position_encoding=max_positional_encoding_input,\n",
        "                               dropout_rate=dropout_rate,\n",
        "                               layernorm_eps=layernorm_eps)\n",
        "\n",
        "        self.decoder = Decoder(num_layers=num_layers, \n",
        "                               embedding_dim=embedding_dim,\n",
        "                               num_heads=num_heads,\n",
        "                               fully_connected_dim=fully_connected_dim,\n",
        "                               target_vocab_size=target_vocab_size, \n",
        "                               maximum_position_encoding=max_positional_encoding_target,\n",
        "                               dropout_rate=dropout_rate,\n",
        "                               layernorm_eps=layernorm_eps)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size, activation='softmax')\n",
        "    \n",
        "    def call(self, input_sentence, output_sentence, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        \"\"\"\n",
        "        Forward pass for the entire Transformer\n",
        "        Arguments:\n",
        "            input_sentence (tf.Tensor): Tensor of shape (batch_size, input_seq_len, fully_connected_dim)\n",
        "                              An array of the indexes of the words in the input sentence\n",
        "            output_sentence (tf.Tensor): Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
        "                              An array of the indexes of the words in the output sentence\n",
        "            training (bool): Boolean, set to true to activate\n",
        "                        the training mode for dropout layers\n",
        "            enc_padding_mask (tf.Tensor): Boolean mask to ensure that the padding is not \n",
        "                    treated as part of the input\n",
        "            look_ahead_mask (tf.Tensor): Boolean mask for the target_input\n",
        "            dec_padding_mask (tf.Tensor): Boolean mask for the second multihead attention layer\n",
        "        Returns:\n",
        "            final_output (tf.Tensor): The final output of the model\n",
        "            attention_weights (dict[str: tf.Tensor]): Dictionary of tensors containing all the attention weights for the decoder\n",
        "                                each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
        "        \n",
        "        \"\"\"\n",
        "        ### START CODE HERE ###\n",
        "        # call self.encoder with the appropriate arguments to get the encoder output\n",
        "        enc_output = self.encoder(input_sentence, training, enc_padding_mask)\n",
        "        \n",
        "        # call self.decoder with the appropriate arguments to get the decoder output\n",
        "        # dec_output.shape == (batch_size, tar_seq_len, fully_connected_dim)\n",
        "        dec_output, attention_weights_decoder = self.decoder(output_sentence, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "        \n",
        "        # pass decoder output through a linear layer and softmax (~1 line)\n",
        "        final_output = self.final_layer(dec_output)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        return final_output, attention_weights_decoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using num_layers=3, target_vocab_size=350 and num_heads=17:\n",
            "\n",
            "sentence_a has shape:(1, 7)\n",
            "sentence_b has shape:(1, 7)\n",
            "\n",
            "Output of transformer (summary) has shape:(1, 7, 350)\n",
            "\n",
            "Attention weights:\n",
            "decoder_layer_1_block1_self_att has shape:(1, 17, 7, 7)\n",
            "decoder_layer_1_block2_decenc_att has shape:(1, 17, 7, 7)\n",
            "decoder_layer_2_block1_self_att has shape:(1, 17, 7, 7)\n",
            "decoder_layer_2_block2_decenc_att has shape:(1, 17, 7, 7)\n",
            "decoder_layer_3_block1_self_att has shape:(1, 17, 7, 7)\n",
            "decoder_layer_3_block2_decenc_att has shape:(1, 17, 7, 7)\n"
          ]
        }
      ],
      "source": [
        "# Test your function!\n",
        "n_layers = 3\n",
        "emb_d = 13\n",
        "n_heads = 17\n",
        "fully_connected_dim = 8\n",
        "input_vocab_size = 300\n",
        "target_vocab_size = 350\n",
        "max_positional_encoding_input = 12\n",
        "max_positional_encoding_target = 12\n",
        "\n",
        "transformer = Transformer(n_layers, \n",
        "    emb_d, \n",
        "    n_heads, \n",
        "    fully_connected_dim, \n",
        "    input_vocab_size, \n",
        "    target_vocab_size, \n",
        "    max_positional_encoding_input,\n",
        "    max_positional_encoding_target)\n",
        "\n",
        "# 0 is the padding value\n",
        "sentence_a = np.array([[2, 3, 1, 3, 0, 0, 0]])\n",
        "sentence_b = np.array([[1, 3, 4, 0, 0, 0, 0]])\n",
        "\n",
        "enc_padding_mask = create_padding_mask(sentence_a)\n",
        "dec_padding_mask = create_padding_mask(sentence_a)\n",
        "\n",
        "look_ahead_mask = create_look_ahead_mask(sentence_a.shape[1])\n",
        "\n",
        "test_summary, att_weights = transformer(\n",
        "    sentence_a,\n",
        "    sentence_b,\n",
        "    False,\n",
        "    enc_padding_mask,\n",
        "    look_ahead_mask,\n",
        "    dec_padding_mask\n",
        ")\n",
        "\n",
        "print(f\"Using num_layers={n_layers}, target_vocab_size={target_vocab_size} and num_heads={n_heads}:\\n\")\n",
        "print(f\"sentence_a has shape:{sentence_a.shape}\")\n",
        "print(f\"sentence_b has shape:{sentence_b.shape}\")\n",
        "\n",
        "print(f\"\\nOutput of transformer (summary) has shape:{test_summary.shape}\\n\")\n",
        "print(\"Attention weights:\")\n",
        "for name, tensor in att_weights.items():\n",
        "    print(f\"{name} has shape:{tensor.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize the Model\n",
        "Now that you have defined the model, you can initialize and train it. First you can initialize the model with the parameters below. Note that generally these models are much larger and you are using a smaller version to fit this environment and to be able to train it in just a few minutes.\n",
        "\n",
        "The base model described in the original Transformer paper used `num_layers=6`, `embedding_dim=512`, and `fully_connected_dim=2048`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the model parameters\n",
        "num_layers = 2\n",
        "embedding_dim = 128\n",
        "fully_connected_dim = 128\n",
        "num_heads = 2\n",
        "positional_encoding_length = 256\n",
        "\n",
        "# Initialize the model\n",
        "transformer = Transformer(\n",
        "    num_layers, \n",
        "    embedding_dim, \n",
        "    num_heads, \n",
        "    fully_connected_dim,\n",
        "    vocab_size, \n",
        "    vocab_size, \n",
        "    positional_encoding_length, \n",
        "    positional_encoding_length,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare for Training the Model\n",
        "\n",
        "The original transformer paper uses Adam optimizer with custom learning rate scheduling, which we define in the cell below. This was empirically shown to produce faster convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        self.d_model = tf.cast(d_model, dtype=tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, dtype=tf.float32)\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = CustomSchedule(embedding_dim)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(0.0002, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below you can plot, how the custom learning rate looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(learning_rate(tf.range(40000, dtype=tf.float32)))\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.xlabel('Train Step')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, you set up the loss. Since the target sequences are padded, it is important to apply a padding mask when calculating the loss.\n",
        "\n",
        "You will use the sparse categorical cross-entropy loss function (`tf.keras.losses.SparseCategoricalCrossentropy`) and set the parameter `from_logits` to False since the Transformer does not output raw logits since the last layer has a softmax activation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
        "\n",
        "def masked_loss(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "\n",
        "# Here you will store the losses, so you can later plot them\n",
        "losses = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you can define your custom training function. If you are not very advanced with tensorflow, you can understand this function as an alternative to using `model.compile()` and `model.fit()`, but with added extra flexibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(model, inp, tar):\n",
        "    \"\"\"\n",
        "    One training step for the transformer\n",
        "    Arguments:\n",
        "        inp (tf.Tensor): Input data to summarize\n",
        "        tar (tf.Tensor): Target (summary)\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    # Create masks\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar_inp)[1])\n",
        "    dec_padding_mask = create_padding_mask(inp) # Notice that both encoder and decoder padding masks are equal\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = model(\n",
        "            inp,\n",
        "            tar_inp, \n",
        "            True, \n",
        "            enc_padding_mask, \n",
        "            look_ahead_mask, \n",
        "            dec_padding_mask\n",
        "        )\n",
        "        loss = masked_loss(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you are ready for training the model. But before starting the training, you can also define one more set of functions to perform the inference. Because you are using a custom training loop, you can do whatever you want between the training steps. And wouldnt't it be fun to see after each epoch some examples of how the model performs?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summarization\n",
        "\n",
        "The last thing you will implement is inference. With this, you will be able to produce actual summaries of the documents. You will use a simple method called greedy decoding, which means you will predict one word at a time and append it to the output. You will start with an `[SOS]` token and repeat the word by word inference until the model returns you the `[EOS]` token or until you reach the maximum length of the sentence (you need to add this limit, otherwise a poorly trained model could give you infinite sentences without ever producing the `[EOS]` token.\n",
        "\n",
        "\n",
        "Write a helper function that predicts the next word, so you can use it to write the whole sentences. Hint: this is very similar to what happens in the train_step, but you have to set the training of the model to False."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def next_word(model, encoder_input, output):\n",
        "    \"\"\"\n",
        "    Helper function for summarization that uses the model to predict just the next word.\n",
        "    Arguments:\n",
        "        encoder_input (tf.Tensor): Input data to summarize\n",
        "        output (tf.Tensor): (incomplete) target (summary)\n",
        "    Returns:\n",
        "        predicted_id (tf.Tensor): The id of the predicted word\n",
        "    \"\"\"\n",
        "    # Create a padding mask for the input (encoder)\n",
        "    enc_padding_mask = create_padding_mask(encoder_input)\n",
        "\n",
        "    # Create a look-ahead mask for the output\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(output)[1])\n",
        "\n",
        "    # Create a padding mask for the input (decoder)\n",
        "    dec_padding_mask = create_padding_mask(encoder_input)\n",
        "\n",
        "    # Run the prediction of the next word with the transformer model\n",
        "    predictions, attention_weights = model.call(encoder_input, output, training=False, \n",
        "                                                enc_padding_mask=enc_padding_mask, \n",
        "                                                look_ahead_mask=look_ahead_mask, \n",
        "                                                dec_padding_mask=dec_padding_mask)\n",
        "\n",
        "    predictions = predictions[:, -1:, :]\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "    \n",
        "    return predicted_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted token: [[30832]]\n",
            "Predicted word: hahahaha😂😂😂😂😂😂😂\n"
          ]
        }
      ],
      "source": [
        "# Take a random sentence as an input\n",
        "input_document = tokenizer.texts_to_sequences([\"a random sentence\"])\n",
        "input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "encoder_input = tf.expand_dims(input_document[0], 0)\n",
        "\n",
        "# Take the start of sentence token as the only token in the output to predict the next word\n",
        "output = tf.expand_dims([tokenizer.word_index[\"[SOS]\"]], 0)\n",
        "\n",
        "# predict the next word with your function\n",
        "predicted_token = next_word(transformer, encoder_input, output)\n",
        "print(f\"Predicted token: {predicted_token}\")\n",
        "\n",
        "predicted_word = tokenizer.sequences_to_texts(predicted_token.numpy())[0]\n",
        "print(f\"Predicted word: {predicted_word}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def summarize(model, input_document):\n",
        "    \"\"\"\n",
        "    A function for summarization using the transformer model\n",
        "    Arguments:\n",
        "        input_document (tf.Tensor): Input data to summarize\n",
        "    Returns:\n",
        "        _ (str): The summary of the input_document\n",
        "    \"\"\"    \n",
        "    input_document = tokenizer.texts_to_sequences([input_document])\n",
        "    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "    encoder_input = tf.expand_dims(input_document[0], 0)\n",
        "    \n",
        "    output = tf.expand_dims([tokenizer.word_index[\"[SOS]\"]], 0)\n",
        "    \n",
        "    for i in range(decoder_maxlen):\n",
        "        predicted_id = next_word(model, encoder_input, output)\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "        \n",
        "        if predicted_id == tokenizer.word_index[\"[EOS]\"]:\n",
        "            break\n",
        "\n",
        "    return tokenizer.sequences_to_texts(output.numpy())[0]  # since there is just one translated document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you can already summarize a sentence! But beware, since the model was not yet trained at all, it will just produce nonsense."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set example:\n",
            "[SOS] amanda: i baked  cookies. do you want some?  jerry: sure!  amanda: i'll bring you tomorrow :-) [EOS]\n",
            "\n",
            "Human written summary:\n",
            "[SOS] amanda baked cookies and will bring jerry some tomorrow. [EOS]\n",
            "\n",
            "Model written summary:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"[SOS] hahahaha😂😂😂😂😂😂😂 recheck messing messing amanda bettina affected die amanda tomasz electoral pranksters time… hahhah 👍👍20th moaning taht coctails helicoptered wild' mock venezia 👱\\u200d♀️ taht north north wrapped wrapped wrapped maybelline fluffly clarissa's developments developments developments developments offering developments tvs developments haemorrhoids maybelline 00pm spuds denon elary sorbonne carey's pressing sauté\""
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_set_example = 0\n",
        "\n",
        "# Check a summary of a document from the training set\n",
        "print('Training set example:')\n",
        "print(document[training_set_example])\n",
        "print('\\nHuman written summary:')\n",
        "print(summary[training_set_example])\n",
        "print('\\nModel written summary:')\n",
        "summarize(transformer, document[training_set_example])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the model\n",
        "\n",
        "Now you can finally train the model. Below is a loop that will train your model for 20 epochs. note that it should take about 30 seconds per epoch (with the exception of the first few epochs which can take a few minutes each).\n",
        "\n",
        "Note that after each epoch you perform the summarization on one of the sentences in the test set and print it out, so you can see how your model is improving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss 7.886631\n",
            "Time taken for one epoch: 65.038494348526 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] [EOS]\n",
            "\n",
            "Epoch 2, Loss 6.599931\n",
            "Time taken for one epoch: 24.798268795013428 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] is going to the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
            "\n",
            "Epoch 3, Loss 6.028731\n",
            "Time taken for one epoch: 16.792022466659546 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] tom is going to the new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new\n",
            "\n",
            "Epoch 4, Loss 5.683131\n",
            "Time taken for one epoch: 12.960438251495361 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] tom is going to the new new new new new new new job [EOS]\n",
            "\n",
            "Epoch 5, Loss 5.474731\n",
            "Time taken for one epoch: 13.100719690322876 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] the new new new new new new job and she will be at the weekend [EOS]\n",
            "\n",
            "Epoch 6, Loss 5.321831\n",
            "Time taken for one epoch: 11.09825587272644 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] tom is going to the new job at the party [EOS]\n",
            "\n",
            "Epoch 7, Loss 5.193931\n",
            "Time taken for one epoch: 13.835851430892944 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] tom is going to the party with her [EOS]\n",
            "\n",
            "Epoch 8, Loss 5.082331\n",
            "Time taken for one epoch: 20.40879774093628 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] the new year's eve is going to the party [EOS]\n",
            "\n",
            "Epoch 9, Loss 4.976031\n",
            "Time taken for one epoch: 19.52234983444214 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] the new year's eve is going to the party [EOS]\n",
            "\n",
            "Epoch 10, Loss 4.876031\n",
            "Time taken for one epoch: 18.84659504890442 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] the car is going to the party with her [EOS]\n",
            "\n",
            "Epoch 11, Loss 4.776531\n",
            "Time taken for one epoch: 18.813578844070435 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] mark will buy the office on the office [EOS]\n",
            "\n",
            "Epoch 12, Loss 4.675331\n",
            "Time taken for one epoch: 12.727384328842163 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] the car is going to the cinema with her [EOS]\n",
            "\n",
            "Epoch 13, Loss 4.572831\n",
            "Time taken for one epoch: 9.572580337524414 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] ben will buy the red dress for the office [EOS]\n",
            "\n",
            "Epoch 14, Loss 4.472631\n",
            "Time taken for one epoch: 9.436481475830078 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] alice has just arrived to the office today [EOS]\n",
            "\n",
            "Epoch 15, Loss 4.370731\n",
            "Time taken for one epoch: 9.085449934005737 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] alice has just arrived to the office today [EOS]\n",
            "\n",
            "Epoch 16, Loss 4.270431\n",
            "Time taken for one epoch: 9.101617097854614 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] alice and alice are going to the cinema with the office today [EOS]\n",
            "\n",
            "Epoch 17, Loss 4.167531\n",
            "Time taken for one epoch: 10.1934654712677 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] alice has just arrived to the cinema with her [EOS]\n",
            "\n",
            "Epoch 18, Loss 4.073231\n",
            "Time taken for one epoch: 8.96051573753357 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] alice and alice are going to the cinema with the wedding on saturday [EOS]\n",
            "\n",
            "Epoch 19, Loss 3.971231\n",
            "Time taken for one epoch: 9.142754077911377 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] alice has just arrived to the store he has to go to the cinema with alice [EOS]\n",
            "\n",
            "Epoch 20, Loss 3.879931\n",
            "Time taken for one epoch: 8.610831499099731 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] alice has just arrived to the store with amanda and hannah will go to the cinema [EOS]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Take an example from the test set, to monitor it during training\n",
        "test_example = 0\n",
        "true_summary = summary_test[test_example]\n",
        "true_document = document_test[test_example]\n",
        "\n",
        "# Define the number of epochs\n",
        "epochs = 20\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    start = time.time()\n",
        "    train_loss.reset_states()\n",
        "    number_of_batches=len(list(enumerate(dataset)))\n",
        "\n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        print(f'Epoch {epoch+1}, Batch {batch+1}/{number_of_batches}', end='\\r')\n",
        "        train_step(transformer, inp, tar)\n",
        "    \n",
        "    print (f'Epoch {epoch+1}, Loss {train_loss.result():.4f}')\n",
        "    losses.append(train_loss.result())\n",
        "    \n",
        "    print (f'Time taken for one epoch: {time.time() - start} sec')\n",
        "    print('Example summarization on the test set:')\n",
        "    print('  True summarization:')\n",
        "    print(f'    {true_summary}')\n",
        "    print('  Predicted summarization:')\n",
        "    print(f'    {summarize(transformer, true_document)}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the loss funtion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Epoch')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(losses)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summarize some Sentences!\n",
        "\n",
        "Below you can see an example of summarization of a sentence from the training set and a sentence from the test set. See if you notice anything interesting about them!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set example:\n",
            "[SOS] amanda: i baked  cookies. do you want some?  jerry: sure!  amanda: i'll bring you tomorrow :-) [EOS]\n",
            "\n",
            "Human written summary:\n",
            "[SOS] amanda baked cookies and will bring jerry some tomorrow. [EOS]\n",
            "\n",
            "Model written summary:\n",
            "[SOS] hahahaha😂😂😂😂😂😂😂 recheck messing messing amanda bettina affected die amanda tomasz electoral pranksters time… hahhah 👍👍20th moaning taht coctails helicoptered wild' mock venezia 👱‍♀️ taht north north wrapped wrapped wrapped maybelline fluffly clarissa's developments developments developments developments offering developments tvs developments haemorrhoids maybelline 00pm spuds denon elary sorbonne carey's pressing sauté\n"
          ]
        }
      ],
      "source": [
        "training_set_example = 0\n",
        "\n",
        "# Check a summary of a document from the training set\n",
        "print('Training set example:')\n",
        "print(document[training_set_example])\n",
        "print('\\nHuman written summary:')\n",
        "print(summary[training_set_example])\n",
        "print('\\nModel written summary:')\n",
        "print(summarize(transformer, document[training_set_example]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set example:\n",
            "[SOS] will: hey babe, what do you want for dinner tonight?  emma:  gah, don't even worry about it tonight  will: what do you mean? everything ok?  emma: not really, but it's ok, don't worry about cooking though, i'm not hungry  will: well what time will you be home?  emma: soon, hopefully  will: you sure? maybe you want me to pick you up?  emma: no no it's alright. i'll be home soon, i'll tell you when i get home.   will: alright, love you.   emma: love you too.  [EOS]\n",
            "\n",
            "Human written summary:\n",
            "[SOS] emma will be home soon and she will let will know. [EOS]\n",
            "\n",
            "Model written summary:\n",
            "[SOS] emma will pick up with emma at home tonight [EOS]\n"
          ]
        }
      ],
      "source": [
        "test_set_example = 3\n",
        "\n",
        "# Check a summary of a document from the test set\n",
        "print('Test set example:')\n",
        "print(document_test[test_set_example])\n",
        "print('\\nHuman written summary:')\n",
        "print(summary_test[test_set_example])\n",
        "print('\\nModel written summary:')\n",
        "print(summarize(transformer, document_test[test_set_example]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you critically examine the output of the model, you can notice a few things:\n",
        " - In the training set the model output is (almost) identical to the real output (already after 20 epochs and even more so with more epochs). This might be because the training set is relatively small and the model is relatively big and has thus learned the sentences in the training set by heart (overfitting).\n",
        " - While the performance on the training set looks amazing, it is not so good on the test set. The model overfits, but fails to generalize. Again an easy candidate to blame is the small training set and a comparatively large model, but there might be a variety of other factors.\n",
        " - Look at the test set example 3 and its summarization. Would you summarize it the same way as it is written here? Sometimes the data may be ambiguous. And the training of **your model can only be as good as your data**.\n",
        "\n",
        "Here you only use a small dataset, to show that something can be learned in a reasonable amount of time in a relatively small environment. Generally, large transformers are trained on more than one task and on very large quantities of data to achieve superb performance. You will learn more about this in the rest of this course."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Gwp2GLa1JOIB",
        "eaAZxryLLNpY",
        "Eporz7XLIuQm",
        "JJ0Ytku_IuQ2",
        "MhFCstzdIuQ-",
        "gPVNnwTFIuQ_",
        "j1uK0bZQPI0s",
        "NHt4O5_AIuRV",
        "exarCLToKeqx",
        "21ryqYvLIuRX",
        "iY_0yZMTIuRY",
        "H7nmrUaqIuRZ",
        "4u196kGPIuRb"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
